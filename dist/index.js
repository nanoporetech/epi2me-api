/**
 * Copyright Metrichor Ltd. (An Oxford Nanopore Technologies Company) 2020
 */

"use strict";function _interopDefault(e){return e&&"object"===typeof e&&"default"in e?e.default:e}var lodash=require("lodash"),fs=_interopDefault(require("fs-extra")),os=require("os"),os__default=_interopDefault(os),path=_interopDefault(require("path")),AWS=_interopDefault(require("aws-sdk")),axios=_interopDefault(require("axios")),crypto=_interopDefault(require("crypto")),tunnel=require("tunnel"),proxy=_interopDefault(require("proxy-agent")),readline=_interopDefault(require("readline")),zlib=_interopDefault(require("zlib")),gql=_interopDefault(require("graphql-tag")),apolloClient=require("apollo-client"),apolloCacheInmemory=require("apollo-cache-inmemory"),apolloLink=require("apollo-link"),apolloLinkHttp=require("apollo-link-http"),axiosFetch=require("@lifeomic/axios-fetch"),io=_interopDefault(require("socket.io-client")),sqlite=_interopDefault(require("sqlite")),name="@metrichor/epi2me-api",version="3.0.1821",license="MPL-2.0",repository="https://git.oxfordnanolabs.local/metrichor/api.git",description="API for communicating with the EPI2ME website(s)",main="dist/index.js",module$1="dist/index.es.js",dependencies={"@lifeomic/axios-fetch":"^1.4.1","@types/axios":"^0.14.0","@types/socket.io-client":"^1.4.32","apollo-cache-inmemory":"^1.6.3","apollo-client":"^2.6.4","apollo-link":"^1.2.13","apollo-link-context":"^1.0.19","apollo-link-http":"^1.5.16","aws-sdk":"^2.585.0",axios:"^0.19.0","core-js":"^3.4.8","fs-extra":"^8.1.0",graphql:"^14.5.8","graphql-tag":"^2.10.1",lodash:"4.17.15","proxy-agent":"^3.1.1",save:"^2.4.0","socket.io-client":"^2.3.0",sqlite:"^3.0.3",tunnel:"^0.0.6"},devDependencies={"@babel/cli":"^7.7.5","@babel/core":"^7.7.5","@babel/plugin-proposal-object-rest-spread":"^7.7.4","@babel/preset-env":"^7.7.6","@babel/register":"^7.7.4","@types/bunyan":"^1.8.6","@types/rollup":"^0.54.0","@types/rollup-plugin-json":"^3.0.2","babel-eslint":"^10.0.3",bunyan:"^1.8.12",eslint:"^6.7.2","eslint-config-airbnb-base":"^14.0.0","eslint-config-defaults":"9.0.0","eslint-config-prettier":"^6.7.0","eslint-plugin-babel":"^5.3.0","eslint-plugin-import":"^2.19.1","eslint-plugin-prettier":"^3.1.1",husky:"^3.0.8","lint-staged":"^9.5.0",mocha:"6.2.2",nyc:"^14.1.1",prettier:"^1.18.2","prettier-eslint":"^9.0.0",rollup:"^1.27.9","rollup-plugin-analyzer":"^3.2.1","rollup-plugin-cpy":"^2.0.1","rollup-plugin-eslint":"^7.0.0","rollup-plugin-generate-package-json":"^3.1.3","rollup-plugin-json":"^4.0.0","rollup-plugin-license":"^0.13.0","rollup-plugin-terser":"^5.1.2",sinon:"7.5.0",tmp:"0.1.0","xunit-file":"*"},browserslist=[">0.2%","not dead","not ie <= 11","not op_mini all"],scripts={"build:version":'jq ".version=\\"$(jq -r .version package.json | cut -d . -f 1-2).${PATCH:-$(date +%-H%M)}\\"" < package.json > package.json.tmp && mv package.json.tmp package.json',"lint-js":'eslint --ignore-path .eslintignore --ignore-pattern "!**/.*" .',"fix-js":"npm run lint-js --fix",lint:"npm run lint-js",deps:"npm ci","clean:dist":"rm -rf dist","clean:build":"rm -rf build && rm -rf dist/lib",clean:"npm run clean:build && npm run clean:dist",test:"npx mocha --recursive --require @babel/register test",cover:"npm install && npm run lint && npx nyc --reporter=html --reporter=text mocha --recursive --require @babel/register test",build:"npm ci && npm run build:dist","rollup:build":"npx rollup -c","rollup:watch":"npx rollup -cw","build:dist":"npm run build:version && npm run clean:dist && npm ci && npm run rollup:build"},pkg={name:name,version:version,license:license,repository:repository,description:description,main:main,module:module$1,dependencies:dependencies,devDependencies:devDependencies,browserslist:browserslist,"lint-staged":{"*.{ts,tsx,js,jsx}":["npm run fix-ts --fix","git add --force"],"*.{json,md,graphql}":["prettier --write","git add --force"]},scripts:scripts};axios.defaults.validateStatus=e=>e<=504;const utils=function(){const e=(e,t)=>{e.headers||(e.headers={});let s=t;if(s||(s={}),!s.apikey)return;if(e.headers["X-EPI2ME-ApiKey"]=s.apikey,!s.apisecret)return;e.headers["X-EPI2ME-SignatureDate"]=(new Date).toISOString(),e.url.match(/^https:/)&&(e.url=e.url.replace(/:443/,"")),e.url.match(/^http:/)&&(e.url=e.url.replace(/:80/,""));const i=[e.url,Object.keys(e.headers).sort().filter(e=>e.match(/^x-epi2me/i)).map(t=>`${t}:${e.headers[t]}`).join("\n")].join("\n"),o=crypto.createHmac("sha1",s.apisecret).update(i).digest("hex");e.headers["X-EPI2ME-SignatureV0"]=o},t=async e=>{const t=e?e.data:null;if(!t)return Promise.reject(new Error("unexpected non-json response"));if(e&&e.status>=400){let s=`Network error ${e.status}`;return t.error&&(s=t.error),504===e.status&&(s="Please check your network connection and try again."),Promise.reject(new Error(s))}return t.error?Promise.reject(new Error(t.error)):Promise.resolve(t)};return{version:version,headers:(t,s)=>{const{log:i}=lodash.merge({log:{debug:()=>{}}},s);let o=s;if(o||(o={}),t.headers=lodash.merge({Accept:"application/json","Content-Type":"application/json","X-EPI2ME-Client":o.user_agent||"api","X-EPI2ME-Version":o.agent_version||utils.version},t.headers,o.headers),"signing"in o&&!o.signing||e(t,o),o.proxy){const e=o.proxy.match(/https?:\/\/((\S+):(\S+)@)?(\S+):(\d+)/),s=e[2],r=e[3],n={host:e[4],port:e[5]};s&&r&&(n.proxyAuth=`${s}:${r}`),o.proxy.match(/^https/)?(i.debug("using HTTPS over HTTPS proxy",JSON.stringify(n)),t.httpsAgent=tunnel.httpsOverHttps({proxy:n})):(i.debug("using HTTPS over HTTP proxy",JSON.stringify(n)),t.httpsAgent=tunnel.httpsOverHttp({proxy:n})),t.proxy=!1}},get:async(e,s)=>{const{log:i}=lodash.merge({log:{debug:()=>{}}},s);let o,r=s.url,n=e;s.skip_url_mangle?o=n:(n=`/${n}`,o=(r=r.replace(/\/+$/,""))+(n=n.replace(/\/+/g,"/")));const a={url:o,gzip:!0};let l;utils.headers(a,s);try{i.debug(`GET ${a.url}`),l=await axios.get(a.url,a)}catch(c){return Promise.reject(c)}return t(l,s)},post:async(e,s,i)=>{const{log:o}=lodash.merge({log:{debug:()=>{}}},i);let r=i.url;const n={url:`${r=r.replace(/\/+$/,"")}/${e.replace(/\/+/g,"/")}`,gzip:!0,data:s,headers:{}};if(i.legacy_form){const e=[],t=lodash.merge({json:JSON.stringify(s)},s);Object.keys(t).sort().forEach(s=>{e.push(`${s}=${escape(t[s])}`)}),n.data=e.join("&"),n.headers["Content-Type"]="application/x-www-form-urlencoded"}utils.headers(n,i);const{data:a}=n;let l;delete n.data;try{o.debug(`POST ${n.url}`),l=await axios.post(n.url,a,n)}catch(c){return Promise.reject(c)}return i.handler?i.handler(l):t(l,i)},put:async(e,s,i,o)=>{const{log:r}=lodash.merge({log:{debug:()=>{}}},o);let n=o.url;const a={url:`${n=n.replace(/\/+$/,"")}/${e.replace(/\/+/g,"/")}/${s}`,gzip:!0,data:i,headers:{}};if(o.legacy_form){const e=[],t=lodash.merge({json:JSON.stringify(i)},i);Object.keys(t).sort().forEach(s=>{e.push(`${s}=${escape(t[s])}`)}),a.data=e.join("&"),a.headers["Content-Type"]="application/x-www-form-urlencoded"}utils.headers(a,o);const{data:l}=a;let c;delete a.data;try{r.debug(`PUT ${a.url}`),c=await axios.put(a.url,l,a)}catch(u){return Promise.reject(u)}return t(c,o)}}}();utils.pipe=async(e,t,s,i)=>{let o=s.url,r=`/${e}`;const n={url:(o=o.replace(/\/+$/,""))+(r=r.replace(/\/+/g,"/")),gzip:!0,headers:{"Accept-Encoding":"gzip",Accept:"application/gzip"}};return utils.headers(n,s),s.proxy&&(n.proxy=s.proxy),i&&(n.onUploadProgress=i),n.responseType="stream",new Promise((e,s)=>{axios.get(n.url,n).then(i=>{const o=fs.createWriteStream(t);i.data.pipe(o),o.on("finish",()=>{e(t)}),o.on("error",e=>{s(new Error(`writer failed ${String(e)}`))})}).catch(e=>{s(e)})})};let IdCounter=0;utils.getFileID=()=>`FILE_${IdCounter+=1}`,utils.lsRecursive=async(e,t,s)=>{let i=e;const o=fs.statSync(t);if(s){if(await s(t,o))return null}return o.isDirectory()?fs.readdir(t).then(e=>e.map(e=>path.join(t,e))).then(e=>Promise.all(e.map(e=>utils.lsRecursive(i,e,s)))).then(e=>lodash.flatten(e)):(o.isFile()&&i===t&&(i=path.dirname(t)),[{name:path.parse(t).base,path:t,relative:t.replace(i,""),size:o.size,id:utils.getFileID()}])},utils.loadInputFiles=async({inputFolder:e,outputFolder:t,filetype:s},i,o)=>{let r=s;r instanceof Array||(r=[r]),r=r.map(e=>e&&0!==e.indexOf(".")?`.${e}`:e);const n=await utils.lsRecursive(e,e,async(e,s)=>{const i=path.basename(e),n=[new Promise((t,s)=>"downloads"===i||"skip"===i||"fail"===i||"fastq_fail"===i||"tmp"===i?s(new Error(`${e} failed basic filename`)):t("basic ok")),new Promise((o,n)=>{const a=r.length?new RegExp(`(?:${r.join("|")})$`):null;return e.split(path.sep).filter(e=>e.match(/^[.]/)).length||t&&i===path.basename(t)||a&&!e.match(a)&&s.isFile()?n(new Error(`${e} failed extended filename`)):o("extended ok")}),o?new Promise((t,s)=>{o(e).then(i=>i?s(new Error(`${e} failed extraFilter`)):t("extra ok"))}):Promise.resolve("extra skip")];return Promise.all(n).then(()=>null).catch(()=>"exclude")});return Promise.resolve(lodash.remove(n,null))};var local=!1,url="https://epi2me.nanoporetech.com",user_agent="EPI2ME API",region="eu-west-1",sessionGrace=5,uploadTimeout=1200,downloadTimeout=1200,fileCheckInterval=5,downloadCheckInterval=3,stateCheckInterval=60,inFlightDelay=600,waitTimeSeconds=20,waitTokenError=30,transferPoolSize=3,downloadMode="data+telemetry",filetype=[".fastq",".fq",".fastq.gz",".fq.gz"],signing=!0,DEFAULTS={local:local,url:url,user_agent:user_agent,region:region,sessionGrace:sessionGrace,uploadTimeout:uploadTimeout,downloadTimeout:downloadTimeout,fileCheckInterval:fileCheckInterval,downloadCheckInterval:downloadCheckInterval,stateCheckInterval:stateCheckInterval,inFlightDelay:inFlightDelay,waitTimeSeconds:waitTimeSeconds,waitTokenError:waitTokenError,transferPoolSize:transferPoolSize,downloadMode:downloadMode,filetype:filetype,signing:signing};class REST{constructor(e){this.options=lodash.assign({agent_version:utils.version,local:local,url:url,user_agent:user_agent,signing:signing},e),this.log=this.options.log}async list(e){try{const t=await utils.get(e,this.options),s=e.match(/^[a-z_]+/i)[0];return Promise.resolve(t[`${s}s`])}catch(t){return this.log.error(`list error ${String(t)}`),Promise.reject(t)}}async read(e,t){try{const s=await utils.get(`${e}/${t}`,this.options);return Promise.resolve(s)}catch(s){return this.log.error("read",s),Promise.reject(s)}}async user(){return this.options.local?{accounts:[{id_user_account:"none",number:"NONE",name:"None"}]}:utils.get("user",this.options)}async status(){return utils.get("status",this.options)}async jwt(){try{const e=e=>e.headers["x-epi2me-jwt"]?Promise.resolve(e.headers["x-epi2me-jwt"]):Promise.reject(new Error("failed to fetch JWT")),t=await utils.post("authenticate",{},lodash.merge({handler:e},this.options));return Promise.resolve(t)}catch(e){return Promise.reject(e)}}async instanceToken(e,t){return utils.post("token",lodash.merge(t,{id_workflow_instance:e}),lodash.assign({},this.options,{legacy_form:!0}))}async installToken(e){return utils.post("token/install",{id_workflow:e},lodash.assign({},this.options,{legacy_form:!0}))}async attributes(){return this.list("attribute")}async workflows(){return this.list("workflow")}async amiImages(){if(this.options.local)throw new Error("amiImages unsupported in local mode");return this.list("ami_image")}async amiImage(e,t,s){let i,o,r,n;if(e&&t&&s instanceof Function?(i=e,o=t,r=s,n="update"):e&&t instanceof Object&&!(t instanceof Function)?(i=e,o=t,n="update"):e instanceof Object&&t instanceof Function?(o=e,r=t,n="create"):e instanceof Object&&!t?(o=e,n="create"):(n="read",i=e,r=t instanceof Function?t:null),this.options.local){const e=new Error("ami_image unsupported in local mode");return r?r(e):Promise.reject(e)}if("update"===n)try{const e=await utils.put("ami_image",i,o,this.options);return r?r(null,e):Promise.resolve(e)}catch(a){return r?r(a):Promise.reject(a)}if("create"===n)try{const e=await utils.post("ami_image",o,this.options);return r?r(null,e):Promise.resolve(e)}catch(a){return r?r(a):Promise.reject(a)}if(!i){const e=new Error("no id_ami_image specified");return r?r(e):Promise.reject(e)}try{const e=await this.read("ami_image",i);return r?r(null,e):Promise.resolve(e)}catch(a){return r?r(a):Promise.reject(a)}}async workflow(e,t,s){let i,o,r,n;if(e&&t&&s instanceof Function?(i=e,o=t,r=s,n="update"):e&&t instanceof Object&&!(t instanceof Function)?(i=e,o=t,n="update"):e instanceof Object&&t instanceof Function?(o=e,r=t,n="create"):e instanceof Object&&!t?(o=e,n="create"):(n="read",i=e,r=t instanceof Function?t:null),"update"===n)try{const e=await utils.put("workflow",i,o,this.options);return r?r(null,e):Promise.resolve(e)}catch(u){return r?r(u):Promise.reject(u)}if("create"===n)try{const e=await utils.post("workflow",o,this.options);return r?r(null,e):Promise.resolve(e)}catch(u){return r?r(u):Promise.reject(u)}if(!i){const e=new Error("no workflow id specified");return r?r(e):Promise.reject(e)}const a={};try{const e=await this.read("workflow",i);if(e.error)throw new Error(e.error);lodash.merge(a,e)}catch(u){return this.log.error(`${i}: error fetching workflow ${String(u)}`),r?r(u):Promise.reject(u)}lodash.merge(a,{params:{}});try{const e=await utils.get(`workflow/config/${i}`,this.options);if(e.error)throw new Error(e.error);lodash.merge(a,e)}catch(u){return this.log.error(`${i}: error fetching workflow config ${String(u)}`),r?r(u):Promise.reject(u)}const l=lodash.filter(a.params,{widget:"ajax_dropdown"}),c=[...l.map((e,t)=>{const s=l[t];return new Promise((e,t)=>{const i=s.values.source.replace("{{EPI2ME_HOST}}","").replace(/&?apikey=\{\{EPI2ME_API_KEY\}\}/,"");utils.get(i,this.options).then(t=>{const i=t[s.values.data_root];return i&&(s.values=i.map(e=>({label:e[s.values.items.label_key],value:e[s.values.items.value_key]}))),e()}).catch(e=>(this.log.error(`failed to fetch ${i}`),t(e)))})})];try{return await Promise.all(c),r?r(null,a):Promise.resolve(a)}catch(u){return this.log.error(`${i}: error fetching config and parameters ${String(u)}`),r?r(u):Promise.reject(u)}}async startWorkflow(e){return utils.post("workflow_instance",e,lodash.assign({},this.options,{legacy_form:!0}))}async stopWorkflow(e){return utils.put("workflow_instance/stop",e,null,lodash.assign({},this.options,{legacy_form:!0}))}async workflowInstances(e){if(e&&e.run_id)try{const t=(await utils.get(`workflow_instance/wi?show=all&columns[0][name]=run_id;columns[0][searchable]=true;columns[0][search][regex]=true;columns[0][search][value]=${e.run_id};`,this.options)).data.map(e=>({id_workflow_instance:e.id_ins,id_workflow:e.id_flo,run_id:e.run_id,description:e.desc,rev:e.rev}));return Promise.resolve(t)}catch(t){return Promise.reject(t)}return this.list("workflow_instance")}async workflowInstance(e){return this.read("workflow_instance",e)}async workflowConfig(e){return utils.get(`workflow/config/${e}`,this.options)}async register(e,t){return utils.put("reg",e,{description:t||`${os__default.userInfo().username}@${os__default.hostname()}`},lodash.assign({},this.options,{signing:!1}))}async datasets(e){let t=e;return t||(t={}),t.show||(t.show="mine"),this.list(`dataset?show=${t.show}`)}async dataset(e){return this.options.local?this.datasets().then(t=>t.find(t=>t.id_dataset===e)):this.read("dataset",e)}async fetchContent(e,t){const s=lodash.assign({},this.options,{skip_url_mangle:!0,headers:{"Content-Type":""}});try{const i=await utils.get(e,s);return t?t(null,i):Promise.resolve(i)}catch(i){return t?t(i):Promise.reject(i)}}}class REST_FS extends REST{async workflows(e){if(!this.options.local)return super.workflows(e);const t=path.join(this.options.url,"workflows");let s;try{return s=(await fs.readdir(t)).filter(e=>fs.statSync(path.join(t,e)).isDirectory()).map(e=>path.join(t,e,"workflow.json")).map(e=>fs.readJsonSync(e)),e?e(null,s):Promise.resolve(s)}catch(i){return this.log.warn(i),e?e(void 0):Promise.reject(void 0)}}async workflow(e,t,s){if(!this.options.local||!e||"object"===typeof e||s)return super.workflow(e,t,s);const i=path.join(this.options.url,"workflows"),o=path.join(i,e,"workflow.json");try{const e=await fs.readJson(o);return s?s(null,e):Promise.resolve(e)}catch(r){return s?s(r):Promise.reject(r)}}async workflowInstances(e,t){if(!this.options.local)return super.workflowInstances(e,t);let s,i;if(!e||e instanceof Function||void 0!==t?(s=e,i=t):i=e,i){const e=new Error("querying of local instances unsupported in local mode");return s?s(e):Promise.reject(e)}const o=path.join(this.options.url,"instances");try{let e=await fs.readdir(o);return e=(e=e.filter(e=>fs.statSync(path.join(o,e)).isDirectory())).map(e=>{const t=path.join(o,e,"workflow.json");let s;try{s=fs.readJsonSync(t)}catch(i){s={id_workflow:"-",description:"-",rev:"0.0"}}return s.id_workflow_instance=e,s.filename=t,s}),s?s(null,e):Promise.resolve(e)}catch(r){return s?s(r):Promise.reject(r)}}async datasets(e,t){if(!this.options.local)return super.datasets(e,t);let s,i;if(!e||e instanceof Function||void 0!==t?(s=e,i=t):i=e,i||(i={}),i.show||(i.show="mine"),"mine"!==i.show)return s(new Error("querying of local datasets unsupported in local mode"));const o=path.join(this.options.url,"datasets");try{let e=await fs.readdir(o);e=e.filter(e=>fs.statSync(path.join(o,e)).isDirectory());let t=0;return e=e.sort().map(e=>({is_reference_dataset:!0,summary:null,dataset_status:{status_label:"Active",status_value:"active"},size:0,prefix:e,id_workflow_instance:null,id_account:null,is_consented_human:null,data_fields:null,component_id:null,uuid:e,is_shared:!1,id_dataset:t+=1,id_user:null,last_modified:null,created:null,name:e,source:e,attributes:null})),s?s(null,e):Promise.resolve(e)}catch(r){return this.log.warn(r),s?s(null,[]):Promise.resolve([])}}async bundleWorkflow(e,t,s){return utils.pipe(`workflow/bundle/${e}.tar.gz`,t,this.options,s)}}class SessionManager{constructor(e,t,s,i){if(this.id_workflow_instance=e,this.children=s,this.options=lodash.merge(i),this.log=this.options.log,this.REST=t,!e)throw new Error("must specify id_workflow_instance");if(!s||!s.length)throw new Error("must specify children to session")}async session(){if(this.sts_expiration&&this.sts_expiration>Date.now())return Promise.resolve();this.log.debug("new instance token needed");try{const e=await this.REST.instanceToken(this.id_workflow_instance,this.options);this.log.debug(`allocated new instance token expiring at ${e.expiration}`),this.sts_expiration=new Date(e.expiration).getTime()-60*parseInt(this.options.sessionGrace||"0",10);const t={};this.options.proxy&&lodash.merge(t,{httpOptions:{agent:proxy(this.options.proxy,!0)}}),lodash.merge(t,{region:this.options.region},e),this.children.forEach(e=>{try{e.config.update(t)}catch(s){this.log.warn(`failed to update config on ${String(e)}: ${String(s)}`)}})}catch(e){this.log.warn(`failed to fetch instance token: ${String(e)}`)}return Promise.resolve()}}async function bytes(e){return fs.stat(e).then(e=>({type:"bytes",bytes:e.size}))}function fastq(e){return new Promise((t,s)=>{let i,o=1,r={size:0};try{r=fs.statSync(e)}catch(n){return void s(n)}fs.createReadStream(e).on("data",e=>{i=-1,o-=1;do{i=e.indexOf(10,i+1),o+=1}while(-1!==i)}).on("end",()=>t({type:"fastq",bytes:r.size,reads:Math.floor(o/4)})).on("error",s)})}function fasta(e){return new Promise((t,s)=>{let i,o=1,r={size:0};try{r=fs.statSync(e)}catch(n){s(n)}fs.createReadStream(e).on("data",e=>{i=-1,o-=1;do{i=e.indexOf(62,i+1),o+=1}while(-1!==i)}).on("end",()=>t({type:"fasta",bytes:r.size,sequences:Math.floor((1+o)/2)})).on("error",s)})}const mapping={fastq:fastq,fasta:fasta,default:bytes};function filestats(e){if("string"!==typeof e&&!(e instanceof String))return Promise.resolve({});let t=path.extname(e).toLowerCase().replace(/^[.]/,"");return"fq"===t?t="fastq":"fa"===t&&(t="fasta"),mapping[t]||(t="default"),mapping[t](e)}async function fastqCommon(e,t,s,i,o,r){const{maxChunkBytes:n,maxChunkReads:a}=lodash.merge({},t),l=path.dirname(e),c=path.basename(e),u=c.match(/^[^.]+/)[0],h=c.replace(u,""),p=path.join(l,u);if(!n&&!a)return s(e).then(()=>({source:e,split:!1,chunks:[e]}));const d=await fs.stat(e);return n&&d.size<n?s(e).then(()=>({source:e,split:!1,chunks:[e]})):new Promise(t=>{let l,c,u=0,d=0,g="",f=0,m=0;const w={source:e,split:!0,chunks:[]};let y;const k=[new Promise(e=>{y=e})];readline.createInterface({input:o(e)}).on("line",async e=>{g+=e,g+="\n",(d+=1)>=4&&(d=0,(async e=>{if(!f){l=`${p}_${u+=1}${h}`;const e=new Promise((e,t)=>{const o=l,n=()=>{s(o).then(()=>{e(o)}).catch(e=>{t(e)}).finally(()=>{fs.unlink(o).catch(e=>{i.warn(`Error unlinking chunk ${o}: ${String(e)}`)})})};r?c=r(o,n):(c=fs.createWriteStream(o)).on("close",n)});k.push(e)}f+=1,m+=e.length,c.write(e,()=>{}),(n&&m>=n||a&&f>=a)&&(f=0,m=0,c.end())})(g),g="")}).on("close",()=>{c.end(),y(),Promise.all(k).then(e=>{e.shift(),t(lodash.merge({chunks:e},w))})}).on("error",t=>{i.error(`Error chunking ${e}: ${String(t)}`)})})}async function fastqSplitter(e,t,s,i){return fastqCommon(e,t,s,i,e=>fs.createReadStream(e))}async function fastqGzipSplitter(e,t,s,i){return fastqCommon(e,t,s,i,e=>fs.createReadStream(e).pipe(zlib.createGunzip()),(e,t)=>{const s=fs.createWriteStream(e);s.on("close",t);const i=zlib.createGzip();return i.pipe(s),i})}const niceSize=(e,t)=>{const s=["","K","M","G","T","P","E","Z"];let i=t||0,o=e||0;return o>=1e3?(o/=1e3,(i+=1)>=s.length?"???":niceSize(o,i)):0===i?`${o}${s[i]}`:`${o.toFixed(1)}${s[i]}`},gqlUtils=function(){const e=(e,t)=>{e.headers||(e.headers={});let s=t;if(s||(s={}),!s.apikey||!s.apisecret)return;e.headers["X-EPI2ME-APIKEY"]=s.apikey,e.headers["X-EPI2ME-SIGNATUREDATE"]=(new Date).toISOString();const i=[Object.keys(e.headers).sort().filter(e=>e.match(/^x-epi2me/i)).map(t=>`${t}:${e.headers[t]}`).join("\n"),e.body].join("\n"),o=crypto.createHmac("sha1",s.apisecret).update(i).digest("hex");e.headers["X-EPI2ME-SIGNATUREV0"]=o};return{version:version,setHeaders:(t,s)=>{const{log:i}=lodash.merge({log:{debug:()=>{}}},s);let o=s;if(o||(o={}),t.headers=lodash.merge({Accept:"application/json","Content-Type":"application/json","X-EPI2ME-CLIENT":o.user_agent||"api","X-EPI2ME-VERSION":o.agent_version||gqlUtils.version},t.headers,o.headers),"signing"in o&&!o.signing||e(t,o),o.proxy){const e=o.proxy.match(/https?:\/\/((\S+):(\S+)@)?(\S+):(\d+)/),s=e[2],r=e[3],n={host:e[4],port:e[5]};s&&r&&(n.proxyAuth=`${s}:${r}`),o.proxy.match(/^https/)?(i.debug("using HTTPS over HTTPS proxy",JSON.stringify(n)),t.httpsAgent=tunnel.httpsOverHttps({proxy:n})):(i.debug("using HTTPS over HTTP proxy",JSON.stringify(n)),t.httpsAgent=tunnel.httpsOverHttp({proxy:n})),t.proxy=!1}}}}(),fetcher=axiosFetch.buildAxiosFetch(axios),customFetcher=(e,t)=>{const{apikey:s,apisecret:i}=t.headers.keys;return delete t.headers.keys,gqlUtils.setHeaders(t,{apikey:s,apisecret:i,signing:!0}),fetcher(e,t)},link=new apolloLink.ApolloLink(e=>{const{apikey:t,apisecret:s,url:i}=e.getContext(),o=apolloLinkHttp.createHttpLink({uri:`${i}/graphql`,fetch:customFetcher,headers:{keys:{apikey:t,apisecret:s}}});return apolloLink.execute(o,e)}),cache=new apolloCacheInmemory.InMemoryCache,client=new apolloClient.ApolloClient({link:link,cache:cache}),PageFragment="\npage\npages\nhasNext\nhasPrevious\ntotalCount\n",WorkflowFragment="\nidWorkflow\nname\ndescription\nsummary\n",WorkflowInstanceFragment="\nidWorkflowInstance\noutputqueue\nstartDate\n";class GraphQL{constructor(e){this.options=lodash.assign({agent_version:utils.version,local:local,url:url,user_agent:user_agent,signing:signing},e),this.options.url=this.options.url.replace(/:\/\//,"://graphql."),this.log=this.options.log,this.client=client}createContext(e){const{apikey:t,apisecret:s,url:i}=this.options;return lodash.merge({apikey:t,apisecret:s,url:i},e)}workflows(e={},t={}){const s=gql`
      query allWorkflows($page: Int) {
        allWorkflows(page: $page) {
          ${PageFragment}
          results {
            ${WorkflowFragment}
          }
        }
      }
    `,i=this.createContext(e);return this.client.query({query:s,variables:t,context:i})}workflow(e){const t=gql`
      query workflow($idWorkflow: ID!) {
        workflow(idWorkflow: $idWorkflow) {
          ${WorkflowFragment}
        }
      }
    `;return this.client.query({query:t,variables:e})}workflowInstances(e){const t=gql`
      query allWorkflowInstances($page: Int) {
        allWorkflowInstances(page: $page) {
          ${PageFragment}
          results {
            ${WorkflowInstanceFragment}
          }
        }
      }
    `;return this.client.query({query:t,variables:e})}workflowInstance(e){const t=gql`
      query workflowInstance($idWorkflowInstance: ID!) {
        workflowInstance(idWorkflowInstance: $idWorkflowInstance) {
          ${WorkflowInstanceFragment}
        }
      }
    `;return this.client.query({query:t,variables:e})}startWorkflow(e){const t=gql`
      mutation startWorkflow(
        $idWorkflow: ID!
        $computeAccountId: Int!
        $storageAccountId: Int
        $isConsentedHuman: Int = 0
      ) {
        startWorkflowInstance(
          idWorkflow: $idWorkflow
          computeAccountId: $computeAccountId
          storageAccountId: $storageAccountId
          isConsentedHuman: $isConsentedHuman
        ) {
          bucket
          idUser
          idWorkflowInstance
          inputqueue
          outputqueue
          region
          keyId
          chain
        }
      }
    `;return this.client.mutate({mutation:t,variables:e})}async register(e,t,s){let i,o;t&&t instanceof Function?o=t:(i=t,o=s);try{const t=await utils.post("apiaccess",{code:e,description:i||`${os__default.userInfo().username}@${os__default.hostname()}`},this.options);return o?o(null,t):Promise.resolve(t)}catch(r){return o?o(r):Promise.reject(r)}}}class Profile{constructor(e,t){this.allProfileData={},this.defaultEndpoint=process.env.METRICHOR||DEFAULTS.endpoint||DEFAULTS.url,this.raiseExceptions=t,e&&(this.allProfileData=lodash.merge(e,{profiles:{}})),this.allProfileData.endpoint&&(this.defaultEndpoint=this.allProfileData.endpoint)}profile(e){return e?lodash.merge({endpoint:this.defaultEndpoint},this.allProfileData.profiles[e]):{}}profiles(){return Object.keys(this.allProfileData.profiles||{})}}class Socket{constructor(e,t){this.debounces={},this.debounceWindow=lodash.merge({debounceWindow:2e3},t).debounceWindow,this.log=lodash.merge({log:{debug:()=>{}}},t).log,e.jwt().then(e=>{this.socket=io(t.url,{transportOptions:{polling:{extraHeaders:{Cookie:`x-epi2me-jwt=${e}`}}}}),this.socket.on("connect",()=>{this.log.debug("socket ready")})})}debounce(e,t){const s=lodash.merge(e)._uuid;if(s){if(this.debounces[s])return;this.debounces[s]=1,setTimeout(()=>{delete this.debounces[s]},this.debounceWindow)}t&&t(e)}watch(e,t){if(!this.socket)return this.log.debug(`socket not ready. requeueing watch on ${e}`),void setTimeout(()=>{this.watch(e,t)},1e3);this.socket.on(e,e=>this.debounce(e,t))}emit(e,t){if(!this.socket)return this.log.debug(`socket not ready. requeueing emit on ${e}`),void setTimeout(()=>{this.emit(e,t)},1e3);this.log.debug(`socket emit ${e} ${JSON.stringify(t)}`),this.socket.emit(e,t)}}class EPI2ME{constructor(e){let t;if((t="string"===typeof e||"object"===typeof e&&e.constructor===String?JSON.parse(e):e||{}).log){if(!lodash.every([t.log.info,t.log.warn,t.log.error,t.log.debug,t.log.json],lodash.isFunction))throw new Error("expected log object to have error, debug, info, warn and json methods");this.log=t.log}else this.log={info:e=>{console.info(`[${(new Date).toISOString()}] INFO: ${e}`)},debug:e=>{console.debug(`[${(new Date).toISOString()}] DEBUG: ${e}`)},warn:e=>{console.warn(`[${(new Date).toISOString()}] WARN: ${e}`)},error:e=>{console.error(`[${(new Date).toISOString()}] ERROR: ${e}`)},json:e=>{console.log(JSON.stringify(e))}};this.stopped=!0,this.states={upload:{filesCount:0,success:{files:0,bytes:0,reads:0},types:{},niceTypes:"",progress:{bytes:0,total:0}},download:{progress:{},success:{files:0,reads:0,bytes:0},fail:0,types:{},niceTypes:""},warnings:[]},this.config={options:lodash.defaults(t,DEFAULTS),instance:{id_workflow_instance:t.id_workflow_instance,inputQueueName:null,outputQueueName:null,outputQueueURL:null,discoverQueueCache:{},bucket:null,bucketFolder:null,remote_addr:null,chain:null,key_id:null}},this.config.instance.awssettings={region:this.config.options.region},this.REST=new REST(lodash.merge({log:this.log},this.config.options)),this.graphQL=new GraphQL(lodash.merge({log:this.log},this.config.options)),this.timers={downloadCheckInterval:null,stateCheckInterval:null,fileCheckInterval:null,transferTimeouts:{},visibilityIntervals:{},summaryTelemetryInterval:null}}async socket(){return this.mySocket?this.mySocket:(this.mySocket=new Socket(this.REST,lodash.merge({log:this.log},this.config.options)),this.mySocket)}async realtimeFeedback(e,t){(await this.socket()).emit(e,t)}async stopEverything(){this.stopped=!0,this.log.debug("stopping watchers"),["downloadCheckInterval","stateCheckInterval","fileCheckInterval","summaryTelemetryInterval"].forEach(e=>{this.timers[e]&&(this.log.debug(`clearing ${e} interval`),clearInterval(this.timers[e]),this.timers[e]=null)}),Object.keys(this.timers.transferTimeouts).forEach(e=>{this.log.debug(`clearing transferTimeout for ${e}`),clearTimeout(this.timers.transferTimeouts[e]),delete this.timers.transferTimeouts[e]}),Object.keys(this.timers.visibilityIntervals).forEach(e=>{this.log.debug(`clearing visibilityInterval for ${e}`),clearInterval(this.timers.visibilityIntervals[e]),delete this.timers.visibilityIntervals[e]}),this.downloadWorkerPool&&(this.log.debug("clearing downloadWorkerPool"),await Promise.all(Object.values(this.downloadWorkerPool)),this.downloadWorkerPool=null);const{id_workflow_instance:e}=this.config.instance;if(e){try{await this.REST.stopWorkflow(e)}catch(t){return this.log.error(`Error stopping instance: ${String(t)}`),Promise.reject(t)}this.log.info(`workflow instance ${e} stopped`)}return Promise.resolve()}reportProgress(){const{upload:e,download:t}=this.states;this.log.json({progress:{download:t,upload:e}})}storeState(e,t,s,i){const o=i||{};this.states[e]||(this.states[e]={}),this.states[e][t]||(this.states[e][t]={}),"incr"===s?Object.keys(o).forEach(s=>{this.states[e][t][s]=this.states[e][t][s]?this.states[e][t][s]+parseInt(o[s],10):parseInt(o[s],10)}):Object.keys(o).forEach(s=>{this.states[e][t][s]=this.states[e][t][s]?this.states[e][t][s]-parseInt(o[s],10):-parseInt(o[s],10)});try{this.states[e].success.niceReads=niceSize(this.states[e].success.reads)}catch(n){this.states[e].success.niceReads=0}try{this.states[e].progress.niceSize=niceSize(this.states[e].success.bytes+this.states[e].progress.bytes||0)}catch(n){this.states[e].progress.niceSize=0}try{this.states[e].success.niceSize=niceSize(this.states[e].success.bytes)}catch(n){this.states[e].success.niceSize=0}this.states[e].niceTypes=Object.keys(this.states[e].types||{}).sort().map(t=>`${this.states[e].types[t]} ${t}`).join(", ");const r=Date.now();(!this.stateReportTime||r-this.stateReportTime>2e3)&&(this.stateReportTime=r,this.reportProgress())}uploadState(e,t,s){return this.storeState("upload",e,t,s)}downloadState(e,t,s){return this.storeState("download",e,t,s)}url(){return this.config.options.url}apikey(){return this.config.options.apikey}attr(e,t){if(!(e in this.config.options))throw new Error(`config object does not contain property ${e}`);return t?(this.config.options[e]=t,this):this.config.options[e]}stats(e){return this.states[e]}}EPI2ME.version=utils.version,EPI2ME.Profile=Profile,EPI2ME.REST=REST,EPI2ME.utils=utils;class db{constructor(e,t,s){const i=lodash.merge({},t);this.options=i,this.log=s;const{idWorkflowInstance:o}=i;s.debug(`setting up ${e}/db.sqlite for ${o}`),this.db=fs.mkdirp(e).then(()=>(this.log.debug(`opening ${e}/db.sqlite`),sqlite.open(path.join(e,"db.sqlite"),{Promise:Promise}).then(async t=>{this.log.debug(`opened ${e}/db.sqlite`);try{return await Promise.all([t.run("CREATE TABLE IF NOT EXISTS meta (version CHAR(12) DEFAULT '' NOT NULL, idWorkflowInstance INTEGER UNSIGNED, inputFolder CHAR(255) default '')").then(()=>{t.run("INSERT INTO meta (version, idWorkflowInstance, inputFolder) VALUES(?, ?, ?)",pkg.version,o,i.inputFolder)}),t.run("CREATE TABLE IF NOT EXISTS uploads (filename CHAR(255) DEFAULT '' NOT NULL PRIMARY KEY)"),t.run("CREATE TABLE IF NOT EXISTS skips (filename CHAR(255) DEFAULT '' NOT NULL PRIMARY KEY)"),t.run("CREATE TABLE IF NOT EXISTS splits (filename CHAR(255) DEFAULT '' NOT NULL PRIMARY KEY, parent CHAR(255) DEFAULT '' NOT NULL, start DATETIME NOT NULL, end DATETIME)")]),Promise.resolve(t)}catch(s){return this.log.error(s),Promise.reject(s)}}))).catch(e=>{throw this.log.error(e),e})}async uploadFile(e){const t=await this.db,s=e.replace(new RegExp(`^${this.options.inputFolder}`),"");return t.run("INSERT INTO uploads VALUES(?)",s)}async skipFile(e){const t=await this.db,s=e.replace(new RegExp(`^${this.options.inputFolder}`),"");return t.run("INSERT INTO skips VALUES(?)",s)}async splitFile(e,t){const s=await this.db,i=e.replace(new RegExp(`^${this.options.inputFolder}`),""),o=t.replace(new RegExp(`^${this.options.inputFolder}`),"");return s.run("INSERT INTO splits VALUES(?, ?, CURRENT_TIMESTAMP, NULL)",i,o)}async splitDone(e){const t=await this.db,s=e.replace(new RegExp(`^${this.options.inputFolder}`),"");return t.run("UPDATE splits SET end=CURRENT_TIMESTAMP WHERE filename=?",s)}async splitClean(){return(await this.db).all("SELECT filename FROM splits WHERE end IS NULL").then(e=>{if(!e)return this.log.info("no split files to clean"),Promise.resolve();this.log.info(`cleaning ${e.length} split files`),this.log.debug(`going to clean: ${e.map(e=>e.filename).join(" ")}`);const t=e.map(e=>fs.unlink(path.join(this.options.inputFolder,e.filename)).catch(()=>{}));return Promise.all(t)})}async seenUpload(e){const t=await this.db,s=e.replace(new RegExp(`^${this.options.inputFolder}`),"");return Promise.all([t.get("SELECT * FROM uploads u WHERE u.filename=? LIMIT 1",s),t.get("SELECT * FROM skips s WHERE s.filename=? LIMIT 1",s)]).then(e=>lodash.remove(e,void 0).length)}}class Profile_FS extends Profile{constructor(e,t){super({},t),this.prefsFile=e||Profile_FS.profilePath(),this.allProfileData={};try{this.allProfileData=lodash.merge(fs.readJSONSync(this.prefsFile),{profiles:{}}),this.allProfileData.endpoint&&(this.defaultEndpoint=this.allProfileData.endpoint)}catch(s){if(this.raiseExceptions)throw s}}static profilePath(){return path.join(os.homedir(),".epi2me.json")}profile(e,t){if(e&&t){lodash.merge(this.allProfileData,{profiles:{[e]:t}});try{fs.writeJSONSync(this.prefsFile,this.allProfileData)}catch(s){if(this.raiseExceptions)throw s}}return e?lodash.merge({endpoint:this.defaultEndpoint},this.allProfileData.profiles[e]):{}}}class PromisePipeline{static MakeQueryablePromise(e){if(e.isResolved)return e;let t=!0,s=!1,i=!1;const o=e.then(e=>(i=!0,t=!1,e)).catch(e=>{throw s=!0,t=!1,e});return o.dependsOn=e,o.isResolved=()=>i,o.isPending=()=>t,o.isRejected=()=>s,o}constructor(e){const t=lodash.merge({bandwidth:1,interval:500},e);this.bandwidth=t.bandwidth,this.interval=t.interval,this.pipeline=[],this.running=[],this.completed=0,this.intervalId=null,"start"in t&&!t.start||this.start()}enqueue(e){this.pipeline.push(e)}start(){this.intervalId||(this.intervalId=setInterval(()=>{this.monitorInterval()},this.interval))}stop(){clearInterval(this.intervalId),delete this.intervalId}state(){return{queued:this.pipeline.length,running:this.running.length,completed:this.completed,state:this.intervalId?"running":"stopped"}}monitorInterval(){this.running.map((e,t)=>e.isPending()?null:t).filter(e=>e).reverse().forEach(e=>{this.running.splice(e,1),this.completed+=1});const e=this.bandwidth-this.running.length;for(let t=0;t<e;t+=1){const e=this.pipeline.shift();if(!e)return;this.running.push(PromisePipeline.MakeQueryablePromise(e()))}}}const rootDir=()=>{const e=process.env.APPDATA||("darwin"===process.platform?path.join(os.homedir(),"Library/Application Support"):os.homedir());return process.env.EPI2ME_HOME||path.join(e,"linux"===process.platform?".epi2me":"EPI2ME")};class EPI2ME_FS extends EPI2ME{constructor(e){super(e),this.REST=new REST_FS(lodash.merge({},{log:this.log},this.config.options))}async sessionedS3(){return await this.sessionManager.session(),new AWS.S3({useAccelerateEndpoint:"on"===this.config.options.awsAcceleration})}async sessionedSQS(){return await this.sessionManager.session(),new AWS.SQS}async deleteMessage(e){try{const t=await this.discoverQueue(this.config.instance.outputQueueName);return(await this.sessionedSQS()).deleteMessage({QueueUrl:t,ReceiptHandle:e.ReceiptHandle}).promise()}catch(t){return this.log.error(`deleteMessage exception: ${String(t)}`),this.states.download.failure||(this.states.download.failure={}),this.states.download.failure[t]=this.states.download.failure[t]?this.states.download.failure[t]+1:1,Promise.reject(t)}}async discoverQueue(e){if(this.config.instance.discoverQueueCache[e])return Promise.resolve(this.config.instance.discoverQueueCache[e]);let t;this.log.debug(`discovering queue for ${e}`);try{const s=await this.sessionedSQS();t=await s.getQueueUrl({QueueName:e}).promise()}catch(s){return this.log.error(`Error: failed to find queue for ${e}: ${String(s)}`),Promise.reject(s)}return this.log.debug(`found queue ${t.QueueUrl}`),this.config.instance.discoverQueueCache[e]=t.QueueUrl,Promise.resolve(t.QueueUrl)}async queueLength(e){if(!e)return Promise.reject(new Error("no queueURL specified"));const t=e.match(/([\w\-_]+)$/)[0];this.log.debug(`querying queue length of ${t}`);try{const t=await this.sessionedSQS(),s=await t.getQueueAttributes({QueueUrl:e,AttributeNames:["ApproximateNumberOfMessages"]}).promise();if(s&&s.Attributes&&"ApproximateNumberOfMessages"in s.Attributes){let e=s.Attributes.ApproximateNumberOfMessages;return e=parseInt(e,10)||0,Promise.resolve(e)}return Promise.reject(new Error("unexpected response"))}catch(s){return this.log.error(`error in getQueueAttributes ${String(s)}`),Promise.reject(s)}}async autoStart(e,t){let s;this.stopped=!1;try{s=await this.REST.startWorkflow(e)}catch(i){const e=`Failed to start workflow: ${String(i)}`;return this.log.warn(e),t?t(e):Promise.reject(i)}return this.config.workflow=JSON.parse(JSON.stringify(e)),this.log.info(`instance ${JSON.stringify(s)}`),this.log.info(`workflow config ${JSON.stringify(this.config.workflow)}`),this.autoConfigure(s,t)}async autoJoin(e,t){let s;this.stopped=!1,this.config.instance.id_workflow_instance=e;try{s=await this.REST.workflowInstance(e)}catch(i){const e=`Failed to join workflow instance: ${String(i)}`;return this.log.warn(e),t?t(e):Promise.reject(i)}return"stopped"===s.state?(this.log.warn(`workflow ${e} is already stopped`),t?t("could not join workflow"):Promise.reject(new Error("could not join workflow"))):(this.config.workflow=this.config.workflow||{},this.log.debug(`instance ${JSON.stringify(s)}`),this.log.debug(`workflow config ${JSON.stringify(this.config.workflow)}`),this.autoConfigure(s,t))}initSessionManager(e,t){return new SessionManager(this.config.instance.id_workflow_instance,this.REST,[AWS,...t||[]],lodash.merge({sessionGrace:this.config.options.sessionGrace,proxy:this.config.options.proxy,region:this.config.instance.region,log:this.log},e))}async autoConfigure(e,t){if(["id_workflow_instance","id_workflow","remote_addr","key_id","bucket","user_defined","start_date","id_user"].forEach(t=>{this.config.instance[t]=e[t]}),this.config.instance.inputQueueName=e.inputqueue,this.config.instance.outputQueueName=e.outputqueue,this.config.instance.region=e.region||this.config.options.region,this.config.instance.bucketFolder=`${e.outputqueue}/${e.id_user}/${e.id_workflow_instance}`,this.config.instance.summaryTelemetry=e.telemetry,e.chain)if("object"===typeof e.chain)this.config.instance.chain=e.chain;else try{this.config.instance.chain=JSON.parse(e.chain)}catch(a){throw new Error(`exception parsing chain JSON ${String(a)}`)}if(!this.config.options.inputFolder)throw new Error("must set inputFolder");if(!this.config.options.outputFolder)throw new Error("must set outputFolder");if(!this.config.instance.bucketFolder)throw new Error("bucketFolder must be set");if(!this.config.instance.inputQueueName)throw new Error("inputQueueName must be set");if(!this.config.instance.outputQueueName)throw new Error("outputQueueName must be set");fs.mkdirpSync(this.config.options.outputFolder);const s=path.join(rootDir(),"instances"),i=path.join(s,this.config.instance.id_workflow_instance);this.db=new db(i,{idWorkflowInstance:this.config.instance.id_workflow_instance,inputFolder:this.config.options.inputFolder},this.log);const o=this.config.instance.id_workflow_instance?`telemetry-${this.config.instance.id_workflow_instance}.log`:"telemetry.log",r=path.join(this.config.options.outputFolder,"epi2me-logs"),n=path.join(r,o);return fs.mkdirp(r,e=>{if(e&&!String(e).match(/EEXIST/))this.log.error(`error opening telemetry log stream: mkdirpException:${String(e)}`);else try{this.telemetryLogStream=fs.createWriteStream(n,{flags:"a"}),this.log.info(`logging telemetry to ${n}`)}catch(t){this.log.error(`error opening telemetry log stream: ${String(t)}`)}}),t&&t(null,this.config.instance),this.timers.summaryTelemetryInterval=setInterval(()=>{this.stopped?clearInterval(this.timers.summaryTelemetryInterval):this.fetchTelemetry()},1e4*this.config.options.downloadCheckInterval),this.timers.downloadCheckInterval=setInterval(()=>{this.stopped?clearInterval(this.timers.downloadCheckInterval):this.checkForDownloads()},1e3*this.config.options.downloadCheckInterval),this.timers.stateCheckInterval=setInterval(async()=>{if(this.stopped)clearInterval(this.timers.stateCheckInterval);else try{const t=await this.REST.workflowInstance(this.config.instance.id_workflow_instance);if("stopped"===t.state){this.log.warn(`instance was stopped remotely at ${t.stop_date}. shutting down the workflow.`);try{const e=await this.stopEverything();"function"===typeof e.config.options.remoteShutdownCb&&e.config.options.remoteShutdownCb(`instance was stopped remotely at ${t.stop_date}`)}catch(e){this.log.error(`Error whilst stopping: ${String(e)}`)}}}catch(t){this.log.warn(`failed to check instance state: ${t&&t.error?t.error:t}`)}},1e3*this.config.options.stateCheckInterval),this.sessionManager=this.initSessionManager(),await this.sessionManager.session(),this.reportProgress(),this.loadUploadFiles(),this.timers.fileCheckInterval=setInterval(this.loadUploadFiles.bind(this),1e3*this.config.options.fileCheckInterval),Promise.resolve(e)}async stopEverything(){return await super.stopEverything(),delete this.sessionManager,this.log.debug("clearing split files"),this.db?this.db.splitClean():Promise.resolve()}async checkForDownloads(){if(this.checkForDownloadsRunning)return Promise.resolve();this.checkForDownloadsRunning=!0,this.log.debug("checkForDownloads checking for downloads");try{const e=await this.discoverQueue(this.config.instance.outputQueueName),t=await this.queueLength(e);t?(this.log.debug(`downloads available: ${t}`),await this.downloadAvailable()):this.log.debug("no downloads available")}catch(e){this.log.warn(`checkForDownloads error ${String(e)}`),this.states.download.failure||(this.states.download.failure={}),this.states.download.failure[e]=this.states.download.failure[e]?this.states.download.failure[e]+1:1}return this.checkForDownloadsRunning=!1,Promise.resolve()}async downloadAvailable(){const e=Object.keys(this.downloadWorkerPool||{}).length;if(e>=this.config.options.transferPoolSize)return this.log.debug(`${e} downloads already queued`),Promise.resolve();let t;try{const s=await this.discoverQueue(this.config.instance.outputQueueName);this.log.debug("fetching messages");const i=await this.sessionedSQS();t=await i.receiveMessage({AttributeNames:["All"],QueueUrl:s,VisibilityTimeout:this.config.options.inFlightDelay,MaxNumberOfMessages:this.config.options.transferPoolSize-e,WaitTimeSeconds:this.config.options.waitTimeSeconds}).promise()}catch(s){return this.log.error(`receiveMessage exception: ${String(s)}`),this.states.download.failure[s]=this.states.download.failure[s]?this.states.download.failure[s]+1:1,Promise.reject(s)}return this.receiveMessages(t)}async loadUploadFiles(){if(this.dirScanInProgress)return Promise.resolve();this.dirScanInProgress=!0,this.log.debug("upload: started directory scan");try{const e=e=>this.db.seenUpload(e),t=await utils.loadInputFiles(this.config.options,this.log,e);let s=0;const i=()=>new Promise(e=>{if(this.stopped)return t.length=0,this.log.debug("upload: skipping, stopped"),void e();if(s>this.config.options.transferPoolSize)return void setTimeout(e,1e3);const i=t.splice(0,this.config.options.transferPoolSize-s);s+=i.length,this.enqueueUploadFiles(i).then().catch(e=>{this.log.error(`upload: exception in enqueueUploadFiles: ${String(e)}`)}).finally(()=>{s-=i.length,e()})});for(;t.length;)await i()}catch(e){this.log.error(`upload: exception in loadInputFiles: ${String(e)}`)}return this.dirScanInProgress=!1,this.log.debug("upload: finished directory scan"),Promise.resolve()}async enqueueUploadFiles(e){let t=0,s=0,i=0,o=0,r={};if(!lodash.isArray(e)||!e.length)return Promise.resolve();if(this.log.info(`enqueueUploadFiles ${e.length} files: ${e.map(e=>e.path).join(" ")}.`),"workflow"in this.config)if("workflow_attributes"in this.config.workflow)r=this.config.workflow.workflow_attributes;else if("attributes"in this.config.workflow){let{attributes:e}=this.config.workflow;if(e||(e={}),["max_size","max_files","split_size","split_reads"].forEach(t=>{`epi2me:${t}`in e&&(r[t]=parseInt(e[`epi2me:${t}`],10))}),"epi2me:category"in e){e["epi2me:category"].includes("storage")&&(r.requires_storage=!0)}}if(this.log.info(`enqueueUploadFiles settings ${JSON.stringify(r)}`),"requires_storage"in r&&r.requires_storage&&!("storage_account"in this.config.workflow)){const e={msg:"ERROR: Workflow requires storage enabled. Please provide a valid storage account [ --storage ].",type:"WARNING_STORAGE_ENABLED"};return this.log.error(e.msg),this.states.warnings.push(e),Promise.resolve()}if("split_size"in r&&(i=parseInt(r.split_size,10),this.log.info(`enqueueUploadFiles splitting supported files at ${i} bytes`)),"split_reads"in r&&(o=parseInt(r.split_reads,10),this.log.info(`enqueueUploadFiles splitting supported files at ${o} reads`)),"max_size"in r&&(s=parseInt(r.max_size,10),this.log.info(`enqueueUploadFiles restricting file size to ${s}`)),"max_files"in r&&(t=parseInt(r.max_files,10),this.log.info(`enqueueUploadFiles restricting file count to ${t}`),e.length>t)){const s={msg:`ERROR: ${e.length} files found. Workflow can only accept ${t}. Please move the extra files away.`,type:"WARNING_FILE_TOO_MANY"};return this.log.error(s.msg),this.states.warnings.push(s),Promise.resolve()}this.states.upload.filesCount+=e.length;const n=e.map(async e=>{const r=e;if(t&&this.states.upload.filesCount>t){const e=`Maximum ${t} file(s) already uploaded. Marking ${r.relative} as skipped.`,s={msg:e,type:"WARNING_FILE_TOO_MANY"};this.log.error(e),this.states.warnings.push(s),this.states.upload.filesCount-=1,r.skip="SKIP_TOO_MANY"}else if(0===r.size){const e=`The file "${r.relative}" is empty. It will be skipped.`,t={msg:e,type:"WARNING_FILE_EMPTY"};r.skip="SKIP_EMPTY",this.states.upload.filesCount-=1,this.log.error(e),this.states.warnings.push(t)}else{if(r.path&&r.path.match(/\.(?:fastq|fq)(?:\.gz)?$/)&&(i&&r.size>i||o)){const e=`${r.relative}${r.size>i?" is too big and":""} is going to be split`;this.log.warn(e);const t={msg:e,type:"WARNING_FILE_SPLIT"};this.states.warnings.push(t);const s=i?{maxChunkBytes:i}:{maxChunkReads:o},a=r.path.match(/\.gz$/)?fastqGzipSplitter:fastqSplitter,l=utils.getFileID(),c=new PromisePipeline({bandwidth:this.config.options.transferPoolSize});let u=0;const h=async e=>(this.log.debug(`chunkHandler for ${e}`),await this.db.splitFile(e,r.path),this.stopped?(c.stop(),this.log.info(`stopped, so skipping ${e}`),Promise.reject(new Error("stopped"))):(u+=1,filestats(e).then(t=>({name:path.basename(e),path:e,relative:e.replace(this.config.options.inputFolder,""),id:`${l}_${u}`,stats:t,size:t.bytes})).then(async e=>{const t=new Promise(t=>{c.enqueue(()=>(this.log.info(`chunk upload starting ${e.id} ${e.path}`),this.stopped?(this.log.info(`chunk upload skipped (stopped) ${e.id} ${e.path}`),c.stop(),t(),Promise.resolve()):this.uploadJob(e).then(()=>this.db.splitDone(e.path)).catch(t=>{this.log.error(`chunk upload failed ${e.id} ${e.path}: ${String(t)}`)}).finally(t)))});await t})));try{await a(r.path,s,h,this.log),c.stop()}catch(n){if(c.stop(),"Error: stopped"===String(n))return Promise.resolve();throw n}return this.db.uploadFile(r.path)}if(s&&r.size>s){const e=`The file "${r.relative}" is bigger than the maximum size limit (${niceSize(s)}B). It will be skipped.`,t={msg:e,type:"WARNING_FILE_TOO_BIG"};r.skip="SKIP_TOO_BIG",this.states.upload.filesCount-=1,this.log.error(e),this.states.warnings.push(t)}else try{r.stats=await filestats(r.path)}catch(a){this.log.error(`failed to stat ${r.path}: ${String(a)}`)}}return this.uploadJob(r)});try{return await Promise.all(n),this.log.info(`upload: inputBatchQueue (${n.length} jobs) complete`),this.loadUploadFiles()}catch(a){return this.log.error(`upload: enqueueUploadFiles exception ${String(a)}`),Promise.reject(a)}}async uploadJob(e){if("skip"in e)return this.db.skipFile(e.path);let t,s;try{this.log.info(`upload: ${e.id} starting`),t=await this.uploadHandler(e),this.log.info(`upload: ${t.id} uploaded and notified`)}catch(i){s=i,this.log.error(`upload: ${e.id} done, but failed: ${String(s)}`)}if(t||(t={}),s)this.log.error(`uploadJob ${s}`),this.states.upload.failure||(this.states.upload.failure={}),this.states.upload.failure[s]=this.states.upload.failure[s]?this.states.upload.failure[s]+1:1;else if(this.uploadState("success","incr",lodash.merge({files:1},t.stats)),t.name){const e=path.extname(t.name);this.uploadState("types","incr",{[e]:1})}return Promise.resolve()}async receiveMessages(e){return e&&e.Messages&&e.Messages.length?(this.downloadWorkerPool||(this.downloadWorkerPool={}),e.Messages.forEach(e=>{this.downloadWorkerPool[e.MessageId]=1;const t=setTimeout(()=>{throw this.log.error(`this.downloadWorkerPool timeoutHandle. Clearing queue slot for message: ${e.MessageId}`),new Error("download timed out")},1e3*(60+this.config.options.downloadTimeout));this.processMessage(e).catch(e=>{this.log.error(`processMessage ${String(e)}`)}).finally(()=>{clearTimeout(t),e&&delete this.downloadWorkerPool[e.MessageId]})}),this.log.info(`downloader queued ${e.Messages.length} messages for processing`),Promise.resolve()):(this.log.info("complete (empty)"),Promise.resolve())}async processMessage(e){let t,s;if(!e)return this.log.debug("download.processMessage: empty message"),Promise.resolve();"Attributes"in e&&"ApproximateReceiveCount"in e.Attributes&&this.log.debug(`download.processMessage: ${e.MessageId} / ${e.Attributes.ApproximateReceiveCount}`);try{t=JSON.parse(e.Body)}catch(n){this.log.error(`error parsing JSON message.Body from message: ${JSON.stringify(e)} ${String(n)}`);try{await this.deleteMessage(e)}catch(a){this.log.error(`Exception deleting message: ${String(a)}`)}return Promise.resolve()}if(t.telemetry){const{telemetry:s}=t;if(s.tm_path)try{this.log.debug(`download.processMessage: ${e.MessageId} fetching telemetry`);const i=await this.sessionedS3(),o=await i.getObject({Bucket:t.bucket,Key:s.tm_path}).promise();this.log.info(`download.processMessage: ${e.MessageId} fetched telemetry`),s.batch=o.Body.toString("utf-8").split("\n").filter(e=>e&&e.length>0).map(e=>{try{return JSON.parse(e)}catch(a){return this.log.error(`Telemetry Batch JSON Parse error: ${String(a)}`),e}})}catch(l){this.log.error(`Could not fetch telemetry JSON: ${String(l)}`)}try{this.telemetryLogStream.write(JSON.stringify(s)+os.EOL)}catch(c){this.log.error(`error writing telemetry: ${c}`)}this.config.options.telemetryCb&&this.config.options.telemetryCb(s)}if(!t.path)return this.log.warn("nothing to download"),Promise.resolve();const i=t.path.match(/[\w\W]*\/([\w\W]*?)$/),o=i?i[1]:"";if(s=this.config.options.outputFolder,t.telemetry&&t.telemetry.hints&&t.telemetry.hints.folder){this.log.debug(`using folder hint ${t.telemetry.hints.folder}`);const e=t.telemetry.hints.folder.split("/").map(e=>e.toUpperCase());s=path.join.apply(null,[s,...e])}fs.mkdirpSync(s);const r=path.join(s,o);if("data+telemetry"===this.config.options.downloadMode){const s=[""];let i=this.config&&this.config.workflow&&this.config.workflow.settings&&this.config.workflow.settings.output_format?this.config.workflow.settings.output_format:[];("string"===typeof i||i instanceof String)&&(i=i.trim().split(/[\s,]+/));try{s.push(...i)}catch(a){this.log.error(`Failed to work out workflow file suffixes: ${String(a)}`)}try{const i=s.map(s=>{const i=t.path+s,o=r+s;return this.log.debug(`download.processMessage: ${e.MessageId} downloading ${i} to ${o}`),new Promise((r,n)=>{this.initiateDownloadStream({bucket:t.bucket,path:i},e,o).then(r).catch(e=>{this.log.error(`Caught exception waiting for initiateDownloadStream: ${String(e)}`),s?n(e):r()})})});await Promise.all(i)}catch(a){this.log.error(`Exception fetching file batch: ${String(a)}`)}try{const e=!(!t.telemetry||!t.telemetry.json)&&t.telemetry.json.exit_status;e&&this.config.options.dataCb&&this.config.options.dataCb(r,e)}catch(l){this.log.warn(`failed to fire data callback: ${l}`)}}else{const e=t.telemetry.batch_summary&&t.telemetry.batch_summary.reads_num?t.telemetry.batch_summary.reads_num:1;this.downloadState("success","incr",{files:1,reads:e})}try{await this.deleteMessage(e)}catch(a){this.log.error(`Exception deleting message: ${String(a)}`)}return this.realtimeFeedback("workflow_instance:state",{type:"stop",id_workflow_instance:this.config.instance.id_workflow_instance,id_workflow:this.config.instance.id_workflow,component_id:"0",message_id:lodash.merge(e).MessageId,id_user:this.config.instance.id_user}).catch(e=>{this.log.warn(`realtimeFeedback failed: ${String(e)}`)}),Promise.resolve()}async initiateDownloadStream(e,t,s){return new Promise(async(i,o)=>{let r,n,a;try{r=await this.sessionedS3()}catch(u){o(u)}const l=t=>{if(this.log.error(`Error during stream of bucket=${e.bucket} path=${e.path} to file=${s} ${String(t)}`),clearTimeout(this.timers.transferTimeouts[s]),delete this.timers.transferTimeouts[s],!n.networkStreamError)try{n.networkStreamError=1,n.close(),fs.remove(s).then(()=>{this.log.warn(`removed failed download ${s}`)}).catch(e=>{this.log.warn(`failed to remove ${s}. unlinkException: ${String(e)}`)}),a.destroy&&(this.log.error(`destroying read stream for ${s}`),a.destroy())}catch(u){this.log.error(`error handling stream error: ${String(u)}`)}};try{const t={Bucket:e.bucket,Key:e.path};n=fs.createWriteStream(s);const i=r.getObject(t);i.on("httpHeaders",(e,t)=>{this.downloadState("progress","incr",{total:parseInt(t["content-length"],10)})}),a=i.createReadStream()}catch(h){return this.log.error(`getObject/createReadStream exception: ${String(h)}`),void o(h)}a.on("error",l),n.on("finish",async()=>{if(!n.networkStreamError){this.log.debug(`downloaded ${s}`);try{const e=path.extname(s),t=await filestats(s);this.downloadState("success","incr",lodash.merge({files:1},t)),this.downloadState("types","incr",{[e]:1}),this.downloadState("progress","decr",{total:t.bytes,bytes:t.bytes})}catch(e){this.log.warn(`failed to stat ${s}: ${String(e)}`)}this.reportProgress()}}),n.on("close",o=>{this.log.debug(`closing writeStream ${s}`),o&&this.log.error(`error closing write stream ${o}`),clearInterval(this.timers.visibilityIntervals[s]),delete this.timers.visibilityIntervals[s],clearTimeout(this.timers.transferTimeouts[s]),delete this.timers.transferTimeouts[s],setTimeout(this.checkForDownloads.bind(this)),this.log.info(`download.initiateDownloadStream: ${t.MessageId} downloaded ${e.path} to ${s}`),i()}),n.on("error",l);const c=()=>{l(new Error("transfer timed out"))};this.timers.transferTimeouts[s]=setTimeout(c,1e3*this.config.options.downloadTimeout);this.timers.visibilityIntervals[s]=setInterval(async()=>{this.stopped&&(clearInterval(this.timers.visibilityIntervals[s]),delete this.timers.visibilityIntervals[s]);const e=this.config.instance.outputQueueURL,i=t.ReceiptHandle;this.log.debug({message_id:t.MessageId},"updateVisibility");try{await this.sqs.changeMessageVisibility({QueueUrl:e,ReceiptHandle:i,VisibilityTimeout:this.config.options.inFlightDelay}).promise()}catch(o){this.log.error({message_id:t.MessageId,queue:e,error:o},"Error setting visibility"),clearInterval(this.timers.visibilityIntervals[s])}},900*this.config.options.inFlightDelay),a.on("data",e=>{clearTimeout(this.timers.transferTimeouts[s]),this.timers.transferTimeouts[s]=setTimeout(c,1e3*this.config.options.downloadTimeout),this.downloadState("progress","incr",{bytes:e.length})}).pipe(n)})}async uploadHandler(e){const t=await this.sessionedS3();let s;const i=e.relative.replace(/^[\\/]+/,"").replace(/\\/g,"/").replace(/\//g,"_"),o=[this.config.instance.bucketFolder,"component-0",i,i].join("/").replace(/\/+/g,"/");let r;return new Promise((i,n)=>{const a=()=>{s&&!s.closed&&s.close(),n(new Error(`${e.name} timed out`))};r=setTimeout(a,1e3*(this.config.options.uploadTimeout+5));try{s=fs.createReadStream(e.path)}catch(l){return clearTimeout(r),void n(l)}s.on("error",e=>{s.close();let t="error in upload readstream";e&&e.message&&(t+=`: ${e.message}`),clearTimeout(r),n(new Error(t))}),s.on("open",()=>{const l={Bucket:this.config.instance.bucket,Key:o,Body:s};this.config.instance.key_id&&(l.SSEKMSKeyId=this.config.instance.key_id,l.ServerSideEncryption="aws:kms"),e.size&&(l["Content-Length"]=e.size),this.uploadState("progress","incr",{total:e.size});let c=0;const u=t.upload(l,{partSize:10485760,queueSize:1}),h=this.initSessionManager(null,[u.service]);h.sts_expiration=this.sessionManager.sts_expiration,u.on("httpUploadProgress",async e=>{if(this.stopped)n(new Error("stopped"));else{this.uploadState("progress","incr",{bytes:e.loaded-c}),c=e.loaded,clearTimeout(r),r=setTimeout(a,1e3*(this.config.options.uploadTimeout+5));try{await h.session()}catch(t){this.log.warn(`Error refreshing token: ${String(t)}`)}}}),u.promise().then(()=>{this.log.info(`${e.id} S3 upload complete`),s.close(),clearTimeout(r),this.uploadComplete(o,e).then(()=>{i(e)}).catch(e=>{n(e)}).finally(()=>{this.uploadState("progress","decr",{total:e.size,bytes:e.size})})}).catch(t=>{this.log.warn(`${e.id} uploadStreamError ${t}`),n(t)})})})}async uploadComplete(e,t){this.log.info(`${t.id} uploaded to S3: ${e}`);const s={bucket:this.config.instance.bucket,outputQueue:this.config.instance.outputQueueName,remote_addr:this.config.instance.remote_addr,user_defined:this.config.instance.user_defined||null,apikey:this.config.options.apikey,id_workflow_instance:this.config.instance.id_workflow_instance,id_master:this.config.instance.id_workflow,utc:(new Date).toISOString(),path:e,prefix:e.substring(0,e.lastIndexOf("/"))};if(this.config.instance.chain)try{s.components=JSON.parse(JSON.stringify(this.config.instance.chain.components)),s.targetComponentId=this.config.instance.chain.targetComponentId}catch(o){return this.log.error(`${t.id} exception parsing components JSON ${String(o)}`),Promise.reject(o)}if(this.config.instance.key_id&&(s.key_id=this.config.instance.key_id),this.config.options.agent_address)try{s.agent_address=JSON.parse(this.config.options.agent_address)}catch(r){this.log.error(`${t.id} Could not parse agent_address ${String(r)}`)}s.components&&Object.keys(s.components).forEach(e=>{"uploadMessageQueue"===s.components[e].inputQueueName&&(s.components[e].inputQueueName=this.uploadMessageQueue),"downloadMessageQueue"===s.components[e].inputQueueName&&(s.components[e].inputQueueName=this.downloadMessageQueue)});let i={};try{const e=await this.discoverQueue(this.config.instance.inputQueueName),o=await this.sessionedSQS();this.log.info(`${t.id} sending SQS message to input queue`),i=await o.sendMessage({QueueUrl:e,MessageBody:JSON.stringify(s)}).promise()}catch(n){return this.log.error(`${t.id} exception sending SQS message: ${String(n)}`),Promise.reject(n)}return this.realtimeFeedback("workflow_instance:state",{type:"start",id_workflow_instance:this.config.instance.id_workflow_instance,id_workflow:this.config.instance.id_workflow,component_id:"0",message_id:lodash.merge(i).MessageId,id_user:this.config.instance.id_user}).catch(e=>{this.log.warn(`realtimeFeedback failed: ${String(e)}`)}),this.log.info(`${t.id} SQS message sent. Mark as uploaded`),this.db.uploadFile(t.path)}async fetchTelemetry(){if(!this.config||!this.config.instance||!this.config.instance.summaryTelemetry)return Promise.resolve();const e=path.join(rootDir(),"instances"),t=path.join(e,this.config.instance.id_workflow_instance),s=[];Object.keys(this.config.instance.summaryTelemetry).forEach(e=>{const i=this.config.instance.summaryTelemetry[e]||{},o=i[Object.keys(i)[0]];if(!o)return;const r=path.join(t,`${e}.json`);s.push(this.REST.fetchContent(o).then(e=>{fs.writeJSONSync(r,e),this.log.debug(`fetched telemetry summary ${r}`)}).catch(e=>{this.log.debug(`Error fetching telemetry: ${String(e)}`)}))});let i=0;try{await Promise.all(s)}catch(o){i+=1}return i&&this.log.warn("summary telemetry incomplete"),Promise.resolve()}}EPI2ME_FS.version=utils.version,EPI2ME_FS.REST=REST_FS,EPI2ME_FS.utils=utils,EPI2ME_FS.SessionManager=SessionManager,EPI2ME_FS.EPI2ME_HOME=rootDir(),EPI2ME_FS.Profile=Profile_FS,module.exports=EPI2ME_FS;
