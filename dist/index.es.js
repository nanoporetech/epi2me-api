/**
 * Copyright Metrichor Ltd. (An Oxford Nanopore Technologies Company) 2019
 */

import{merge as e,flatten as t,remove as s,assign as i,filter as o,every as r,isFunction as n,defaults as a,isArray as l}from"lodash";import c from"fs-extra";import u,{homedir as h,EOL as d}from"os";import p from"path";import g from"axios";import f from"crypto";import{httpsOverHttps as m,httpsOverHttp as w}from"tunnel";import y from"readline";import k from"zlib";import b from"aws-sdk";import S from"proxy-agent";import v from"socket.io-client";import P from"sqlite";var $={name:"@metrichor/epi2me-api",version:"3.0.1653",license:"MPL-2.0",repository:"https://git.oxfordnanolabs.local/metrichor/api.git",description:"API for communicating with the EPI2ME website(s)",main:"dist/index.js",module:"dist/index.es.js",dependencies:{"@lifeomic/axios-fetch":"^1.4.1","@types/axios":"^0.14.0","@types/socket.io-client":"^1.4.32","apollo-cache-inmemory":"^1.6.3","apollo-client":"^2.6.4","apollo-link":"^1.2.13","apollo-link-context":"^1.0.19","apollo-link-http":"^1.5.16","aws-sdk":"^2.574.0",axios:"^0.19.0","core-js":"^3.2.1","fs-extra":"^8.1.0",graphql:"^14.5.8","graphql-tag":"^2.10.1",lodash:"4.17.15","proxy-agent":"^3.1.1",save:"^2.4.0","socket.io-client":"^2.3.0",sqlite:"^3.0.3",tunnel:"^0.0.6"},devDependencies:{"@babel/cli":"^7.6.2","@babel/core":"^7.6.2","@babel/plugin-proposal-object-rest-spread":"^7.6.2","@babel/preset-env":"^7.6.2","@babel/register":"^7.6.2","@types/bunyan":"^1.8.6","@types/rollup":"^0.54.0","@types/rollup-plugin-json":"^3.0.2","babel-eslint":"^10.0.3",bunyan:"^1.8.12",eslint:"6.6.0","eslint-config-airbnb-base":"^14.0.0","eslint-config-defaults":"9.0.0","eslint-config-prettier":"^6.7.0","eslint-plugin-babel":"^5.3.0","eslint-plugin-import":"^2.18.2","eslint-plugin-prettier":"^3.1.1",husky:"^3.0.8","lint-staged":"^9.4.2",mocha:"6.2.2",nyc:"^14.1.1",prettier:"^1.18.2","prettier-eslint":"^9.0.0",rollup:"^1.27.2","rollup-plugin-analyzer":"^3.2.1","rollup-plugin-cpy":"^2.0.1","rollup-plugin-eslint":"^7.0.0","rollup-plugin-generate-package-json":"^3.1.3","rollup-plugin-json":"^4.0.0","rollup-plugin-license":"^0.12.1","rollup-plugin-terser":"^5.1.2",sinon:"7.5.0",tmp:"0.1.0","xunit-file":"*"},browserslist:[">0.2%","not dead","not ie <= 11","not op_mini all"],"lint-staged":{"*.{ts,tsx,js,jsx}":["npm run fix-ts --fix","git add --force"],"*.{json,md,graphql}":["prettier --write","git add --force"]},scripts:{"build:version":'jq ".version=\\"$(jq -r .version package.json | cut -d . -f 1-2).${PATCH:-$(date +%-H%M)}\\"" < package.json > package.json.tmp && mv package.json.tmp package.json',"lint-js":'eslint --ignore-path .eslintignore --ignore-pattern "!**/.*" .',"fix-js":"npm run lint-js --fix",lint:"npm run lint-js",deps:"npm ci","clean:dist":"rm -rf dist","clean:build":"rm -rf build && rm -rf dist/lib",clean:"npm run clean:build && npm run clean:dist",test:"npx mocha --recursive --require @babel/register test",cover:"npm install && npm run lint && npx nyc --reporter=html --reporter=text mocha --recursive --require @babel/register test",build:"npm ci && npm run build:dist","rollup:build":"npx rollup -c","rollup:watch":"npx rollup -cw","build:dist":"npm run build:version && npm run clean:dist && npm ci && npm run rollup:build"}};g.defaults.validateStatus=e=>e<=504;const E=function(){const t=(e,t)=>{e.headers||(e.headers={});let s=t;if(s||(s={}),!s.apikey)return;if(e.headers["X-EPI2ME-ApiKey"]=s.apikey,!s.apisecret)return;e.headers["X-EPI2ME-SignatureDate"]=(new Date).toISOString(),e.url.match(/^https:/)&&(e.url=e.url.replace(/:443/,"")),e.url.match(/^http:/)&&(e.url=e.url.replace(/:80/,""));const i=[e.url,Object.keys(e.headers).sort().filter(e=>e.match(/^x-epi2me/i)).map(t=>`${t}:${e.headers[t]}`).join("\n")].join("\n"),o=f.createHmac("sha1",s.apisecret).update(i).digest("hex");e.headers["X-EPI2ME-SignatureV0"]=o},s=async e=>{const t=e?e.data:null;if(!t)return Promise.reject(new Error("unexpected non-json response"));if(e&&e.status>=400){let s=`Network error ${e.status}`;return t.error&&(s=t.error),504===e.status&&(s="Please check your network connection and try again."),Promise.reject(new Error(s))}return t.error?Promise.reject(new Error(t.error)):Promise.resolve(t)};return{version:"3.0.1653",headers:(s,i)=>{const{log:o}=e({log:{debug:()=>{}}},i);let r=i;if(r||(r={}),s.headers=e({Accept:"application/json","Content-Type":"application/json","X-EPI2ME-Client":r.user_agent||"api","X-EPI2ME-Version":r.agent_version||E.version},s.headers,r.headers),"signing"in r&&!r.signing||t(s,r),r.proxy){const e=r.proxy.match(/https?:\/\/((\S+):(\S+)@)?(\S+):(\d+)/),t=e[2],i=e[3],n={host:e[4],port:e[5]};t&&i&&(n.proxyAuth=`${t}:${i}`),r.proxy.match(/^https/)?(o.debug("using HTTPS over HTTPS proxy",JSON.stringify(n)),s.httpsAgent=m({proxy:n})):(o.debug("using HTTPS over HTTP proxy",JSON.stringify(n)),s.httpsAgent=w({proxy:n})),s.proxy=!1}},get:async(t,i)=>{const{log:o}=e({log:{debug:()=>{}}},i);let r,n=i.url,a=t;i.skip_url_mangle?r=a:(a=`/${a}`,r=(n=n.replace(/\/+$/,""))+(a=a.replace(/\/+/g,"/")));const l={url:r,gzip:!0};let c;E.headers(l,i);try{o.debug(`GET ${l.url}`),c=await g.get(l.url,l)}catch(u){return Promise.reject(u)}return s(c,i)},post:async(t,i,o)=>{const{log:r}=e({log:{debug:()=>{}}},o);let n=o.url;const a={url:`${n=n.replace(/\/+$/,"")}/${t.replace(/\/+/g,"/")}`,gzip:!0,data:i,headers:{}};if(o.legacy_form){const t=[],s=e({json:JSON.stringify(i)},i);Object.keys(s).sort().forEach(e=>{t.push(`${e}=${escape(s[e])}`)}),a.data=t.join("&"),a.headers["Content-Type"]="application/x-www-form-urlencoded"}E.headers(a,o);const{data:l}=a;let c;delete a.data;try{r.debug(`POST ${a.url}`),c=await g.post(a.url,l,a)}catch(u){return Promise.reject(u)}return o.handler?o.handler(c):s(c,o)},put:async(t,i,o,r)=>{const{log:n}=e({log:{debug:()=>{}}},r);let a=r.url;const l={url:`${a=a.replace(/\/+$/,"")}/${t.replace(/\/+/g,"/")}/${i}`,gzip:!0,data:o,headers:{}};if(r.legacy_form){const t=[],s=e({json:JSON.stringify(o)},o);Object.keys(s).sort().forEach(e=>{t.push(`${e}=${escape(s[e])}`)}),l.data=t.join("&"),l.headers["Content-Type"]="application/x-www-form-urlencoded"}E.headers(l,r);const{data:c}=l;let u;delete l.data;try{n.debug(`PUT ${l.url}`),u=await g.put(l.url,c,l)}catch(h){return Promise.reject(h)}return s(u,r)}}}();E.pipe=async(e,t,s,i)=>{let o=s.url,r=`/${e}`;const n={url:(o=o.replace(/\/+$/,""))+(r=r.replace(/\/+/g,"/")),gzip:!0,headers:{"Accept-Encoding":"gzip",Accept:"application/gzip"}};return E.headers(n,s),s.proxy&&(n.proxy=s.proxy),i&&(n.onUploadProgress=i),n.responseType="stream",new Promise((e,s)=>{g.get(n.url,n).then(i=>{const o=c.createWriteStream(t);i.data.pipe(o),o.on("finish",()=>{e(t)}),o.on("error",e=>{s(new Error(`writer failed ${String(e)}`))})}).catch(e=>{s(e)})})};let _=0;E.getFileID=()=>`FILE_${_+=1}`,E.lsRecursive=async(e,s,i)=>{let o=e;const r=c.statSync(s);if(i){if(await i(s,r))return null}return r.isDirectory()?c.readdir(s).then(e=>e.map(e=>p.join(s,e))).then(e=>Promise.all(e.map(e=>E.lsRecursive(o,e,i)))).then(e=>t(e)):(r.isFile()&&o===s&&(o=p.dirname(s)),[{name:p.parse(s).base,path:s,relative:s.replace(o,""),size:r.size,id:E.getFileID()}])},E.loadInputFiles=async({inputFolder:e,outputFolder:t,filetype:i},o,r)=>{let n=i;n instanceof Array||(n=[n]),n=n.map(e=>e&&0!==e.indexOf(".")?`.${e}`:e);const a=await E.lsRecursive(e,e,async(e,s)=>{const i=p.basename(e),o=[new Promise((t,s)=>"downloads"===i||"skip"===i||"fail"===i||"fastq_fail"===i||"tmp"===i?s(new Error(`${e} failed basic filename`)):t("basic ok")),new Promise((o,r)=>{const a=n.length?new RegExp(`(?:${n.join("|")})$`):null;return e.split(p.sep).filter(e=>e.match(/^[.]/)).length||t&&i===p.basename(t)||a&&!e.match(a)&&s.isFile()?r(new Error(`${e} failed extended filename`)):o("extended ok")}),r?new Promise((t,s)=>{r(e).then(i=>i?s(new Error(`${e} failed extraFilter`)):t("extra ok"))}):Promise.resolve("extra skip")];return Promise.all(o).then(()=>null).catch(()=>"exclude")});return Promise.resolve(s(a,null))};var I=!1,j="https://epi2me.nanoporetech.com",T="EPI2ME API",x=!0,F={local:I,url:j,gqlUrl:"https://graphql.epi2me-dev.nanoporetech.com",user_agent:T,region:"eu-west-1",sessionGrace:5,uploadTimeout:1200,downloadTimeout:1200,fileCheckInterval:5,downloadCheckInterval:3,stateCheckInterval:60,inFlightDelay:600,waitTimeSeconds:20,waitTokenError:30,transferPoolSize:3,downloadMode:"data+telemetry",filetype:[".fastq",".fq",".fastq.gz",".fq.gz"],signing:x};class O{constructor(e){this.options=i({agent_version:E.version,local:I,url:j,user_agent:T,signing:x},e),this.log=this.options.log}async list(e){try{const t=await E.get(e,this.options),s=e.match(/^[a-z_]+/i)[0];return Promise.resolve(t[`${s}s`])}catch(t){return this.log.error(`list error ${String(t)}`),Promise.reject(t)}}async read(e,t){try{const s=await E.get(`${e}/${t}`,this.options);return Promise.resolve(s)}catch(s){return this.log.error("read",s),Promise.reject(s)}}async user(e){let t;if(this.options.local)t={accounts:[{id_user_account:"none",number:"NONE",name:"None"}]};else try{t=await E.get("user",this.options)}catch(s){return e?e(s):Promise.reject(s)}return e?e(null,t):Promise.resolve(t)}async status(){try{const e=await E.get("status",this.options);return Promise.resolve(e)}catch(e){return Promise.reject(e)}}async jwt(){try{const t=e=>e.headers["x-epi2me-jwt"]?Promise.resolve(e.headers["x-epi2me-jwt"]):Promise.reject(new Error("failed to fetch JWT")),s=await E.post("authenticate",{},e({handler:t},this.options));return Promise.resolve(s)}catch(t){return Promise.reject(t)}}async instanceToken(t,s){try{const o=await E.post("token",e(s,{id_workflow_instance:t}),i({},this.options,{legacy_form:!0}));return Promise.resolve(o)}catch(o){return Promise.reject(o)}}async installToken(e,t){try{const s=await E.post("token/install",{id_workflow:e},i({},this.options,{legacy_form:!0}));return t?t(null,s):Promise.resolve(s)}catch(s){return t?t(s):Promise.reject(s)}}async attributes(e){try{const t=await this.list("attribute");return e?e(null,t):Promise.resolve(t)}catch(t){return e?e(t):Promise.reject(t)}}async workflows(e){try{const t=await this.list("workflow");return e?e(null,t):Promise.resolve(t)}catch(t){return e?e(t):Promise.reject(t)}}async amiImages(e){if(this.options.local){const t=new Error("amiImages unsupported in local mode");return e?e(t):Promise.reject(t)}try{const t=this.list("ami_image");return e?e(null,t):Promise.resolve(t)}catch(t){return e?e(t):Promise.reject(t)}}async amiImage(e,t,s){let i,o,r,n;if(e&&t&&s instanceof Function?(i=e,o=t,r=s,n="update"):e&&t instanceof Object&&!(t instanceof Function)?(i=e,o=t,n="update"):e instanceof Object&&t instanceof Function?(o=e,r=t,n="create"):e instanceof Object&&!t?(o=e,n="create"):(n="read",i=e,r=t instanceof Function?t:null),this.options.local){const e=new Error("ami_image unsupported in local mode");return r?r(e):Promise.reject(e)}if("update"===n)try{const e=await E.put("ami_image",i,o,this.options);return r?r(null,e):Promise.resolve(e)}catch(a){return r?r(a):Promise.reject(a)}if("create"===n)try{const e=await E.post("ami_image",o,this.options);return r?r(null,e):Promise.resolve(e)}catch(a){return r?r(a):Promise.reject(a)}if(!i){const e=new Error("no id_ami_image specified");return r?r(e):Promise.reject(e)}try{const e=await this.read("ami_image",i);return r?r(null,e):Promise.resolve(e)}catch(a){return r?r(a):Promise.reject(a)}}async workflow(t,s,i){let r,n,a,l;if(t&&s&&i instanceof Function?(r=t,n=s,a=i,l="update"):t&&s instanceof Object&&!(s instanceof Function)?(r=t,n=s,l="update"):t instanceof Object&&s instanceof Function?(n=t,a=s,l="create"):t instanceof Object&&!s?(n=t,l="create"):(l="read",r=t,a=s instanceof Function?s:null),"update"===l)try{const e=await E.put("workflow",r,n,this.options);return a?a(null,e):Promise.resolve(e)}catch(d){return a?a(d):Promise.reject(d)}if("create"===l)try{const e=await E.post("workflow",n,this.options);return a?a(null,e):Promise.resolve(e)}catch(d){return a?a(d):Promise.reject(d)}if(!r){const e=new Error("no workflow id specified");return a?a(e):Promise.reject(e)}const c={};try{const t=await this.read("workflow",r);if(t.error)throw new Error(t.error);e(c,t)}catch(d){return this.log.error(`${r}: error fetching workflow ${String(d)}`),a?a(d):Promise.reject(d)}e(c,{params:{}});try{const t=await E.get(`workflow/config/${r}`,this.options);if(t.error)throw new Error(t.error);e(c,t)}catch(d){return this.log.error(`${r}: error fetching workflow config ${String(d)}`),a?a(d):Promise.reject(d)}const u=o(c.params,{widget:"ajax_dropdown"}),h=[...u.map((e,t)=>{const s=u[t];return new Promise((e,t)=>{const i=s.values.source.replace("{{EPI2ME_HOST}}","").replace(/&?apikey=\{\{EPI2ME_API_KEY\}\}/,"");E.get(i,this.options).then(t=>{const i=t[s.values.data_root];return i&&(s.values=i.map(e=>({label:e[s.values.items.label_key],value:e[s.values.items.value_key]}))),e()}).catch(e=>(this.log.error(`failed to fetch ${i}`),t(e)))})})];try{return await Promise.all(h),a?a(null,c):Promise.resolve(c)}catch(d){return this.log.error(`${r}: error fetching config and parameters ${String(d)}`),a?a(d):Promise.reject(d)}}async startWorkflow(e,t){return E.post("workflow_instance",e,i({},this.options,{legacy_form:!0}),t)}stopWorkflow(e,t){return E.put("workflow_instance/stop",e,null,i({},this.options,{legacy_form:!0}),t)}async workflowInstances(e,t){let s,i;if(!e||e instanceof Function||void 0!==t?(s=e,i=t):i=e,i&&i.run_id)try{const e=(await E.get(`workflow_instance/wi?show=all&columns[0][name]=run_id;columns[0][searchable]=true;columns[0][search][regex]=true;columns[0][search][value]=${i.run_id};`,this.options)).data.map(e=>({id_workflow_instance:e.id_ins,id_workflow:e.id_flo,run_id:e.run_id,description:e.desc,rev:e.rev}));return s?s(null,e):Promise.resolve(e)}catch(o){return s?s(o):Promise.reject(o)}try{const e=await this.list("workflow_instance");return s?s(null,e):Promise.resolve(e)}catch(o){return s?s(o):Promise.reject(o)}}async workflowInstance(e,t){try{const s=await this.read("workflow_instance",e);return t?t(null,s):Promise.resolve(s)}catch(s){return t?t(s):Promise.reject(s)}}workflowConfig(e,t){return E.get(`workflow/config/${e}`,this.options,t)}async register(e,t,s){let o,r;t&&t instanceof Function?r=t:(o=t,r=s);try{const t=await E.put("reg",e,{description:o||`${u.userInfo().username}@${u.hostname()}`},i({},this.options,{signing:!1}));return r?r(null,t):Promise.resolve(t)}catch(n){return r?r(n):Promise.reject(n)}}async datasets(e,t){let s,i;!e||e instanceof Function||void 0!==t?(s=e,i=t):i=e,i||(i={}),i.show||(i.show="mine");try{const e=await this.list(`dataset?show=${i.show}`);return s?s(null,e):Promise.resolve(e)}catch(o){return s?s(o):Promise.reject(o)}}async dataset(e,t){if(!this.options.local)try{const s=await this.read("dataset",e);return t?t(null,s):Promise.resolve(s)}catch(s){return t?t(s):Promise.reject(s)}try{const s=(await this.datasets()).find(t=>t.id_dataset===e);return t?t(null,s):Promise.resolve(s)}catch(s){return t?t(s):Promise.reject(s)}}async fetchContent(e,t){const s=i({},this.options,{skip_url_mangle:!0,headers:{"Content-Type":""}});try{const i=await E.get(e,s);return t?t(null,i):Promise.resolve(i)}catch(o){return t?t(o):Promise.reject(o)}}}class N extends O{async workflows(e){if(!this.options.local)return super.workflows(e);const t=p.join(this.options.url,"workflows");let s;try{return s=(await c.readdir(t)).filter(e=>c.statSync(p.join(t,e)).isDirectory()).map(e=>p.join(t,e,"workflow.json")).map(e=>c.readJsonSync(e)),e?e(null,s):Promise.resolve(s)}catch(i){return this.log.warn(i),e?e(void 0):Promise.reject(void 0)}}async workflow(e,t,s){if(!this.options.local||!e||"object"===typeof e||s)return super.workflow(e,t,s);const i=p.join(this.options.url,"workflows"),o=p.join(i,e,"workflow.json");try{const e=await c.readJson(o);return s?s(null,e):Promise.resolve(e)}catch(r){return s?s(r):Promise.reject(r)}}async workflowInstances(e,t){if(!this.options.local)return super.workflowInstances(e,t);let s,i;if(!e||e instanceof Function||void 0!==t?(s=e,i=t):i=e,i){const e=new Error("querying of local instances unsupported in local mode");return s?s(e):Promise.reject(e)}const o=p.join(this.options.url,"instances");try{let e=await c.readdir(o);return e=(e=e.filter(e=>c.statSync(p.join(o,e)).isDirectory())).map(e=>{const t=p.join(o,e,"workflow.json");let s;try{s=c.readJsonSync(t)}catch(i){s={id_workflow:"-",description:"-",rev:"0.0"}}return s.id_workflow_instance=e,s.filename=t,s}),s?s(null,e):Promise.resolve(e)}catch(r){return s?s(r):Promise.reject(r)}}async datasets(e,t){if(!this.options.local)return super.datasets(e,t);let s,i;if(!e||e instanceof Function||void 0!==t?(s=e,i=t):i=e,i||(i={}),i.show||(i.show="mine"),"mine"!==i.show)return s(new Error("querying of local datasets unsupported in local mode"));const o=p.join(this.options.url,"datasets");try{let e=await c.readdir(o);e=e.filter(e=>c.statSync(p.join(o,e)).isDirectory());let t=0;return e=e.sort().map(e=>({is_reference_dataset:!0,summary:null,dataset_status:{status_label:"Active",status_value:"active"},size:0,prefix:e,id_workflow_instance:null,id_account:null,is_consented_human:null,data_fields:null,component_id:null,uuid:e,is_shared:!1,id_dataset:t+=1,id_user:null,last_modified:null,created:null,name:e,source:e,attributes:null})),s?s(null,e):Promise.resolve(e)}catch(r){return this.log.warn(r),s?s(null,[]):Promise.resolve([])}}async bundleWorkflow(e,t,s){return E.pipe(`workflow/bundle/${e}.tar.gz`,t,this.options,s)}}const R={fastq:function(e){return new Promise((t,s)=>{let i,o=1,r={size:0};try{r=c.statSync(e)}catch(n){return void s(n)}c.createReadStream(e).on("data",e=>{i=-1,o-=1;do{i=e.indexOf(10,i+1),o+=1}while(-1!==i)}).on("end",()=>t({type:"fastq",bytes:r.size,reads:Math.floor(o/4)})).on("error",s)})},fasta:function(e){return new Promise((t,s)=>{let i,o=1,r={size:0};try{r=c.statSync(e)}catch(n){s(n)}c.createReadStream(e).on("data",e=>{i=-1,o-=1;do{i=e.indexOf(62,i+1),o+=1}while(-1!==i)}).on("end",()=>t({type:"fasta",bytes:r.size,sequences:Math.floor((1+o)/2)})).on("error",s)})},default:async function(e){return c.stat(e).then(e=>({type:"bytes",bytes:e.size}))}};function M(e){if("string"!==typeof e&&!(e instanceof String))return Promise.resolve({});let t=p.extname(e).toLowerCase().replace(/^[.]/,"");return"fq"===t?t="fastq":"fa"===t&&(t="fasta"),R[t]||(t="default"),R[t](e)}async function C(t,s,i,o,r){const{maxChunkBytes:n,maxChunkReads:a}=e({},s),l=p.dirname(t),u=p.basename(t),h=u.match(/^[^.]+/)[0],d=u.replace(h,""),g=p.join(l,h);if(!n&&!a)return i(t).then(()=>({source:t,split:!1,chunks:[t]}));const f=await c.stat(t);return n&&f.size<n?i(t).then(()=>({source:t,split:!1,chunks:[t]})):new Promise(e=>{let s,l,u=0,h=0,p="",f=0,m=0;const w={source:t,split:!0,chunks:[]};y.createInterface({input:o(t)}).on("line",async t=>{p+=t,p+="\n",(h+=1)>=4&&(h=0,(async t=>{if(!f){s=`${g}_${u+=1}${d}`,w.chunks.push(s);const t=()=>{i(s).then(()=>{e(w)})};r?l=r(s,t):(l=c.createWriteStream(s)).on("close",t)}f+=1,m+=t.length,l.write(t,()=>{}),(n&&m>=n||a&&f>=a)&&(f=0,m=0,l.end())})(p),p="")})})}async function A(e,t,s){return C(e,t,s,e=>c.createReadStream(e))}async function q(e,t,s){return C(e,t,s,e=>c.createReadStream(e).pipe(k.createGunzip()),(e,t)=>{const s=c.createWriteStream(e);s.on("close",t);const i=k.createGzip();return i.pipe(s),i})}const Q=(e,t)=>{const s=["","K","M","G","T","P","E","Z"];let i=t||0,o=e||0;return o>=1e3?(o/=1e3,(i+=1)>=s.length?"???":Q(o,i)):0===i?`${o}${s[i]}`:`${o.toFixed(1)}${s[i]}`};class D{constructor(t,s){this.debounces={},this.debounceWindow=e({debounceWindow:2e3},s).debounceWindow,this.log=e({log:{debug:()=>{}}},s).log,t.jwt().then(e=>{this.socket=v(s.url,{transportOptions:{polling:{extraHeaders:{Cookie:`x-epi2me-jwt=${e}`}}}}),this.socket.on("connect",()=>{this.log.debug("socket ready")})})}debounce(t,s){const i=e(t)._uuid;if(i){if(this.debounces[i])return;this.debounces[i]=1,setTimeout(()=>{delete this.debounces[i]},this.debounceWindow)}s&&s(t)}watch(e,t){if(!this.socket)return this.log.debug(`socket not ready. requeueing watch on ${e}`),void setTimeout(()=>{this.watch(e,t)},1e3);this.socket.on(e,e=>this.debounce(e,t))}emit(e,t){if(!this.socket)return this.log.debug(`socket not ready. requeueing emit on ${e}`),void setTimeout(()=>{this.emit(e,t)},1e3);this.socket.emit(e,t)}}class U{constructor(t){let s;if((s="string"===typeof t||"object"===typeof t&&t.constructor===String?JSON.parse(t):t||{}).log){if(!r([s.log.info,s.log.warn,s.log.error,s.log.debug,s.log.json],n))throw new Error("expected log object to have error, debug, info, warn and json methods");this.log=s.log}else this.log={info:e=>{console.info(`[${(new Date).toISOString()}] INFO: ${e}`)},debug:e=>{console.debug(`[${(new Date).toISOString()}] DEBUG: ${e}`)},warn:e=>{console.warn(`[${(new Date).toISOString()}] WARN: ${e}`)},error:e=>{console.error(`[${(new Date).toISOString()}] ERROR: ${e}`)},json:e=>{console.log(JSON.stringify(e))}};this.stopped=!0,this.states={upload:{filesCount:0,success:{files:0,bytes:0,reads:0},types:{},niceTypes:"",progress:{bytes:0,total:0}},download:{progress:{},success:{files:0,reads:0,bytes:0},fail:0,types:{},niceTypes:""},warnings:[]},this.config={options:a(s,F),instance:{id_workflow_instance:s.id_workflow_instance,inputQueueName:null,outputQueueName:null,outputQueueURL:null,discoverQueueCache:{},bucket:null,bucketFolder:null,remote_addr:null,chain:null,key_id:null}},this.config.instance.awssettings={region:this.config.options.region},this.REST=new O(e({log:this.log},this.config.options)),this.timers={downloadCheckInterval:null,stateCheckInterval:null,fileCheckInterval:null,transferTimeouts:{},visibilityIntervals:{},summaryTelemetryInterval:null}}async socket(){return this.mySocket?this.mySocket:(this.mySocket=new D(this.REST,e({log:this.log},this.config.options)),this.mySocket)}async stopEverything(){this.stopped=!0,this.log.debug("stopping watchers"),["downloadCheckInterval","stateCheckInterval","fileCheckInterval","summaryTelemetryInterval"].forEach(e=>{this.timers[e]&&(this.log.debug(`clearing ${e} interval`),clearInterval(this.timers[e]),this.timers[e]=null)}),Object.keys(this.timers.transferTimeouts).forEach(e=>{this.log.debug(`clearing transferTimeout for ${e}`),clearTimeout(this.timers.transferTimeouts[e]),delete this.timers.transferTimeouts[e]}),Object.keys(this.timers.visibilityIntervals).forEach(e=>{this.log.debug(`clearing visibilityInterval for ${e}`),clearInterval(this.timers.visibilityIntervals[e]),delete this.timers.visibilityIntervals[e]}),this.downloadWorkerPool&&(this.log.debug("clearing downloadWorkerPool"),await Promise.all(Object.values(this.downloadWorkerPool)),this.downloadWorkerPool=null);const{id_workflow_instance:e}=this.config.instance;if(e){try{await this.REST.stopWorkflow(e)}catch(t){return this.log.error(`Error stopping instance: ${String(t)}`),Promise.reject(t)}this.log.info(`workflow instance ${e} stopped`)}return Promise.resolve()}async session(e,t){let s=!1;if(e&&e.length&&(s=!0),!s){if(this.sessioning)return Promise.resolve();if(this.states.sts_expiration&&this.states.sts_expiration>Date.now())return Promise.resolve();this.sessioning=!0}let i=null;try{await this.fetchInstanceToken(e,t)}catch(o){i=o,this.log.error(`session error ${String(i)}`)}finally{s||(this.sessioning=!1)}return i?Promise.reject(i):Promise.resolve()}async fetchInstanceToken(e,t){if(!this.config.instance.id_workflow_instance)return Promise.reject(new Error("must specify id_workflow_instance"));this.log.debug("new instance token needed");try{const s=await this.REST.instanceToken(this.config.instance.id_workflow_instance,t);this.log.debug(`allocated new instance token expiring at ${s.expiration}`),this.states.sts_expiration=new Date(s.expiration).getTime()-60*this.config.options.sessionGrace,this.config.options.proxy&&b.config.update({httpOptions:{agent:S(this.config.options.proxy,!0)}}),b.config.update(this.config.instance.awssettings),b.config.update(s),e&&e.forEach(e=>{try{e.config.update(s)}catch(t){this.log.warn(`failed to update config on ${String(e)}: ${String(t)}`)}})}catch(s){this.log.warn(`failed to fetch instance token: ${String(s)}`)}return Promise.resolve()}async sessionedS3(e){return await this.session(null,e),new b.S3({useAccelerateEndpoint:"on"===this.config.options.awsAcceleration})}async sessionedSQS(e){return await this.session(null,e),new b.SQS}reportProgress(){const{upload:e,download:t}=this.states;this.log.json({progress:{download:t,upload:e}})}storeState(e,t,s,i){const o=i||{};this.states[e]||(this.states[e]={}),this.states[e][t]||(this.states[e][t]={}),"incr"===s?Object.keys(o).forEach(s=>{this.states[e][t][s]=this.states[e][t][s]?this.states[e][t][s]+parseInt(o[s],10):parseInt(o[s],10)}):Object.keys(o).forEach(s=>{this.states[e][t][s]=this.states[e][t][s]?this.states[e][t][s]-parseInt(o[s],10):-parseInt(o[s],10)});try{this.states[e].success.niceReads=Q(this.states[e].success.reads)}catch(n){this.states[e].success.niceReads=0}try{this.states[e].progress.niceSize=Q(this.states[e].success.bytes+this.states[e].progress.bytes||0)}catch(n){this.states[e].progress.niceSize=0}try{this.states[e].success.niceSize=Q(this.states[e].success.bytes)}catch(n){this.states[e].success.niceSize=0}this.states[e].niceTypes=Object.keys(this.states[e].types||{}).sort().map(t=>`${this.states[e].types[t]} ${t}`).join(", ");const r=Date.now();(!this.stateReportTime||r-this.stateReportTime>2e3)&&(this.stateReportTime=r,this.reportProgress())}uploadState(e,t,s){return this.storeState("upload",e,t,s)}downloadState(e,t,s){return this.storeState("download",e,t,s)}async deleteMessage(e){try{const t=await this.discoverQueue(this.config.instance.outputQueueName);return(await this.sessionedSQS()).deleteMessage({QueueUrl:t,ReceiptHandle:e.ReceiptHandle}).promise()}catch(t){return this.log.error(`deleteMessage exception: ${String(t)}`),this.states.download.failure||(this.states.download.failure={}),this.states.download.failure[t]=this.states.download.failure[t]?this.states.download.failure[t]+1:1,Promise.reject(t)}}async discoverQueue(e){if(this.config.instance.discoverQueueCache[e])return Promise.resolve(this.config.instance.discoverQueueCache[e]);let t;this.log.debug(`discovering queue for ${e}`);try{const s=await this.sessionedSQS();t=await s.getQueueUrl({QueueName:e}).promise()}catch(s){return this.log.error(`Error: failed to find queue for ${e}: ${String(s)}`),Promise.reject(s)}return this.log.debug(`found queue ${t.QueueUrl}`),this.config.instance.discoverQueueCache[e]=t.QueueUrl,Promise.resolve(t.QueueUrl)}async queueLength(e){if(!e)return Promise.reject(new Error("no queueURL specified"));const t=e.match(/([\w\-_]+)$/)[0];this.log.debug(`querying queue length of ${t}`);try{const t=await this.sessionedSQS(),s=await t.getQueueAttributes({QueueUrl:e,AttributeNames:["ApproximateNumberOfMessages"]}).promise();if(s&&s.Attributes&&"ApproximateNumberOfMessages"in s.Attributes){let e=s.Attributes.ApproximateNumberOfMessages;return e=parseInt(e,10)||0,Promise.resolve(e)}return Promise.reject(new Error("unexpected response"))}catch(s){return this.log.error(`error in getQueueAttributes ${String(s)}`),Promise.reject(s)}}url(){return this.config.options.url}apikey(){return this.config.options.apikey}attr(e,t){if(!(e in this.config.options))throw new Error(`config object does not contain property ${e}`);return t?(this.config.options[e]=t,this):this.config.options[e]}stats(e){return this.states[e]}}U.version=E.version,U.REST=O,U.utils=E;class z{constructor(t,s,i){const o=e({},s);this.options=o,this.log=i;const{idWorkflowInstance:r}=o;i.debug(`setting up ${t}/db.sqlite for ${r}`),this.db=c.mkdirp(t).then(()=>(this.log.debug(`opening ${t}/db.sqlite`),P.open(p.join(t,"db.sqlite"),{Promise:Promise}).then(async e=>{this.log.debug(`opened ${t}/db.sqlite`);try{return await Promise.all([e.run("CREATE TABLE IF NOT EXISTS meta (version CHAR(12) DEFAULT '' NOT NULL, idWorkflowInstance INTEGER UNSIGNED, inputFolder CHAR(255) default '')").then(()=>{e.run("INSERT INTO meta (version, idWorkflowInstance, inputFolder) VALUES(?, ?, ?)",$.version,r,o.inputFolder)}),e.run("CREATE TABLE IF NOT EXISTS uploads (filename CHAR(255) DEFAULT '' NOT NULL PRIMARY KEY)"),e.run("CREATE TABLE IF NOT EXISTS skips (filename CHAR(255) DEFAULT '' NOT NULL PRIMARY KEY)"),e.run("CREATE TABLE IF NOT EXISTS splits (filename CHAR(255) DEFAULT '' NOT NULL PRIMARY KEY, parent CHAR(255) DEFAULT '' NOT NULL, start DATETIME NOT NULL, end DATETIME)")]),Promise.resolve(e)}catch(s){return this.log.error(s),Promise.reject(s)}}))).catch(e=>{throw this.log.error(e),e})}async uploadFile(e){const t=await this.db,s=e.replace(new RegExp(`^${this.options.inputFolder}`),"");return t.run("INSERT INTO uploads VALUES(?)",s)}async skipFile(e){const t=await this.db,s=e.replace(new RegExp(`^${this.options.inputFolder}`),"");return t.run("INSERT INTO skips VALUES(?)",s)}async splitFile(e,t){const s=await this.db,i=e.replace(new RegExp(`^${this.options.inputFolder}`),""),o=t.replace(new RegExp(`^${this.options.inputFolder}`),"");return s.run("INSERT INTO splits VALUES(?, ?, CURRENT_TIMESTAMP, NULL)",i,o)}async splitDone(e){const t=await this.db,s=e.replace(new RegExp(`^${this.options.inputFolder}`),"");return t.run("UPDATE splits SET end=CURRENT_TIMESTAMP WHERE filename=?",s)}async splitClean(){return(await this.db).all("SELECT filename FROM splits WHERE end IS NULL").then(e=>{if(!e)return this.log.info("no split files to clean"),Promise.resolve();this.log.info(`cleaning ${e.length} split files`),this.log.debug(`going to clean: ${e.map(e=>e.filename).join(" ")}`);const t=e.map(e=>c.unlink(p.join(this.options.inputFolder,e.filename)).catch(()=>{}));return Promise.all(t)})}async seenUpload(e){const t=await this.db,i=e.replace(new RegExp(`^${this.options.inputFolder}`),"");return Promise.all([t.get("SELECT * FROM uploads u WHERE u.filename=? LIMIT 1",i),t.get("SELECT * FROM skips s WHERE s.filename=? LIMIT 1",i)]).then(e=>s(e,void 0).length)}}class L{constructor(t,s){this.prefsFile=t||L.profilePath(),this.allProfileData={},this.defaultEndpoint=process.env.METRICHOR||F.endpoint||F.url,this.raiseExceptions=s;try{this.allProfileData=e(c.readJSONSync(this.prefsFile),{profiles:{}}),this.allProfileData.endpoint&&(this.defaultEndpoint=this.allProfileData.endpoint)}catch(i){if(this.raiseExceptions)throw i}}static profilePath(){return p.join(h(),".epi2me.json")}profile(t,s){if(t&&s){e(this.allProfileData,{profiles:{[t]:s}});try{c.writeJSONSync(this.prefsFile,this.allProfileData)}catch(i){if(this.raiseExceptions)throw i}}return t?e({endpoint:this.defaultEndpoint},this.allProfileData.profiles[t]):{}}profiles(){return Object.keys(this.allProfileData.profiles||{})}}class W{static MakeQueryablePromise(e){if(e.isResolved)return e;let t=!0,s=!1,i=!1;const o=e.then(e=>(i=!0,t=!1,e)).catch(e=>{throw s=!0,t=!1,e});return o.dependsOn=e,o.isResolved=()=>i,o.isPending=()=>t,o.isRejected=()=>s,o}constructor(t){const s=e({bandwidth:1,interval:500},t);this.bandwidth=s.bandwidth,this.interval=s.interval,this.pipeline=[],this.running=[],this.completed=0,this.intervalId=null,"start"in s&&!s.start||this.start()}enqueue(e){this.pipeline.push(e)}start(){this.intervalId||(this.intervalId=setInterval(()=>{this.monitorInterval()},this.interval))}stop(){clearInterval(this.intervalId),delete this.intervalId}state(){return{queued:this.pipeline.length,running:this.running.length,completed:this.completed,state:this.intervalId?"running":"stopped"}}monitorInterval(){this.running.map((e,t)=>e.isPending()?null:t).filter(e=>e).reverse().forEach(e=>{this.running.splice(e,1),this.completed+=1});const e=this.bandwidth-this.running.length;for(let t=0;t<e;t+=1){const e=this.pipeline.shift();if(!e)return;this.running.push(W.MakeQueryablePromise(e()))}}}const J=()=>{const e=process.env.APPDATA||("darwin"===process.platform?p.join(h(),"Library/Application Support"):h());return process.env.EPI2ME_HOME||p.join(e,"linux"===process.platform?".epi2me":"EPI2ME")};class H extends U{constructor(t){super(t),this.REST=new N(e({},{log:this.log},this.config.options))}async autoStart(e,t){let s;this.stopped=!1;try{s=await this.REST.startWorkflow(e)}catch(i){const e=`Failed to start workflow: ${String(i)}`;return this.log.warn(e),t?t(e):Promise.reject(i)}return this.config.workflow=JSON.parse(JSON.stringify(e)),this.log.info(`instance ${JSON.stringify(s)}`),this.log.info(`workflow config ${JSON.stringify(this.config.workflow)}`),this.autoConfigure(s,t)}async autoJoin(e,t){let s;this.stopped=!1,this.config.instance.id_workflow_instance=e;try{s=await this.REST.workflowInstance(e)}catch(i){const e=`Failed to join workflow instance: ${String(i)}`;return this.log.warn(e),t?t(e):Promise.reject(i)}return"stopped"===s.state?(this.log.warn(`workflow ${e} is already stopped`),t?t("could not join workflow"):Promise.reject(new Error("could not join workflow"))):(this.config.workflow=this.config.workflow||{},this.log.debug(`instance ${JSON.stringify(s)}`),this.log.debug(`workflow config ${JSON.stringify(this.config.workflow)}`),this.autoConfigure(s,t))}async autoConfigure(e,t){if(["id_workflow_instance","id_workflow","remote_addr","key_id","bucket","user_defined","start_date"].forEach(t=>{this.config.instance[t]=e[t]}),this.config.instance.inputQueueName=e.inputqueue,this.config.instance.outputQueueName=e.outputqueue,this.config.instance.awssettings.region=e.region||this.config.options.region,this.config.instance.bucketFolder=`${e.outputqueue}/${e.id_user}/${e.id_workflow_instance}`,this.config.instance.summaryTelemetry=e.telemetry,e.chain)if("object"===typeof e.chain)this.config.instance.chain=e.chain;else try{this.config.instance.chain=JSON.parse(e.chain)}catch(a){throw new Error(`exception parsing chain JSON ${String(a)}`)}if(!this.config.options.inputFolder)throw new Error("must set inputFolder");if(!this.config.options.outputFolder)throw new Error("must set outputFolder");if(!this.config.instance.bucketFolder)throw new Error("bucketFolder must be set");if(!this.config.instance.inputQueueName)throw new Error("inputQueueName must be set");if(!this.config.instance.outputQueueName)throw new Error("outputQueueName must be set");c.mkdirpSync(this.config.options.outputFolder);const s=p.join(J(),"instances"),i=p.join(s,this.config.instance.id_workflow_instance);this.db=new z(i,{idWorkflowInstance:this.config.instance.id_workflow_instance,inputFolder:this.config.options.inputFolder},this.log);const o=this.config.instance.id_workflow_instance?`telemetry-${this.config.instance.id_workflow_instance}.log`:"telemetry.log",r=p.join(this.config.options.outputFolder,"epi2me-logs"),n=p.join(r,o);return c.mkdirp(r,e=>{if(e&&!String(e).match(/EEXIST/))this.log.error(`error opening telemetry log stream: mkdirpException:${String(e)}`);else try{this.telemetryLogStream=c.createWriteStream(n,{flags:"a"}),this.log.info(`logging telemetry to ${n}`)}catch(t){this.log.error(`error opening telemetry log stream: ${String(t)}`)}}),t&&t(null,this.config.instance),this.timers.summaryTelemetryInterval=setInterval(()=>{this.stopped?clearInterval(this.timers.summaryTelemetryInterval):this.fetchTelemetry()},1e4*this.config.options.downloadCheckInterval),this.timers.downloadCheckInterval=setInterval(()=>{this.stopped?clearInterval(this.timers.downloadCheckInterval):this.checkForDownloads()},1e3*this.config.options.downloadCheckInterval),this.timers.stateCheckInterval=setInterval(async()=>{if(this.stopped)clearInterval(this.timers.stateCheckInterval);else try{const t=await this.REST.workflowInstance(this.config.instance.id_workflow_instance);if("stopped"===t.state){this.log.warn(`instance was stopped remotely at ${t.stop_date}. shutting down the workflow.`);try{const e=await this.stopEverything();"function"===typeof e.config.options.remoteShutdownCb&&e.config.options.remoteShutdownCb(`instance was stopped remotely at ${t.stop_date}`)}catch(e){this.log.error(`Error whilst stopping: ${String(e)}`)}}}catch(t){this.log.warn(`failed to check instance state: ${t&&t.error?t.error:t}`)}},1e3*this.config.options.stateCheckInterval),await this.session(),this.reportProgress(),this.loadUploadFiles(),this.timers.fileCheckInterval=setInterval(this.loadUploadFiles.bind(this),1e3*this.config.options.fileCheckInterval),Promise.resolve(e)}async stopEverything(){return await super.stopEverything(),this.log.debug("clearing split files"),this.db?this.db.splitClean():Promise.resolve()}async checkForDownloads(){if(this.checkForDownloadsRunning)return Promise.resolve();this.checkForDownloadsRunning=!0,this.log.debug("checkForDownloads checking for downloads");try{const e=await this.discoverQueue(this.config.instance.outputQueueName),t=await this.queueLength(e);t?(this.log.debug(`downloads available: ${t}`),await this.downloadAvailable()):this.log.debug("no downloads available")}catch(e){this.log.warn(`checkForDownloads error ${String(e)}`),this.states.download.failure||(this.states.download.failure={}),this.states.download.failure[e]=this.states.download.failure[e]?this.states.download.failure[e]+1:1}return this.checkForDownloadsRunning=!1,Promise.resolve()}async downloadAvailable(){const e=Object.keys(this.downloadWorkerPool||{}).length;if(e>=this.config.options.transferPoolSize)return this.log.debug(`${e} downloads already queued`),Promise.resolve();let t;try{const s=await this.discoverQueue(this.config.instance.outputQueueName);this.log.debug("fetching messages");const i=await this.sessionedSQS();t=await i.receiveMessage({AttributeNames:["All"],QueueUrl:s,VisibilityTimeout:this.config.options.inFlightDelay,MaxNumberOfMessages:this.config.options.transferPoolSize-e,WaitTimeSeconds:this.config.options.waitTimeSeconds}).promise()}catch(s){return this.log.error(`receiveMessage exception: ${String(s)}`),this.states.download.failure[s]=this.states.download.failure[s]?this.states.download.failure[s]+1:1,Promise.reject(s)}return this.receiveMessages(t)}async loadUploadFiles(){if(this.dirScanInProgress)return Promise.resolve();this.dirScanInProgress=!0,this.log.debug("upload: started directory scan");try{const e=e=>this.db.seenUpload(e),t=await E.loadInputFiles(this.config.options,this.log,e);let s=0;const i=()=>new Promise(e=>{if(this.stopped)return t.length=0,this.log.debug("upload: skipping, stopped"),void e();if(s>this.config.options.transferPoolSize)return void setTimeout(e,1e3);const i=t.splice(0,this.config.options.transferPoolSize-s);s+=i.length,this.enqueueUploadFiles(i).then().catch(e=>{this.log.error(`upload: exception in enqueueUploadFiles: ${String(e)}`)}).finally(()=>{s-=i.length,e()})});for(;t.length;)await i()}catch(e){this.log.error(`upload: exception in loadInputFiles: ${String(e)}`)}return this.dirScanInProgress=!1,this.log.debug("upload: finished directory scan"),Promise.resolve()}async enqueueUploadFiles(e){let t=0,s=0,i=0,o=0,r={};if(!l(e)||!e.length)return Promise.resolve();if(this.log.info(`enqueueUploadFiles ${e.length} files: ${e.map(e=>e.path).join(" ")}.`),"workflow"in this.config)if("workflow_attributes"in this.config.workflow)r=this.config.workflow.workflow_attributes;else if("attributes"in this.config.workflow){let{attributes:e}=this.config.workflow;if(e||(e={}),["max_size","max_files","split_size","split_reads"].forEach(t=>{`epi2me:${t}`in e&&(r[t]=parseInt(e[`epi2me:${t}`],10))}),"epi2me:category"in e){e["epi2me:category"].includes("storage")&&(r.requires_storage=!0)}}if(this.log.info(`enqueueUploadFiles settings ${JSON.stringify(r)}`),"requires_storage"in r&&r.requires_storage&&!("storage_account"in this.config.workflow)){const e={msg:"ERROR: Workflow requires storage enabled. Please provide a valid storage account [ --storage ].",type:"WARNING_STORAGE_ENABLED"};return this.log.error(e.msg),this.states.warnings.push(e),Promise.resolve()}if("split_size"in r&&(i=parseInt(r.split_size,10),this.log.info(`enqueueUploadFiles splitting supported files at ${i} bytes`)),"split_reads"in r&&(o=parseInt(r.split_reads,10),this.log.info(`enqueueUploadFiles splitting supported files at ${o} reads`)),"max_size"in r&&(s=parseInt(r.max_size,10),this.log.info(`enqueueUploadFiles restricting file size to ${s}`)),"max_files"in r&&(t=parseInt(r.max_files,10),this.log.info(`enqueueUploadFiles restricting file count to ${t}`),e.length>t)){const s={msg:`ERROR: ${e.length} files found. Workflow can only accept ${t}. Please move the extra files away.`,type:"WARNING_FILE_TOO_MANY"};return this.log.error(s.msg),this.states.warnings.push(s),Promise.resolve()}this.states.upload.filesCount+=e.length;const n=e.map(async e=>{const r=e;if(t&&this.states.upload.filesCount>t){const e=`Maximum ${t} file(s) already uploaded. Marking ${r.relative} as skipped.`,s={msg:e,type:"WARNING_FILE_TOO_MANY"};this.log.error(e),this.states.warnings.push(s),this.states.upload.filesCount-=1,r.skip="SKIP_TOO_MANY"}else if(0===r.size){const e=`The file "${r.relative}" is empty. It will be skipped.`,t={msg:e,type:"WARNING_FILE_EMPTY"};r.skip="SKIP_EMPTY",this.states.upload.filesCount-=1,this.log.error(e),this.states.warnings.push(t)}else{if(r.path&&r.path.match(/\.(?:fastq|fq)(?:\.gz)?$/)&&(i&&r.size>i||o&&r.reads&&r.reads>o)){const e=`${r.relative} is too big and is going to be split`;this.log.warn(e);const t={msg:e,type:"WARNING_FILE_SPLIT"};this.states.warnings.push(t);const s=i?{maxChunkBytes:i}:{maxChunkReads:o},a=r.path.match(/\.gz$/)?q:A,l=E.getFileID(),c=new W({bandwidth:this.config.options.transferPoolSize});let u=0;const h=async e=>(await this.db.splitFile(e,r.path),this.stopped?(c.stop(),this.log.info(`stopped, so skipping ${e}`),Promise.reject(new Error("stopped"))):(u+=1,M(e).then(t=>({name:p.basename(e),path:e,relative:e.replace(this.config.options.inputFolder,""),id:`${l}_${u}`,stats:t,size:t.bytes})).then(e=>{return new Promise(t=>{c.enqueue(()=>(this.log.info(`chunk upload starting ${e.id} ${e.path}`),this.stopped?(this.log.info(`chunk upload skipped (stopped) ${e.id} ${e.path}`),c.stop(),t(),Promise.resolve()):this.uploadJob(e).then(()=>this.db.splitDone(e.path)).catch(t=>{this.log.error(`chunk upload failed ${e.id} ${e.path}: ${String(t)}`)}).finally(t)))})})));try{await a(r.path,s,h),c.stop()}catch(n){if(c.stop(),"Error: stopped"===String(n))return Promise.resolve();throw n}return this.db.uploadFile(r.path)}if(s&&r.size>s){const e=`The file "${r.relative}" is bigger than the maximum size limit (${Q(s)}B). It will be skipped.`,t={msg:e,type:"WARNING_FILE_TOO_BIG"};r.skip="SKIP_TOO_BIG",this.states.upload.filesCount-=1,this.log.error(e),this.states.warnings.push(t)}else try{r.stats=await M(r.path)}catch(a){this.log.error(`failed to stat ${r.path}: ${String(a)}`)}}return this.uploadJob(r)});try{return await Promise.all(n),this.log.info(`upload: inputBatchQueue (${n.length} jobs) complete`),this.loadUploadFiles()}catch(a){return this.log.error(`upload: enqueueUploadFiles exception ${String(a)}`),Promise.reject(a)}}async uploadJob(t){if("skip"in t)return this.db.skipFile(t.path);let s,i;try{this.log.info(`upload: ${t.id} starting`),s=await this.uploadHandler(t),this.log.info(`upload: ${s.id} uploaded and notified`)}catch(o){i=o,this.log.error(`upload: ${t.id} done, but failed: ${String(i)}`)}if(s||(s={}),i)this.log.error(`uploadJob ${i}`),this.states.upload.failure||(this.states.upload.failure={}),this.states.upload.failure[i]=this.states.upload.failure[i]?this.states.upload.failure[i]+1:1;else if(this.uploadState("success","incr",e({files:1},s.stats)),s.name){const e=p.extname(s.name);this.uploadState("types","incr",{[e]:1})}return Promise.resolve()}async receiveMessages(e){return e&&e.Messages&&e.Messages.length?(this.downloadWorkerPool||(this.downloadWorkerPool={}),e.Messages.forEach(e=>{this.downloadWorkerPool[e.MessageId]=1;const t=setTimeout(()=>{throw this.log.error(`this.downloadWorkerPool timeoutHandle. Clearing queue slot for message: ${e.MessageId}`),new Error("download timed out")},1e3*(60+this.config.options.downloadTimeout));this.processMessage(e).catch(e=>{this.log.error(`processMessage ${String(e)}`)}).finally(()=>{clearTimeout(t),e&&delete this.downloadWorkerPool[e.MessageId]})}),this.log.info(`downloader queued ${e.Messages.length} messages for processing`),Promise.resolve()):(this.log.info("complete (empty)"),Promise.resolve())}async processMessage(e){let t,s;if(!e)return this.log.debug("download.processMessage: empty message"),Promise.resolve();"Attributes"in e&&"ApproximateReceiveCount"in e.Attributes&&this.log.debug(`download.processMessage: ${e.MessageId} / ${e.Attributes.ApproximateReceiveCount}`);try{t=JSON.parse(e.Body)}catch(n){this.log.error(`error parsing JSON message.Body from message: ${JSON.stringify(e)} ${String(n)}`);try{await this.deleteMessage(e)}catch(a){this.log.error(`Exception deleting message: ${String(a)}`)}return Promise.resolve()}if(t.telemetry){const{telemetry:s}=t;if(s.tm_path)try{this.log.debug(`download.processMessage: ${e.MessageId} fetching telemetry`);const i=await this.sessionedS3(),o=await i.getObject({Bucket:t.bucket,Key:s.tm_path}).promise();this.log.info(`download.processMessage: ${e.MessageId} fetched telemetry`),s.batch=o.Body.toString("utf-8").split("\n").filter(e=>e&&e.length>0).map(e=>{try{return JSON.parse(e)}catch(a){return this.log.error(`Telemetry Batch JSON Parse error: ${String(a)}`),e}})}catch(l){this.log.error(`Could not fetch telemetry JSON: ${String(l)}`)}try{this.telemetryLogStream.write(JSON.stringify(s)+d)}catch(u){this.log.error(`error writing telemetry: ${u}`)}this.config.options.telemetryCb&&this.config.options.telemetryCb(s)}if(!t.path)return this.log.warn("nothing to download"),Promise.resolve();const i=t.path.match(/[\w\W]*\/([\w\W]*?)$/),o=i?i[1]:"";if(s=this.config.options.outputFolder,t.telemetry&&t.telemetry.hints&&t.telemetry.hints.folder){this.log.debug(`using folder hint ${t.telemetry.hints.folder}`);const e=t.telemetry.hints.folder.split("/").map(e=>e.toUpperCase());s=p.join.apply(null,[s,...e])}c.mkdirpSync(s);const r=p.join(s,o);if("data+telemetry"===this.config.options.downloadMode){const s=[""];let i=this.config&&this.config.workflow&&this.config.workflow.settings&&this.config.workflow.settings.output_format?this.config.workflow.settings.output_format:[];("string"===typeof i||i instanceof String)&&(i=i.trim().split(/[\s,]+/));try{s.push(...i)}catch(a){this.log.error(`Failed to work out workflow file suffixes: ${String(a)}`)}try{const i=s.map(s=>{const i=t.path+s,o=r+s;return this.log.debug(`download.processMessage: ${e.MessageId} downloading ${i} to ${o}`),new Promise((r,n)=>{this.initiateDownloadStream({bucket:t.bucket,path:i},e,o).then(r).catch(e=>{this.log.error(`Caught exception waiting for initiateDownloadStream: ${String(e)}`),s?n(e):r()})})});await Promise.all(i)}catch(a){this.log.error(`Exception fetching file batch: ${String(a)}`)}try{const e=!(!t.telemetry||!t.telemetry.json)&&t.telemetry.json.exit_status;e&&this.config.options.dataCb&&this.config.options.dataCb(r,e)}catch(l){this.log.warn(`failed to fire data callback: ${l}`)}}else{const e=t.telemetry.batch_summary&&t.telemetry.batch_summary.reads_num?t.telemetry.batch_summary.reads_num:1;this.downloadState("success","incr",{files:1,reads:e})}try{await this.deleteMessage(e)}catch(a){this.log.error(`Exception deleting message: ${String(a)}`)}return Promise.resolve()}async initiateDownloadStream(t,s,i){return new Promise(async(o,r)=>{let n,a,l;try{n=await this.sessionedS3()}catch(d){r(d)}const u=e=>{if(this.log.error(`Error during stream of bucket=${t.bucket} path=${t.path} to file=${i} ${String(e)}`),clearTimeout(this.timers.transferTimeouts[i]),delete this.timers.transferTimeouts[i],!a.networkStreamError)try{a.networkStreamError=1,a.close(),c.remove(i).then(()=>{this.log.warn(`removed failed download ${i}`)}).catch(e=>{this.log.warn(`failed to remove ${i}. unlinkException: ${String(e)}`)}),l.destroy&&(this.log.error(`destroying read stream for ${i}`),l.destroy())}catch(d){this.log.error(`error handling stream error: ${String(d)}`)}};try{const e={Bucket:t.bucket,Key:t.path};a=c.createWriteStream(i);const s=n.getObject(e);s.on("httpHeaders",(e,t)=>{this.downloadState("progress","incr",{total:parseInt(t["content-length"],10)})}),l=s.createReadStream()}catch(g){return this.log.error(`getObject/createReadStream exception: ${String(g)}`),void r(g)}l.on("error",u),a.on("finish",async()=>{if(!a.networkStreamError){this.log.debug(`downloaded ${i}`);try{const t=p.extname(i),s=await M(i);this.downloadState("success","incr",e({files:1},s)),this.downloadState("types","incr",{[t]:1}),this.downloadState("progress","decr",{total:s.bytes,bytes:s.bytes})}catch(t){this.log.warn(`failed to stat ${i}: ${String(t)}`)}this.reportProgress()}}),a.on("close",e=>{this.log.debug(`closing writeStream ${i}`),e&&this.log.error(`error closing write stream ${e}`),clearInterval(this.timers.visibilityIntervals[i]),delete this.timers.visibilityIntervals[i],clearTimeout(this.timers.transferTimeouts[i]),delete this.timers.transferTimeouts[i],setTimeout(this.checkForDownloads.bind(this)),this.log.info(`download.initiateDownloadStream: ${s.MessageId} downloaded ${t.path} to ${i}`),o()}),a.on("error",u);const h=()=>{u(new Error("transfer timed out"))};this.timers.transferTimeouts[i]=setTimeout(h,1e3*this.config.options.downloadTimeout);this.timers.visibilityIntervals[i]=setInterval(async()=>{this.stopped&&(clearInterval(this.timers.visibilityIntervals[i]),delete this.timers.visibilityIntervals[i]);const e=this.config.instance.outputQueueURL,t=s.ReceiptHandle;this.log.debug({message_id:s.MessageId},"updateVisibility");try{await this.sqs.changeMessageVisibility({QueueUrl:e,ReceiptHandle:t,VisibilityTimeout:this.config.options.inFlightDelay}).promise()}catch(o){this.log.error({message_id:s.MessageId,queue:e,error:o},"Error setting visibility"),clearInterval(this.timers.visibilityIntervals[i])}},900*this.config.options.inFlightDelay),l.on("data",e=>{clearTimeout(this.timers.transferTimeouts[i]),this.timers.transferTimeouts[i]=setTimeout(h,1e3*this.config.options.downloadTimeout),this.downloadState("progress","incr",{bytes:e.length})}).pipe(a)})}async uploadHandler(e){const t=await this.sessionedS3();let s;const i=e.relative.replace(/^[\\/]+/,"").replace(/\\/g,"/").replace(/\//g,"_"),o=[this.config.instance.bucketFolder,"component-0",i,i].join("/").replace(/\/+/g,"/");let r;return new Promise((i,n)=>{const a=()=>{s&&!s.closed&&s.close(),n(new Error(`${e.name} timed out`))};r=setTimeout(a,1e3*(this.config.options.uploadTimeout+5));try{s=c.createReadStream(e.path)}catch(l){return clearTimeout(r),void n(l)}s.on("error",e=>{s.close();let t="error in upload readstream";e&&e.message&&(t+=`: ${e.message}`),clearTimeout(r),n(new Error(t))}),s.on("open",()=>{const l={Bucket:this.config.instance.bucket,Key:o,Body:s};this.config.instance.key_id&&(l.SSEKMSKeyId=this.config.instance.key_id,l.ServerSideEncryption="aws:kms"),e.size&&(l["Content-Length"]=e.size),this.uploadState("progress","incr",{total:e.size});let c=0;const u=t.upload(l,{partSize:10485760,queueSize:1});u.on("httpUploadProgress",async e=>{if(this.stopped)n(new Error("stopped"));else{this.uploadState("progress","incr",{bytes:e.loaded-c}),c=e.loaded,clearTimeout(r),r=setTimeout(a,1e3*(this.config.options.uploadTimeout+5));try{await this.session([u.service])}catch(t){this.log.warn(`Error refreshing token: ${String(t)}`)}}}),u.promise().then(()=>{this.log.info(`${e.id} S3 upload complete`),s.close(),clearTimeout(r),this.uploadComplete(o,e).then(()=>{i(e)}).catch(e=>{n(e)}).finally(()=>{this.uploadState("progress","decr",{total:e.size,bytes:e.size})})}).catch(t=>{this.log.warn(`${e.id} uploadStreamError ${t}`),n(t)})})})}async uploadComplete(e,t){this.log.info(`${t.id} uploaded to S3: ${e}`);const s={bucket:this.config.instance.bucket,outputQueue:this.config.instance.outputQueueName,remote_addr:this.config.instance.remote_addr,user_defined:this.config.instance.user_defined||null,apikey:this.config.options.apikey,id_workflow_instance:this.config.instance.id_workflow_instance,id_master:this.config.instance.id_workflow,utc:(new Date).toISOString(),path:e,prefix:e.substring(0,e.lastIndexOf("/"))};if(this.config.instance.chain)try{s.components=JSON.parse(JSON.stringify(this.config.instance.chain.components)),s.targetComponentId=this.config.instance.chain.targetComponentId}catch(i){return this.log.error(`${t.id} exception parsing components JSON ${String(i)}`),Promise.reject(i)}if(this.config.instance.key_id&&(s.key_id=this.config.instance.key_id),this.config.options.agent_address)try{s.agent_address=JSON.parse(this.config.options.agent_address)}catch(o){this.log.error(`${t.id} Could not parse agent_address ${String(o)}`)}s.components&&Object.keys(s.components).forEach(e=>{"uploadMessageQueue"===s.components[e].inputQueueName&&(s.components[e].inputQueueName=this.uploadMessageQueue),"downloadMessageQueue"===s.components[e].inputQueueName&&(s.components[e].inputQueueName=this.downloadMessageQueue)});try{const e=await this.discoverQueue(this.config.instance.inputQueueName),i=await this.sessionedSQS();this.log.info(`${t.id} sending SQS message to input queue`),await i.sendMessage({QueueUrl:e,MessageBody:JSON.stringify(s)}).promise()}catch(r){return this.log.error(`${t.id} exception sending SQS message: ${String(r)}`),Promise.reject(r)}return this.log.info(`${t.id} SQS message sent. Mark as uploaded`),this.db.uploadFile(t.path)}async fetchTelemetry(){if(!this.config||!this.config.instance||!this.config.instance.summaryTelemetry)return Promise.resolve();const e=p.join(J(),"instances"),t=p.join(e,this.config.instance.id_workflow_instance),s=[];Object.keys(this.config.instance.summaryTelemetry).forEach(e=>{const i=this.config.instance.summaryTelemetry[e]||{},o=i[Object.keys(i)[0]];if(!o)return;const r=p.join(t,`${e}.json`);s.push(this.REST.fetchContent(o).then(e=>{c.writeJSONSync(r,e),this.log.debug(`fetched telemetry summary ${r}`)}).catch(e=>{this.log.debug(`Error fetching telemetry: ${String(e)}`)}))});let i=0;try{await Promise.all(s)}catch(o){i+=1}return i&&this.log.warn("summary telemetry incomplete"),Promise.resolve()}}H.version=E.version,H.REST=N,H.utils=E,H.EPI2ME_HOME=J(),H.Profile=L;export default H;
