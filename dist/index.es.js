/**
 * Copyright Metrichor Ltd. (An Oxford Nanopore Technologies Company) 2020
 */

import e from"aws-sdk";import t from"fs-extra";import{merge as s,flatten as i,remove as o,assign as r,filter as n,every as a,isFunction as l,defaults as c,takeRight as h,isArray as u}from"lodash";import d,{homedir as p,EOL as g}from"os";import f from"path";import m from"sqlite";import w from"axios";import y from"crypto";import{httpsOverHttps as k,httpsOverHttp as S}from"tunnel";import $ from"graphql-tag";import{InMemoryCache as b}from"apollo-cache-inmemory";import _ from"apollo-client";import{ApolloLink as v,execute as E}from"apollo-link";import{createHttpLink as I}from"apollo-link-http";import{buildAxiosFetch as P}from"@lifeomic/axios-fetch";import T from"socket.io-client";import x from"recursive-readdir-async";import j from"proxy-agent";import O from"readline";import N from"zlib";var M="3.0.1215";w.defaults.validateStatus=e=>e<=504;const F=function(){const e=(e,t)=>{e.headers||(e.headers={});let s=t;if(s||(s={}),!s.apikey)return;if(e.headers["X-EPI2ME-ApiKey"]=s.apikey,!s.apisecret)return;e.headers["X-EPI2ME-SignatureDate"]=(new Date).toISOString(),e.url.match(/^https:/)&&(e.url=e.url.replace(/:443/,"")),e.url.match(/^http:/)&&(e.url=e.url.replace(/:80/,""));const i=[e.url,Object.keys(e.headers).sort().filter(e=>e.match(/^x-epi2me/i)).map(t=>`${t}:${e.headers[t]}`).join("\n")].join("\n"),o=y.createHmac("sha1",s.apisecret).update(i).digest("hex");e.headers["X-EPI2ME-SignatureV0"]=o},t=async e=>{const t=e?e.data:null;if(!t)return Promise.reject(new Error("unexpected non-json response"));if(e&&e.status>=400){let s=`Network error ${e.status}`;return t.error&&(s=t.error),504===e.status&&(s="Please check your network connection and try again."),Promise.reject(new Error(s))}return t.error?Promise.reject(new Error(t.error)):Promise.resolve(t)};return{version:"3.0.1215",headers:(t,i)=>{const{log:o}=s({log:{debug:()=>{}}},i);let r=i;if(r||(r={}),t.headers=s({Accept:"application/json","Content-Type":"application/json","X-EPI2ME-Client":r.user_agent||"api","X-EPI2ME-Version":r.agent_version||F.version},t.headers,r.headers),"signing"in r&&!r.signing||e(t,r),r.proxy){const e=r.proxy.match(/https?:\/\/((\S+):(\S+)@)?(\S+):(\d+)/),s=e[2],i=e[3],n={host:e[4],port:e[5]};s&&i&&(n.proxyAuth=`${s}:${i}`),r.proxy.match(/^https/)?(o.debug("using HTTPS over HTTPS proxy",JSON.stringify(n)),t.httpsAgent=k({proxy:n})):(o.debug("using HTTPS over HTTP proxy",JSON.stringify(n)),t.httpsAgent=S({proxy:n})),t.proxy=!1}},get:async(e,i)=>{const{log:o}=s({log:{debug:()=>{}}},i);let r,n=i.url,a=e;i.skip_url_mangle?r=a:(a=`/${a}`,n=n.replace(/\/+$/,""),a=a.replace(/\/+/g,"/"),r=n+a);const l={url:r,gzip:!0};let c;F.headers(l,i);try{o.debug(`GET ${l.url}`),c=await w.get(l.url,l)}catch(h){return Promise.reject(h)}return t(c,i)},post:async(e,i,o)=>{const{log:r}=s({log:{debug:()=>{}}},o);let n=o.url;n=n.replace(/\/+$/,"");const a={url:`${n}/${e.replace(/\/+/g,"/")}`,gzip:!0,data:i,headers:{}};if(o.legacy_form){const e=[],t=s({json:JSON.stringify(i)},i);Object.keys(t).sort().forEach(s=>{e.push(`${s}=${escape(t[s])}`)}),a.data=e.join("&"),a.headers["Content-Type"]="application/x-www-form-urlencoded"}F.headers(a,o);const{data:l}=a;let c;delete a.data;try{r.debug(`POST ${a.url}`),c=await w.post(a.url,l,a)}catch(h){return Promise.reject(h)}return o.handler?o.handler(c):t(c,o)},put:async(e,i,o,r)=>{const{log:n}=s({log:{debug:()=>{}}},r);let a=r.url;a=a.replace(/\/+$/,"");const l={url:`${a}/${e.replace(/\/+/g,"/")}/${i}`,gzip:!0,data:o,headers:{}};if(r.legacy_form){const e=[],t=s({json:JSON.stringify(o)},o);Object.keys(t).sort().forEach(s=>{e.push(`${s}=${escape(t[s])}`)}),l.data=e.join("&"),l.headers["Content-Type"]="application/x-www-form-urlencoded"}F.headers(l,r);const{data:c}=l;let h;delete l.data;try{n.debug(`PUT ${l.url}`),h=await w.put(l.url,c,l)}catch(u){return Promise.reject(u)}return t(h,r)}}}();F.pipe=async(e,s,i,o)=>{let r=i.url,n=`/${e}`;r=r.replace(/\/+$/,""),n=n.replace(/\/+/g,"/");const a={url:r+n,gzip:!0,headers:{"Accept-Encoding":"gzip",Accept:"application/gzip"}};return F.headers(a,i),i.proxy&&(a.proxy=i.proxy),o&&(a.onUploadProgress=o),a.responseType="stream",new Promise((e,i)=>{w.get(a.url,a).then(o=>{const r=t.createWriteStream(s);o.data.pipe(r),r.on("finish",()=>{e(s)}),r.on("error",e=>{i(new Error(`writer failed ${String(e)}`))})}).catch(e=>{i(e)})})};let R=0;F.getFileID=()=>(R+=1,`FILE_${R}`),F.lsRecursive=async(e,s,o)=>{let r=e;const n=t.statSync(s);if(o){if(await o(s,n))return null}return n.isDirectory()?t.readdir(s).then(e=>e.map(e=>f.join(s,e))).then(e=>Promise.all(e.map(e=>F.lsRecursive(r,e,o)))).then(e=>i(e)):(n.isFile()&&r===s&&(r=f.dirname(s)),[{name:f.parse(s).base,path:s,relative:s.replace(r,""),size:n.size,id:F.getFileID()}])},F.loadInputFiles=async({inputFolders:e,outputFolder:t,filetype:s},i,o)=>{let r=s;r instanceof Array||(r=[r]),r=r.map(e=>e&&0!==e.indexOf(".")?`.${e}`:e);const n=async(e,s)=>{const i=f.basename(e),n=[new Promise((t,s)=>"downloads"===i||"skip"===i||"fail"===i||"fastq_fail"===i||"tmp"===i?s(new Error(`${e} failed basic filename`)):t("basic ok")),new Promise((o,n)=>{const a=r.length?new RegExp(`(?:${r.join("|")})$`):null;return e.split(f.sep).filter(e=>e.match(/^[.]/)).length||t&&i===f.basename(t)||a&&!e.match(a)&&s.isFile()?n(new Error(`${e} failed extended filename`)):o("extended ok")}),o?new Promise((t,s)=>{o(e).then(i=>i?s(new Error(`${e} failed extraFilter`)):t("extra ok"))}):Promise.resolve("extra skip")];return Promise.all(n).then(()=>null).catch(()=>"exclude")};return(await Promise.all(e.map(e=>F.lsRecursive(e,e,n)))).reduce((e,t)=>[...e,...t.filter(e=>!!e)],[])},F.stripFile=e=>[f.dirname(e),f.basename(e)];class C{constructor(e,i,o){const r=s({},i);this.options=r,this.log=o;const{idWorkflowInstance:n,inputFolders:a}=r;o.debug(`setting up ${e}/db.sqlite for ${n}`),this.db=t.mkdirp(e).then(()=>(this.log.debug(`opening ${e}/db.sqlite`),m.open(f.join(e,"db.sqlite"),{Promise:Promise}).then(async t=>{this.log.debug(`opened ${e}/db.sqlite`),await t.migrate();const s=a.map(()=>"(?)").join(",");try{return await Promise.all([t.run("INSERT INTO meta (version, idWorkflowInstance) VALUES(?, ?)",M,n),t.run(`INSERT INTO folders (folder_path) VALUES ${s}`,a)]),Promise.resolve(t)}catch(i){return this.log.error(i),Promise.reject(i)}}))).catch(e=>{throw this.log.error(e),e})}async uploadFile(e){const t=await this.db,[s,i]=F.stripFile(e);return t.run("INSERT INTO uploads(filename, path_id) VALUES(?, (SELECT folder_id FROM folders WHERE folder_path = ?))",i,s)}async skipFile(e){const t=await this.db,[s,i]=F.stripFile(e);return t.run("INSERT INTO skips(filename, path_id) VALUES(?, (SELECT folder_id FROM folders WHERE folder_path = ?))",i,s)}async splitFile(e,t){const s=await this.db,[i,o]=F.stripFile(e),r=F.stripFile(t)[1];return s.run("INSERT INTO splits VALUES(?, ?, (SELECT folder_id FROM folders WHERE folder_path = ?), CURRENT_TIMESTAMP, NULL)",o,r,i)}async splitDone(e){const t=await this.db,[s,i]=F.stripFile(e);return t.run("UPDATE splits SET end=CURRENT_TIMESTAMP WHERE filename=? AND child_path_id=(SELECT folder_id FROM folders WHERE folder_path=?)",i,s)}async splitClean(){return(await this.db).all("SELECT splits.filename, folders.folder_path FROM splits INNER JOIN folders ON folders.folder_id = splits.child_path_id WHERE end IS NULL").then(e=>{if(!e)return this.log.info("no split files to clean"),Promise.resolve();this.log.info(`cleaning ${e.length} split files`),this.log.debug(`going to clean: ${e.map(e=>e.filename).join(" ")}`);const s=e.map(e=>t.unlink(f.join(e.folder_path,e.filename)).catch(()=>{}));return Promise.all(s)})}async seenUpload(e){const t=await this.db,[s,i]=F.stripFile(e);return Promise.all([t.get("SELECT * FROM uploads u INNER JOIN folders ON folders.folder_id = u.path_id WHERE u.filename=? AND folders.folder_path=? LIMIT 1",i,s),t.get("SELECT * FROM skips s INNER JOIN folders ON folders.folder_id = s.path_id WHERE s.filename=? AND folders.folder_path=? LIMIT 1",i,s)]).then(e=>o(e,void 0).length)}}var A="https://epi2me.nanoporetech.com",q={local:!1,url:A,user_agent:"EPI2ME API",region:"eu-west-1",sessionGrace:5,uploadTimeout:1200,downloadTimeout:1200,fileCheckInterval:5,downloadCheckInterval:3,stateCheckInterval:60,inFlightDelay:600,waitTimeSeconds:20,waitTokenError:30,transferPoolSize:3,downloadMode:"data+telemetry",filetype:[".fastq",".fq",".fastq.gz",".fq.gz"],signing:!0};function W(e,t,s){return t in e?Object.defineProperty(e,t,{value:s,enumerable:!0,configurable:!0,writable:!0}):e[t]=s,e}const D="\npage\npages\nhasNext\nhasPrevious\ntotalCount\n",Q="\nidWorkflowInstance\nstartDate\nworkflowImage{\n  workflow\n  {\n    rev\n    name\n  }\n}\n",z=function(){const e=(e,t)=>{e.headers||(e.headers={});let s=t;if(s||(s={}),!s.apikey||!s.apisecret)return;e.headers["X-EPI2ME-APIKEY"]=s.apikey,e.headers["X-EPI2ME-SIGNATUREDATE"]=(new Date).toISOString();const i=[Object.keys(e.headers).sort().filter(e=>e.match(/^x-epi2me/i)).map(t=>`${t}:${e.headers[t]}`).join("\n"),e.body].join("\n"),o=y.createHmac("sha1",s.apisecret).update(i).digest("hex");e.headers["X-EPI2ME-SIGNATUREV0"]=o};return{version:"3.0.1215",setHeaders:(t,i)=>{const{log:o}=s({log:{debug:()=>{}}},i);let r=i;if(r||(r={}),t.headers=s({Accept:"application/json","Content-Type":"application/json","X-EPI2ME-CLIENT":r.user_agent||"api","X-EPI2ME-VERSION":r.agent_version||z.version},t.headers,r.headers),"signing"in r&&!r.signing||e(t,r),r.proxy){const e=r.proxy.match(/https?:\/\/((\S+):(\S+)@)?(\S+):(\d+)/),s=e[2],i=e[3],n={host:e[4],port:e[5]};s&&i&&(n.proxyAuth=`${s}:${i}`),r.proxy.match(/^https/)?(o.debug("using HTTPS over HTTPS proxy",JSON.stringify(n)),t.httpsAgent=k({proxy:n})):(o.debug("using HTTPS over HTTP proxy",JSON.stringify(n)),t.httpsAgent=S({proxy:n})),t.proxy=!1}}}}(),U=P(w),J=(e,t)=>{const{apikey:s,apisecret:i}=t.headers.keys;return delete t.headers.keys,z.setHeaders(t,{apikey:s,apisecret:i,signing:!0}),U(e,t)},L=new _({link:new v(e=>{const{apikey:t,apisecret:s,url:i}=e.getContext(),o=I({uri:`${i}/graphql`,fetch:J,headers:{keys:{apikey:t,apisecret:s}}});return E(o,e)}),cache:new b});class H{constructor(e){W(this,"createContext",e=>{const{apikey:t,apisecret:i,url:o}=this.options;return s({apikey:t,apisecret:i,url:o},e)}),W(this,"query",e=>({context:t={},variables:s={}}={})=>{const i=this.createContext(t);let o;return o="string"===typeof e?$`
        ${e}
      `:e,this.client.query({query:o,variables:s,context:i})}),W(this,"mutate",e=>({context:t={},variables:s={}}={})=>{const i=this.createContext(t);let o;return o="string"===typeof e?$`
        ${e}
      `:e,this.client.mutate({mutation:o,variables:s,context:i})}),W(this,"workflows",this.query($`
    query allWorkflows($page: Int, $isActive: Int) {
      allWorkflows(page: $page, isActive: $isActive) {
        ${D}
        results {
          ${"\nidWorkflow\nname\ndescription\nsummary\nrev\n"}
        }
      }
    }
  `)),W(this,"workflowPages",async e=>{let t=e,s=await this.workflows({variables:{page:t}});const i=async e=>(t=e,s=await this.workflows({variables:{page:t}}),s);return{data:s,next:()=>i(t+1),previous:()=>i(t-1),first:()=>i(1),last:()=>i(0)}}),W(this,"workflow",this.query($`
    query workflow($idWorkflow: ID!) {
      workflow(idWorkflow: $idWorkflow) {
        ${"\nidWorkflow\nname\ndescription\nsummary\nrev\n"}
      }
    }
   `)),W(this,"workflowInstances",this.query($`
  query allWorkflowInstances($page: Int, $shared: Boolean, $idUser: Int) {
    allWorkflowInstances(page: $page, shared: $shared, idUser: $idUser) {
      ${D}
      results {
        ${Q}
      }
    }
  }
   `)),W(this,"workflowInstance",this.query($`
      query workflowInstance($idWorkflowInstance: ID!) {
        workflowInstance(idWorkflowInstance: $idWorkflowInstance) {
          ${Q}
        }
      }
   `)),W(this,"startWorkflow",this.mutate($`
    mutation startWorkflow(
      $idWorkflow: ID!
      $computeAccountId: Int!
      $storageAccountId: Int
      $isConsentedHuman: Int = 0
    ) {
      startWorkflowInstance(
        idWorkflow: $idWorkflow
        computeAccountId: $computeAccountId
        storageAccountId: $storageAccountId
        isConsentedHuman: $isConsentedHuman
      ) {
        bucket
        idUser
        idWorkflowInstance
        inputqueue
        outputqueue
        region
        keyId
        chain
      }
    }
  `)),W(this,"user",this.query($`
    query user {
      me {
        username
        realname
        useraccountSet {
          idUserAccount
        }
      }
    }
  `)),W(this,"register",this.mutate($`
    mutation registerToken($code: String!, $description: String) {
      registerToken(code: $code, descripton: $description) {
        apikey
        apisecret
        description
      }
    }
  `)),this.options=r({agent_version:F.version,local:!1,url:A,user_agent:"EPI2ME API",signing:!0},e),this.options.url=this.options.url.replace(/:\/\//,"://graphql."),this.log=this.options.log,this.client=L}}const G=(e,t)=>{const s=["","K","M","G","T","P","E","Z"];let i=t||0,o=e||0;return o>=1e3?(o/=1e3,i+=1,i>=s.length?"???":G(o,i)):0===i?`${o}${s[i]}`:`${o.toFixed(1)}${s[i]}`};class B{constructor(e,t){this.allProfileData={},this.defaultEndpoint=process.env.METRICHOR||q.endpoint||q.url,this.raiseExceptions=t,e&&(this.allProfileData=s(e,{profiles:{}})),this.allProfileData.endpoint&&(this.defaultEndpoint=this.allProfileData.endpoint)}profile(e){return e?s({endpoint:this.defaultEndpoint},this.allProfileData.profiles[e]):{}}profiles(){return Object.keys(this.allProfileData.profiles||{})}}class V{constructor(e){this.options=r({agent_version:F.version,local:!1,url:A,user_agent:"EPI2ME API",signing:!0},e),this.log=this.options.log}async list(e){const t=e.match(/^[a-z_]+/i)[0];return F.get(e,this.options).then(e=>e[`${t}s`])}async read(e,t){return F.get(`${e}/${t}`,this.options)}async user(){return this.options.local?{accounts:[{id_user_account:"none",number:"NONE",name:"None"}]}:F.get("user",this.options)}async status(){return F.get("status",this.options)}async jwt(){return F.post("authenticate",{},s({handler:e=>e.headers["x-epi2me-jwt"]?Promise.resolve(e.headers["x-epi2me-jwt"]):Promise.reject(new Error("failed to fetch JWT"))},this.options))}async instanceToken(e,t){return F.post("token",s(t,{id_workflow_instance:e}),r({},this.options,{legacy_form:!0}))}async installToken(e){return F.post("token/install",{id_workflow:e},r({},this.options,{legacy_form:!0}))}async attributes(){return this.list("attribute")}async workflows(){return this.list("workflow")}async amiImages(){if(this.options.local)throw new Error("amiImages unsupported in local mode");return this.list("ami_image")}async amiImage(e,t){let s,i,o;if(e&&t instanceof Object?(s=e,i=t,o="update"):e instanceof Object&&!t?(i=e,o="create"):(o="read",s=e),this.options.local)throw new Error("ami_image unsupported in local mode");if("update"===o)return F.put("ami_image",s,i,this.options);if("create"===o)return F.post("ami_image",i,this.options);if(!s)throw new Error("no id_ami_image specified");return this.read("ami_image",s)}async workflow(e,t,i){let o,r,a,l;if(e&&t&&i instanceof Function?(o=e,r=t,a=i,l="update"):e&&t instanceof Object&&!(t instanceof Function)?(o=e,r=t,l="update"):e instanceof Object&&t instanceof Function?(r=e,a=t,l="create"):e instanceof Object&&!t?(r=e,l="create"):(l="read",o=e,a=t instanceof Function?t:null),"update"===l)try{const e=await F.put("workflow",o,r,this.options);return a?a(null,e):Promise.resolve(e)}catch(d){return a?a(d):Promise.reject(d)}if("create"===l)try{const e=await F.post("workflow",r,this.options);return a?a(null,e):Promise.resolve(e)}catch(d){return a?a(d):Promise.reject(d)}if(!o){const e=new Error("no workflow id specified");return a?a(e):Promise.reject(e)}const c={};try{const e=await this.read("workflow",o);if(e.error)throw new Error(e.error);s(c,e)}catch(d){return this.log.error(`${o}: error fetching workflow ${String(d)}`),a?a(d):Promise.reject(d)}s(c,{params:{}});try{const e=await F.get(`workflow/config/${o}`,this.options);if(e.error)throw new Error(e.error);s(c,e)}catch(d){return this.log.error(`${o}: error fetching workflow config ${String(d)}`),a?a(d):Promise.reject(d)}const h=n(c.params,{widget:"ajax_dropdown"}),u=[...h.map((e,t)=>{const s=h[t];return new Promise((e,t)=>{const i=s.values.source.replace("{{EPI2ME_HOST}}","").replace(/&?apikey=\{\{EPI2ME_API_KEY\}\}/,"");F.get(i,this.options).then(t=>{const i=t[s.values.data_root];return i&&(s.values=i.map(e=>({label:e[s.values.items.label_key],value:e[s.values.items.value_key]}))),e()}).catch(e=>(this.log.error(`failed to fetch ${i}`),t(e)))})})];try{return await Promise.all(u),a?a(null,c):Promise.resolve(c)}catch(d){return this.log.error(`${o}: error fetching config and parameters ${String(d)}`),a?a(d):Promise.reject(d)}}async startWorkflow(e){return F.post("workflow_instance",e,r({},this.options,{legacy_form:!0}))}async stopWorkflow(e){return F.put("workflow_instance/stop",e,null,r({},this.options,{legacy_form:!0}))}async workflowInstances(e){return e&&e.run_id?F.get(`workflow_instance/wi?show=all&columns[0][name]=run_id;columns[0][searchable]=true;columns[0][search][regex]=true;columns[0][search][value]=${e.run_id};`,this.options).then(e=>e.data.map(e=>({id_workflow_instance:e.id_ins,id_workflow:e.id_flo,run_id:e.run_id,description:e.desc,rev:e.rev}))):this.list("workflow_instance")}async workflowInstance(e){return this.read("workflow_instance",e)}async workflowConfig(e){return F.get(`workflow/config/${e}`,this.options)}async register(e,t){return F.put("reg",e,{description:t||`${d.userInfo().username}@${d.hostname()}`},r({},this.options,{signing:!1}))}async datasets(e){let t=e;return t||(t={}),t.show||(t.show="mine"),this.list(`dataset?show=${t.show}`)}async dataset(e){return this.options.local?this.datasets().then(t=>t.find(t=>t.id_dataset===e)):this.read("dataset",e)}async fetchContent(e){const t=r({},this.options,{skip_url_mangle:!0,headers:{"Content-Type":""}});return F.get(e,t)}}class K{constructor(e,t){this.debounces={},this.debounceWindow=s({debounceWindow:2e3},t).debounceWindow,this.log=s({log:{debug:()=>{}}},t).log,e.jwt().then(e=>{this.socket=T(t.url,{transportOptions:{polling:{extraHeaders:{Cookie:`x-epi2me-jwt=${e}`}}}}),this.socket.on("connect",()=>{this.log.debug("socket ready")})})}debounce(e,t){const i=s(e)._uuid;if(i){if(this.debounces[i])return;this.debounces[i]=1,setTimeout(()=>{delete this.debounces[i]},this.debounceWindow)}t&&t(e)}watch(e,t){if(!this.socket)return this.log.debug(`socket not ready. requeueing watch on ${e}`),void setTimeout(()=>{this.watch(e,t)},1e3);this.socket.on(e,e=>this.debounce(e,t))}emit(e,t){if(!this.socket)return this.log.debug(`socket not ready. requeueing emit on ${e}`),void setTimeout(()=>{this.emit(e,t)},1e3);this.log.debug(`socket emit ${e} ${JSON.stringify(t)}`),this.socket.emit(e,t)}}class X{constructor(e){let t;if(t="string"===typeof e||"object"===typeof e&&e.constructor===String?JSON.parse(e):e||{},t.endpoint&&(t.url=t.endpoint,delete t.endpoint),t.log){if(!a([t.log.info,t.log.warn,t.log.error,t.log.debug,t.log.json],l))throw new Error("expected log object to have error, debug, info, warn and json methods");this.log=t.log}else this.log={info:e=>{console.info(`[${(new Date).toISOString()}] INFO: ${e}`)},debug:e=>{console.debug(`[${(new Date).toISOString()}] DEBUG: ${e}`)},warn:e=>{console.warn(`[${(new Date).toISOString()}] WARN: ${e}`)},error:e=>{console.error(`[${(new Date).toISOString()}] ERROR: ${e}`)},json:e=>{console.log(JSON.stringify(e))}};this.stopped=!0,this.states={upload:{filesCount:0,success:{files:0,bytes:0,reads:0},types:{},niceTypes:"",progress:{bytes:0,total:0}},download:{progress:{},success:{files:0,reads:0,bytes:0},fail:0,types:{},niceTypes:""},warnings:[]},this.config={options:c(t,q),instance:{id_workflow_instance:t.id_workflow_instance,inputQueueName:null,outputQueueName:null,outputQueueURL:null,discoverQueueCache:{},bucket:null,bucketFolder:null,remote_addr:null,chain:null,key_id:null}},this.config.instance.awssettings={region:this.config.options.region},this.REST=new V(s({log:this.log},this.config.options)),this.graphQL=new H(s({log:this.log},this.config.options)),this.timers={downloadCheckInterval:null,stateCheckInterval:null,fileCheckInterval:null,transferTimeouts:{},visibilityIntervals:{},summaryTelemetryInterval:null}}async socket(){return this.mySocket?this.mySocket:(this.mySocket=new K(this.REST,s({log:this.log},this.config.options)),this.mySocket)}async realtimeFeedback(e,t){(await this.socket()).emit(e,t)}async stopEverything(){this.stopped=!0,this.log.debug("stopping watchers"),["downloadCheckInterval","stateCheckInterval","fileCheckInterval","summaryTelemetryInterval"].forEach(e=>{this.timers[e]&&(this.log.debug(`clearing ${e} interval`),clearInterval(this.timers[e]),this.timers[e]=null)}),Object.keys(this.timers.transferTimeouts).forEach(e=>{this.log.debug(`clearing transferTimeout for ${e}`),clearTimeout(this.timers.transferTimeouts[e]),delete this.timers.transferTimeouts[e]}),Object.keys(this.timers.visibilityIntervals).forEach(e=>{this.log.debug(`clearing visibilityInterval for ${e}`),clearInterval(this.timers.visibilityIntervals[e]),delete this.timers.visibilityIntervals[e]}),this.downloadWorkerPool&&(this.log.debug("clearing downloadWorkerPool"),await Promise.all(Object.values(this.downloadWorkerPool)),this.downloadWorkerPool=null);const{id_workflow_instance:e}=this.config.instance;if(e){try{await this.REST.stopWorkflow(e)}catch(t){return this.log.error(`Error stopping instance: ${String(t)}`),Promise.reject(t)}this.log.info(`workflow instance ${e} stopped`)}return Promise.resolve()}reportProgress(){const{upload:e,download:t}=this.states;this.log.json({progress:{download:t,upload:e}})}storeState(e,t,s,i){const o=i||{};this.states[e]||(this.states[e]={}),this.states[e][t]||(this.states[e][t]={}),"incr"===s?Object.keys(o).forEach(s=>{this.states[e][t][s]=this.states[e][t][s]?this.states[e][t][s]+parseInt(o[s],10):parseInt(o[s],10)}):Object.keys(o).forEach(s=>{this.states[e][t][s]=this.states[e][t][s]?this.states[e][t][s]-parseInt(o[s],10):-parseInt(o[s],10)});try{this.states[e].success.niceReads=G(this.states[e].success.reads)}catch(n){this.states[e].success.niceReads=0}try{this.states[e].progress.niceSize=G(this.states[e].success.bytes+this.states[e].progress.bytes||0)}catch(n){this.states[e].progress.niceSize=0}try{this.states[e].success.niceSize=G(this.states[e].success.bytes)}catch(n){this.states[e].success.niceSize=0}this.states[e].niceTypes=Object.keys(this.states[e].types||{}).sort().map(t=>`${this.states[e].types[t]} ${t}`).join(", ");const r=Date.now();(!this.stateReportTime||r-this.stateReportTime>2e3)&&(this.stateReportTime=r,this.reportProgress())}uploadState(e,t,s){return this.storeState("upload",e,t,s)}downloadState(e,t,s){return this.storeState("download",e,t,s)}url(){return this.config.options.url}apikey(){return this.config.options.apikey}attr(e,t){if(!(e in this.config.options))throw new Error(`config object does not contain property ${e}`);return t?(this.config.options[e]=t,this):this.config.options[e]}stats(e){return this.states[e]}}X.version=F.version,X.Profile=B,X.REST=V,X.utils=F;const Y={fastq:function(e){return new Promise((s,i)=>{let o,r=1,n={size:0};try{n=t.statSync(e)}catch(a){return void i(a)}t.createReadStream(e).on("data",e=>{o=-1,r-=1;do{o=e.indexOf(10,o+1),r+=1}while(-1!==o)}).on("end",()=>s({type:"fastq",bytes:n.size,reads:Math.floor(r/4)})).on("error",i)})},fasta:function(e){return new Promise((s,i)=>{let o,r=1,n={size:0};try{n=t.statSync(e)}catch(a){i(a)}t.createReadStream(e).on("data",e=>{o=-1,r-=1;do{o=e.indexOf(62,o+1),r+=1}while(-1!==o)}).on("end",()=>s({type:"fasta",bytes:n.size,sequences:Math.floor((1+r)/2)})).on("error",i)})},default:async function(e){return t.stat(e).then(e=>({type:"bytes",bytes:e.size}))}};function Z(e){if("string"!==typeof e&&!(e instanceof String))return Promise.resolve({});let t=f.extname(e).toLowerCase().replace(/^[.]/,"");return"fq"===t?t="fastq":"fa"===t&&(t="fasta"),Y[t]||(t="default"),Y[t](e)}class ee extends B{constructor(e,i){super({},i),this.prefsFile=e||ee.profilePath(),this.allProfileData={};try{this.allProfileData=s(t.readJSONSync(this.prefsFile),{profiles:{}}),this.allProfileData.endpoint&&(this.defaultEndpoint=this.allProfileData.endpoint)}catch(o){if(this.raiseExceptions)throw o}}static profilePath(){return f.join(p(),".epi2me.json")}profile(e,i){if(e&&i){s(this.allProfileData,{profiles:{[e]:i}});try{t.writeJSONSync(this.prefsFile,this.allProfileData)}catch(o){if(this.raiseExceptions)throw o}}return e?s({endpoint:this.defaultEndpoint},this.allProfileData.profiles[e]):{}}}class te{static MakeQueryablePromise(e){if(e.isResolved)return e;let t=!0,s=!1,i=!1;const o=e.then(e=>(i=!0,t=!1,e)).catch(e=>{throw s=!0,t=!1,e});return o.dependsOn=e,o.isResolved=()=>i,o.isPending=()=>t,o.isRejected=()=>s,o}constructor(e){const t=s({bandwidth:1,interval:500},e);this.bandwidth=t.bandwidth,this.interval=t.interval,this.pipeline=[],this.running=[],this.completed=0,this.intervalId=null,"start"in t&&!t.start||this.start()}enqueue(e){this.pipeline.push(e)}start(){this.intervalId||(this.intervalId=setInterval(()=>{this.monitorInterval()},this.interval))}stop(){clearInterval(this.intervalId),delete this.intervalId}state(){return{queued:this.pipeline.length,running:this.running.length,completed:this.completed,state:this.intervalId?"running":"stopped"}}monitorInterval(){this.running.map((e,t)=>e.isPending()?null:t).filter(e=>e).reverse().forEach(e=>{this.running.splice(e,1),this.completed+=1});const e=this.bandwidth-this.running.length;for(let t=0;t<e;t+=1){const e=this.pipeline.shift();if(!e)return;this.running.push(te.MakeQueryablePromise(e()))}}}class se extends V{async workflows(e){if(!this.options.local)return super.workflows(e);const s=f.join(this.options.url,"workflows");let i;try{return i=(await t.readdir(s)).filter(e=>t.statSync(f.join(s,e)).isDirectory()).map(e=>f.join(s,e,"workflow.json")).map(e=>t.readJsonSync(e)),e?e(null,i):Promise.resolve(i)}catch(o){return this.log.warn(o),e?e(void 0):Promise.reject(void 0)}}async workflow(e,s,i){if(!this.options.local||!e||"object"===typeof e||i)return super.workflow(e,s,i);const o=f.join(this.options.url,"workflows"),r=f.join(o,e,"workflow.json");try{const e=await t.readJson(r);return i?i(null,e):Promise.resolve(e)}catch(n){return i?i(n):Promise.reject(n)}}async workflowInstances(e,s){if(!this.options.local)return super.workflowInstances(e,s);let i,o;if(!e||e instanceof Function||void 0!==s?(i=e,o=s):o=e,o){const e=new Error("querying of local instances unsupported in local mode");return i?i(e):Promise.reject(e)}const r=f.join(this.options.url,"instances");try{let e=await t.readdir(r);return e=e.filter(e=>t.statSync(f.join(r,e)).isDirectory()),e=e.map(e=>{const s=f.join(r,e,"workflow.json");let i;try{i=t.readJsonSync(s)}catch(o){i={id_workflow:"-",description:"-",rev:"0.0"}}return i.id_workflow_instance=e,i.filename=s,i}),i?i(null,e):Promise.resolve(e)}catch(n){return i?i(n):Promise.reject(n)}}async datasets(e,s){if(!this.options.local)return super.datasets(e,s);let i,o;if(!e||e instanceof Function||void 0!==s?(i=e,o=s):o=e,o||(o={}),o.show||(o.show="mine"),"mine"!==o.show)return i(new Error("querying of local datasets unsupported in local mode"));const r=f.join(this.options.url,"datasets");try{let e=await t.readdir(r);e=e.filter(e=>t.statSync(f.join(r,e)).isDirectory());let s=0;return e=e.sort().map(e=>(s+=1,{is_reference_dataset:!0,summary:null,dataset_status:{status_label:"Active",status_value:"active"},size:0,prefix:e,id_workflow_instance:null,id_account:null,is_consented_human:null,data_fields:null,component_id:null,uuid:e,is_shared:!1,id_dataset:s,id_user:null,last_modified:null,created:null,name:e,source:e,attributes:null})),i?i(null,e):Promise.resolve(e)}catch(n){return this.log.warn(n),i?i(null,[]):Promise.resolve([])}}async bundleWorkflow(e,t,s){return F.pipe(`workflow/bundle/${e}.tar.gz`,t,this.options,s)}}class ie{constructor(e){this.experiments={},this.options=r({path:"/data"},e)}async getExperiments(e=!1){return Object.keys(this.experiments).length&&!e||await this.updateExperiments(),this.experiments}async updateExperiments(){const e=await x.list(this.options.path,{include:["sequencing_summary"]});this.experiments={},e.forEach(e=>{const[t,s]=h(e.path.split(f.sep),2),i=s.split("_"),o=h(i,2)[0],[r,n]=i.slice(0,2),a=`${r.slice(0,4)}-${r.slice(4,6)}-${r.slice(6,8)}`,l=`T${n.slice(0,2)}:${n.slice(2,4)}:00`,c=new Date(a+l);this.experiments[t]={startDate:`${c.toDateString()} ${c.toLocaleTimeString()}`,samples:[...this.experiments[t]?this.experiments[t].samples:[],{sample:s,flowcell:o,path:`${e.path}/fastq_pass`}]}})}}class oe{constructor(e,t,i,o){if(this.id_workflow_instance=e,this.children=i,this.options=s(o),this.log=this.options.log,this.REST=t,!e)throw new Error("must specify id_workflow_instance");if(!i||!i.length)throw new Error("must specify children to session")}async session(){if(this.sts_expiration&&this.sts_expiration>Date.now())return Promise.resolve();this.log.debug("new instance token needed");try{const e=await this.REST.instanceToken(this.id_workflow_instance,this.options);this.log.debug(`allocated new instance token expiring at ${e.expiration}`),this.sts_expiration=new Date(e.expiration).getTime()-60*parseInt(this.options.sessionGrace||"0",10);const t={};this.options.proxy&&s(t,{httpOptions:{agent:j(this.options.proxy,!0)}}),s(t,{region:this.options.region},e),this.children.forEach(e=>{try{e.config.update(t)}catch(s){this.log.warn(`failed to update config on ${String(e)}: ${String(s)}`)}})}catch(e){this.log.warn(`failed to fetch instance token: ${String(e)}`)}return Promise.resolve()}}async function re(e,i,o,r,n,a){const{maxChunkBytes:l,maxChunkReads:c}=s({},i),h=f.dirname(e),u=f.basename(e),d=u.match(/^[^.]+/)[0],p=u.replace(d,""),g=f.join(h,d);if(!l&&!c)return o(e).then(()=>({source:e,split:!1,chunks:[e]}));const m=await t.stat(e);return l&&m.size<l?o(e).then(()=>({source:e,split:!1,chunks:[e]})):new Promise(i=>{let h,u,d=0,f=0,m="",w=0,y=0;const k={source:e,split:!0,chunks:[]};let S;const $=[new Promise(e=>{S=e})];O.createInterface({input:n(e)}).on("line",async e=>{f+=1,m+=e,m+="\n",f>=4&&(f=0,(async e=>{if(!w){d+=1,h=`${g}_${d}${p}`;const e=new Promise((e,s)=>{const i=h,n=()=>{o(i).then(()=>{e(i)}).catch(e=>{s(e)}).finally(()=>{t.unlink(i).catch(e=>{r.warn(`Error unlinking chunk ${i}: ${String(e)}`)})})};a?u=a(i,n):(u=t.createWriteStream(i),u.on("close",n))});$.push(e)}w+=1,y+=e.length,u.write(e,()=>{}),(l&&y>=l||c&&w>=c)&&(w=0,y=0,u.end())})(m),m="")}).on("close",()=>{u.end(),S(),Promise.all($).then(e=>{e.shift(),i(s({chunks:e},k))})}).on("error",t=>{r.error(`Error chunking ${e}: ${String(t)}`)})})}async function ne(e,s,i,o){return re(e,s,i,o,e=>t.createReadStream(e))}async function ae(e,s,i,o){return re(e,s,i,o,e=>t.createReadStream(e).pipe(N.createGunzip()),(e,s)=>{const i=t.createWriteStream(e);i.on("close",s);const o=N.createGzip();return o.pipe(i),o})}const le=()=>{const e=process.env.APPDATA||("darwin"===process.platform?f.join(p(),"Library/Application Support"):p());return process.env.EPI2ME_HOME||f.join(e,"linux"===process.platform?".epi2me":"EPI2ME")};class ce extends X{constructor(e){super(e),this.config.options.inputFolders=this.config.options.inputFolders||[],this.config.options.inputFolder&&this.config.options.inputFolders.push(this.config.options.inputFolder),this.REST=new se(s({},{log:this.log},this.config.options)),this.SampleReader=new ie}async sessionedS3(){return await this.sessionManager.session(),new e.S3({useAccelerateEndpoint:"on"===this.config.options.awsAcceleration})}async sessionedSQS(){return await this.sessionManager.session(),new e.SQS}async deleteMessage(e){try{const t=await this.discoverQueue(this.config.instance.outputQueueName);return(await this.sessionedSQS()).deleteMessage({QueueUrl:t,ReceiptHandle:e.ReceiptHandle}).promise()}catch(t){return this.log.error(`deleteMessage exception: ${String(t)}`),this.states.download.failure||(this.states.download.failure={}),this.states.download.failure[t]=this.states.download.failure[t]?this.states.download.failure[t]+1:1,Promise.reject(t)}}async discoverQueue(e){if(this.config.instance.discoverQueueCache[e])return Promise.resolve(this.config.instance.discoverQueueCache[e]);let t;this.log.debug(`discovering queue for ${e}`);try{const s=await this.sessionedSQS();t=await s.getQueueUrl({QueueName:e}).promise()}catch(s){return this.log.error(`Error: failed to find queue for ${e}: ${String(s)}`),Promise.reject(s)}return this.log.debug(`found queue ${t.QueueUrl}`),this.config.instance.discoverQueueCache[e]=t.QueueUrl,Promise.resolve(t.QueueUrl)}async queueLength(e){if(!e)return Promise.reject(new Error("no queueURL specified"));const t=e.match(/([\w\-_]+)$/)[0];this.log.debug(`querying queue length of ${t}`);try{const t=await this.sessionedSQS(),s=await t.getQueueAttributes({QueueUrl:e,AttributeNames:["ApproximateNumberOfMessages"]}).promise();if(s&&s.Attributes&&"ApproximateNumberOfMessages"in s.Attributes){let e=s.Attributes.ApproximateNumberOfMessages;return e=parseInt(e,10)||0,Promise.resolve(e)}return Promise.reject(new Error("unexpected response"))}catch(s){return this.log.error(`error in getQueueAttributes ${String(s)}`),Promise.reject(s)}}async autoStart(e,t){let s;this.stopped=!1;try{s=await this.REST.startWorkflow(e)}catch(i){const e=`Failed to start workflow: ${String(i)}`;return this.log.warn(e),t?t(e):Promise.reject(i)}return this.config.workflow=JSON.parse(JSON.stringify(e)),this.log.info(`instance ${JSON.stringify(s)}`),this.log.info(`workflow config ${JSON.stringify(this.config.workflow)}`),this.autoConfigure(s,t)}async autoJoin(e,t){let s;this.stopped=!1,this.config.instance.id_workflow_instance=e;try{s=await this.REST.workflowInstance(e)}catch(i){const e=`Failed to join workflow instance: ${String(i)}`;return this.log.warn(e),t?t(e):Promise.reject(i)}return"stopped"===s.state?(this.log.warn(`workflow ${e} is already stopped`),t?t("could not join workflow"):Promise.reject(new Error("could not join workflow"))):(this.config.workflow=this.config.workflow||{},this.log.debug(`instance ${JSON.stringify(s)}`),this.log.debug(`workflow config ${JSON.stringify(this.config.workflow)}`),this.autoConfigure(s,t))}initSessionManager(t,i){return new oe(this.config.instance.id_workflow_instance,this.REST,[e,...i||[]],s({sessionGrace:this.config.options.sessionGrace,proxy:this.config.options.proxy,region:this.config.instance.region,log:this.log},t))}async autoConfigure(e,s){if(["id_workflow_instance","id_workflow","remote_addr","key_id","bucket","user_defined","start_date","id_user"].forEach(t=>{this.config.instance[t]=e[t]}),this.config.instance.inputQueueName=e.inputqueue,this.config.instance.outputQueueName=e.outputqueue,this.config.instance.region=e.region||this.config.options.region,this.config.instance.bucketFolder=`${e.outputqueue}/${e.id_user}/${e.id_workflow_instance}`,this.config.instance.summaryTelemetry=e.telemetry,e.chain)if("object"===typeof e.chain)this.config.instance.chain=e.chain;else try{this.config.instance.chain=JSON.parse(e.chain)}catch(l){throw new Error(`exception parsing chain JSON ${String(l)}`)}if(!this.config.options.inputFolders.length)throw new Error("must set inputFolder");if(!this.config.options.outputFolder)throw new Error("must set outputFolder");if(!this.config.instance.bucketFolder)throw new Error("bucketFolder must be set");if(!this.config.instance.inputQueueName)throw new Error("inputQueueName must be set");if(!this.config.instance.outputQueueName)throw new Error("outputQueueName must be set");t.mkdirpSync(this.config.options.outputFolder);const i=f.join(le(),"instances"),o=f.join(i,this.config.instance.id_workflow_instance);this.db=new C(o,{idWorkflowInstance:this.config.instance.id_workflow_instance,inputFolders:this.config.options.inputFolders},this.log);const r=this.config.instance.id_workflow_instance?`telemetry-${this.config.instance.id_workflow_instance}.log`:"telemetry.log",n=f.join(this.config.options.outputFolder,"epi2me-logs"),a=f.join(n,r);return t.mkdirp(n,e=>{if(e&&!String(e).match(/EEXIST/))this.log.error(`error opening telemetry log stream: mkdirpException:${String(e)}`);else try{this.telemetryLogStream=t.createWriteStream(a,{flags:"a"}),this.log.info(`logging telemetry to ${a}`)}catch(s){this.log.error(`error opening telemetry log stream: ${String(s)}`)}}),s&&s(null,this.config.instance),this.timers.summaryTelemetryInterval=setInterval(()=>{this.stopped?clearInterval(this.timers.summaryTelemetryInterval):this.fetchTelemetry()},1e4*this.config.options.downloadCheckInterval),this.timers.downloadCheckInterval=setInterval(()=>{this.stopped?clearInterval(this.timers.downloadCheckInterval):this.checkForDownloads()},1e3*this.config.options.downloadCheckInterval),this.timers.stateCheckInterval=setInterval(async()=>{if(this.stopped)clearInterval(this.timers.stateCheckInterval);else try{const t=await this.REST.workflowInstance(this.config.instance.id_workflow_instance);if("stopped"===t.state){this.log.warn(`instance was stopped remotely at ${t.stop_date}. shutting down the workflow.`);try{const e=await this.stopEverything();"function"===typeof e.config.options.remoteShutdownCb&&e.config.options.remoteShutdownCb(`instance was stopped remotely at ${t.stop_date}`)}catch(e){this.log.error(`Error whilst stopping: ${String(e)}`)}}}catch(t){this.log.warn(`failed to check instance state: ${t&&t.error?t.error:t}`)}},1e3*this.config.options.stateCheckInterval),this.sessionManager=this.initSessionManager(),await this.sessionManager.session(),this.reportProgress(),this.loadUploadFiles(),this.timers.fileCheckInterval=setInterval(this.loadUploadFiles.bind(this),1e3*this.config.options.fileCheckInterval),Promise.resolve(e)}async stopEverything(){return await super.stopEverything(),delete this.sessionManager,this.log.debug("clearing split files"),this.db?this.db.splitClean():Promise.resolve()}async checkForDownloads(){if(this.checkForDownloadsRunning)return Promise.resolve();this.checkForDownloadsRunning=!0,this.log.debug("checkForDownloads checking for downloads");try{const e=await this.discoverQueue(this.config.instance.outputQueueName),t=await this.queueLength(e);t?(this.log.debug(`downloads available: ${t}`),await this.downloadAvailable()):this.log.debug("no downloads available")}catch(e){this.log.warn(`checkForDownloads error ${String(e)}`),this.states.download.failure||(this.states.download.failure={}),this.states.download.failure[e]=this.states.download.failure[e]?this.states.download.failure[e]+1:1}return this.checkForDownloadsRunning=!1,Promise.resolve()}async downloadAvailable(){const e=Object.keys(this.downloadWorkerPool||{}).length;if(e>=this.config.options.transferPoolSize)return this.log.debug(`${e} downloads already queued`),Promise.resolve();let t;try{const s=await this.discoverQueue(this.config.instance.outputQueueName);this.log.debug("fetching messages");const i=await this.sessionedSQS();t=await i.receiveMessage({AttributeNames:["All"],QueueUrl:s,VisibilityTimeout:this.config.options.inFlightDelay,MaxNumberOfMessages:this.config.options.transferPoolSize-e,WaitTimeSeconds:this.config.options.waitTimeSeconds}).promise()}catch(s){return this.log.error(`receiveMessage exception: ${String(s)}`),this.states.download.failure[s]=this.states.download.failure[s]?this.states.download.failure[s]+1:1,Promise.reject(s)}return this.receiveMessages(t)}async loadUploadFiles(){if(this.dirScanInProgress)return Promise.resolve();this.dirScanInProgress=!0,this.log.debug("upload: started directory scan");try{const e=e=>this.db.seenUpload(e),t=await F.loadInputFiles(this.config.options,this.log,e);let s=0;const i=()=>new Promise(e=>{if(this.stopped)return t.length=0,this.log.debug("upload: skipping, stopped"),void e();if(s>this.config.options.transferPoolSize)return void setTimeout(e,1e3);const i=t.splice(0,this.config.options.transferPoolSize-s);s+=i.length,this.enqueueUploadFiles(i).then().catch(e=>{this.log.error(`upload: exception in enqueueUploadFiles: ${String(e)}`)}).finally(()=>{s-=i.length,e()})});for(;t.length;)await i()}catch(e){this.log.error(`upload: exception in loadInputFiles: ${String(e)}`)}return this.dirScanInProgress=!1,this.log.debug("upload: finished directory scan"),Promise.resolve()}async enqueueUploadFiles(e){let t=0,s=0,i=0,o=0,r={};if(!u(e)||!e.length)return Promise.resolve();if(this.log.info(`enqueueUploadFiles ${e.length} files: ${e.map(e=>e.path).join(" ")}.`),"workflow"in this.config)if("workflow_attributes"in this.config.workflow)r=this.config.workflow.workflow_attributes;else if("attributes"in this.config.workflow){let{attributes:e}=this.config.workflow;if(e||(e={}),["max_size","max_files","split_size","split_reads"].forEach(t=>{`epi2me:${t}`in e&&(r[t]=parseInt(e[`epi2me:${t}`],10))}),"epi2me:category"in e){e["epi2me:category"].includes("storage")&&(r.requires_storage=!0)}}if(this.log.info(`enqueueUploadFiles settings ${JSON.stringify(r)}`),"requires_storage"in r&&r.requires_storage&&!("storage_account"in this.config.workflow)){const e={msg:"ERROR: Workflow requires storage enabled. Please provide a valid storage account [ --storage ].",type:"WARNING_STORAGE_ENABLED"};return this.log.error(e.msg),this.states.warnings.push(e),Promise.resolve()}if("split_size"in r&&(i=parseInt(r.split_size,10),this.log.info(`enqueueUploadFiles splitting supported files at ${i} bytes`)),"split_reads"in r&&(o=parseInt(r.split_reads,10),this.log.info(`enqueueUploadFiles splitting supported files at ${o} reads`)),"max_size"in r&&(s=parseInt(r.max_size,10),this.log.info(`enqueueUploadFiles restricting file size to ${s}`)),"max_files"in r&&(t=parseInt(r.max_files,10),this.log.info(`enqueueUploadFiles restricting file count to ${t}`),e.length>t)){const s={msg:`ERROR: ${e.length} files found. Workflow can only accept ${t}. Please move the extra files away.`,type:"WARNING_FILE_TOO_MANY"};return this.log.error(s.msg),this.states.warnings.push(s),Promise.resolve()}this.states.upload.filesCount+=e.length;const n=e.map(async e=>{const r=e;if(t&&this.states.upload.filesCount>t){const e=`Maximum ${t} file(s) already uploaded. Marking ${r.relative} as skipped.`,s={msg:e,type:"WARNING_FILE_TOO_MANY"};this.log.error(e),this.states.warnings.push(s),this.states.upload.filesCount-=1,r.skip="SKIP_TOO_MANY"}else if(0===r.size){const e=`The file "${r.relative}" is empty. It will be skipped.`,t={msg:e,type:"WARNING_FILE_EMPTY"};r.skip="SKIP_EMPTY",this.states.upload.filesCount-=1,this.log.error(e),this.states.warnings.push(t)}else{if(r.path&&r.path.match(/\.(?:fastq|fq)(?:\.gz)?$/)&&(i&&r.size>i||o)){const e=`${r.relative}${r.size>i?" is too big and":""} is going to be split`;this.log.warn(e);const t={msg:e,type:"WARNING_FILE_SPLIT"};this.states.warnings.push(t);const s=i?{maxChunkBytes:i}:{maxChunkReads:o},a=r.path.match(/\.gz$/)?ae:ne,l=F.getFileID(),c=new te({bandwidth:this.config.options.transferPoolSize});let h=0;const u=async e=>(this.log.debug(`chunkHandler for ${e}`),await this.db.splitFile(e,r.path),this.stopped?(c.stop(),this.log.info(`stopped, so skipping ${e}`),Promise.reject(new Error("stopped"))):(h+=1,Z(e).then(t=>({name:f.basename(e),path:e,relative:e.replace(this.config.options.inputFolder,""),id:`${l}_${h}`,stats:t,size:t.bytes})).then(async e=>{const t=new Promise(t=>{c.enqueue(()=>(this.log.info(`chunk upload starting ${e.id} ${e.path}`),this.stopped?(this.log.info(`chunk upload skipped (stopped) ${e.id} ${e.path}`),c.stop(),t(),Promise.resolve()):this.uploadJob(e).then(()=>this.db.splitDone(e.path)).catch(t=>{this.log.error(`chunk upload failed ${e.id} ${e.path}: ${String(t)}`)}).finally(t)))});await t})));try{await a(r.path,s,u,this.log),c.stop()}catch(n){if(c.stop(),"Error: stopped"===String(n))return Promise.resolve();throw n}return this.db.uploadFile(r.path)}if(s&&r.size>s){const e=`The file "${r.relative}" is bigger than the maximum size limit (${G(s)}B). It will be skipped.`,t={msg:e,type:"WARNING_FILE_TOO_BIG"};r.skip="SKIP_TOO_BIG",this.states.upload.filesCount-=1,this.log.error(e),this.states.warnings.push(t)}else try{r.stats=await Z(r.path)}catch(a){this.log.error(`failed to stat ${r.path}: ${String(a)}`)}}return this.uploadJob(r)});try{return await Promise.all(n),this.log.info(`upload: inputBatchQueue (${n.length} jobs) complete`),this.loadUploadFiles()}catch(a){return this.log.error(`upload: enqueueUploadFiles exception ${String(a)}`),Promise.reject(a)}}async uploadJob(e){if("skip"in e)return this.db.skipFile(e.path);let t,i;try{this.log.info(`upload: ${e.id} starting`),t=await this.uploadHandler(e),this.log.info(`upload: ${t.id} uploaded and notified`)}catch(o){i=o,this.log.error(`upload: ${e.id} done, but failed: ${String(i)}`)}if(t||(t={}),i)this.log.error(`uploadJob ${i}`),this.states.upload.failure||(this.states.upload.failure={}),this.states.upload.failure[i]=this.states.upload.failure[i]?this.states.upload.failure[i]+1:1;else if(this.uploadState("success","incr",s({files:1},t.stats)),t.name){const e=f.extname(t.name);this.uploadState("types","incr",{[e]:1})}return Promise.resolve()}async receiveMessages(e){return e&&e.Messages&&e.Messages.length?(this.downloadWorkerPool||(this.downloadWorkerPool={}),e.Messages.forEach(e=>{this.downloadWorkerPool[e.MessageId]=1;const t=setTimeout(()=>{throw this.log.error(`this.downloadWorkerPool timeoutHandle. Clearing queue slot for message: ${e.MessageId}`),new Error("download timed out")},1e3*(60+this.config.options.downloadTimeout));this.processMessage(e).catch(e=>{this.log.error(`processMessage ${String(e)}`)}).finally(()=>{clearTimeout(t),e&&delete this.downloadWorkerPool[e.MessageId]})}),this.log.info(`downloader queued ${e.Messages.length} messages for processing`),Promise.resolve()):(this.log.info("complete (empty)"),Promise.resolve())}async processMessage(e){let i,o;if(!e)return this.log.debug("download.processMessage: empty message"),Promise.resolve();"Attributes"in e&&"ApproximateReceiveCount"in e.Attributes&&this.log.debug(`download.processMessage: ${e.MessageId} / ${e.Attributes.ApproximateReceiveCount}`);try{i=JSON.parse(e.Body)}catch(l){this.log.error(`error parsing JSON message.Body from message: ${JSON.stringify(e)} ${String(l)}`);try{await this.deleteMessage(e)}catch(c){this.log.error(`Exception deleting message: ${String(c)}`)}return Promise.resolve()}if(i.telemetry){const{telemetry:t}=i;if(t.tm_path)try{this.log.debug(`download.processMessage: ${e.MessageId} fetching telemetry`);const s=await this.sessionedS3(),o=await s.getObject({Bucket:i.bucket,Key:t.tm_path}).promise();this.log.info(`download.processMessage: ${e.MessageId} fetched telemetry`),t.batch=o.Body.toString("utf-8").split("\n").filter(e=>e&&e.length>0).map(e=>{try{return JSON.parse(e)}catch(c){return this.log.error(`Telemetry Batch JSON Parse error: ${String(c)}`),e}})}catch(h){this.log.error(`Could not fetch telemetry JSON: ${String(h)}`)}try{this.telemetryLogStream.write(JSON.stringify(t)+g)}catch(u){this.log.error(`error writing telemetry: ${u}`)}this.config.options.telemetryCb&&this.config.options.telemetryCb(t)}if(!i.path)return this.log.warn("nothing to download"),Promise.resolve();const r=i.path.match(/[\w\W]*\/([\w\W]*?)$/),n=r?r[1]:"";if(o=this.config.options.outputFolder,i.telemetry&&i.telemetry.hints&&i.telemetry.hints.folder){this.log.debug(`using folder hint ${i.telemetry.hints.folder}`);const e=i.telemetry.hints.folder.split("/").map(e=>e.toUpperCase());o=f.join.apply(null,[o,...e])}t.mkdirpSync(o);const a=f.join(o,n);if("data+telemetry"===this.config.options.downloadMode){const t=[""];let s=this.config&&this.config.workflow&&this.config.workflow.settings&&this.config.workflow.settings.output_format?this.config.workflow.settings.output_format:[];("string"===typeof s||s instanceof String)&&(s=s.trim().split(/[\s,]+/));try{t.push(...s)}catch(c){this.log.error(`Failed to work out workflow file suffixes: ${String(c)}`)}try{const s=t.map(t=>{const s=i.path+t,o=a+t;return this.log.debug(`download.processMessage: ${e.MessageId} downloading ${s} to ${o}`),new Promise((r,n)=>{this.initiateDownloadStream({bucket:i.bucket,path:s},e,o).then(r).catch(e=>{this.log.error(`Caught exception waiting for initiateDownloadStream: ${String(e)}`),t?n(e):r()})})});await Promise.all(s)}catch(c){this.log.error(`Exception fetching file batch: ${String(c)}`)}try{const e=!(!i.telemetry||!i.telemetry.json)&&i.telemetry.json.exit_status;e&&this.config.options.dataCb&&this.config.options.dataCb(a,e)}catch(h){this.log.warn(`failed to fire data callback: ${h}`)}}else{const e=i.telemetry.batch_summary&&i.telemetry.batch_summary.reads_num?i.telemetry.batch_summary.reads_num:1;this.downloadState("success","incr",{files:1,reads:e})}try{await this.deleteMessage(e)}catch(c){this.log.error(`Exception deleting message: ${String(c)}`)}return this.realtimeFeedback("workflow_instance:state",{type:"stop",id_workflow_instance:this.config.instance.id_workflow_instance,id_workflow:this.config.instance.id_workflow,component_id:"0",message_id:s(e).MessageId,id_user:this.config.instance.id_user}).catch(e=>{this.log.warn(`realtimeFeedback failed: ${String(e)}`)}),Promise.resolve()}async initiateDownloadStream(e,i,o){return new Promise(async(r,n)=>{let a,l,c;try{a=await this.sessionedS3()}catch(d){n(d)}const h=s=>{if(this.log.error(`Error during stream of bucket=${e.bucket} path=${e.path} to file=${o} ${String(s)}`),clearTimeout(this.timers.transferTimeouts[o]),delete this.timers.transferTimeouts[o],!l.networkStreamError)try{l.networkStreamError=1,l.close(),t.remove(o).then(()=>{this.log.warn(`removed failed download ${o}`)}).catch(e=>{this.log.warn(`failed to remove ${o}. unlinkException: ${String(e)}`)}),c.destroy&&(this.log.error(`destroying read stream for ${o}`),c.destroy())}catch(d){this.log.error(`error handling stream error: ${String(d)}`)}};try{const s={Bucket:e.bucket,Key:e.path};l=t.createWriteStream(o);const i=a.getObject(s);i.on("httpHeaders",(e,t)=>{this.downloadState("progress","incr",{total:parseInt(t["content-length"],10)})}),c=i.createReadStream()}catch(p){return this.log.error(`getObject/createReadStream exception: ${String(p)}`),void n(p)}c.on("error",h),l.on("finish",async()=>{if(!l.networkStreamError){this.log.debug(`downloaded ${o}`);try{const e=f.extname(o),t=await Z(o);this.downloadState("success","incr",s({files:1},t)),this.downloadState("types","incr",{[e]:1}),this.downloadState("progress","decr",{total:t.bytes,bytes:t.bytes})}catch(e){this.log.warn(`failed to stat ${o}: ${String(e)}`)}this.reportProgress()}}),l.on("close",t=>{this.log.debug(`closing writeStream ${o}`),t&&this.log.error(`error closing write stream ${t}`),clearInterval(this.timers.visibilityIntervals[o]),delete this.timers.visibilityIntervals[o],clearTimeout(this.timers.transferTimeouts[o]),delete this.timers.transferTimeouts[o],setTimeout(this.checkForDownloads.bind(this)),this.log.info(`download.initiateDownloadStream: ${i.MessageId} downloaded ${e.path} to ${o}`),r()}),l.on("error",h);const u=()=>{h(new Error("transfer timed out"))};this.timers.transferTimeouts[o]=setTimeout(u,1e3*this.config.options.downloadTimeout);this.timers.visibilityIntervals[o]=setInterval(async()=>{this.stopped&&(clearInterval(this.timers.visibilityIntervals[o]),delete this.timers.visibilityIntervals[o]);const e=this.config.instance.outputQueueURL,t=i.ReceiptHandle;this.log.debug({message_id:i.MessageId},"updateVisibility");try{await this.sqs.changeMessageVisibility({QueueUrl:e,ReceiptHandle:t,VisibilityTimeout:this.config.options.inFlightDelay}).promise()}catch(s){this.log.error({message_id:i.MessageId,queue:e,error:s},"Error setting visibility"),clearInterval(this.timers.visibilityIntervals[o])}},900*this.config.options.inFlightDelay),c.on("data",e=>{clearTimeout(this.timers.transferTimeouts[o]),this.timers.transferTimeouts[o]=setTimeout(u,1e3*this.config.options.downloadTimeout),this.downloadState("progress","incr",{bytes:e.length})}).pipe(l)})}async uploadHandler(e){const s=await this.sessionedS3();let i;const o=e.relative.replace(/^[\\/]+/,"").replace(/\\/g,"/").replace(/\//g,"_"),r=[this.config.instance.bucketFolder,"component-0",o,o].join("/").replace(/\/+/g,"/");let n;return new Promise((o,a)=>{const l=()=>{i&&!i.closed&&i.close(),a(new Error(`${e.name} timed out`))};n=setTimeout(l,1e3*(this.config.options.uploadTimeout+5));try{i=t.createReadStream(e.path)}catch(c){return clearTimeout(n),void a(c)}i.on("error",e=>{i.close();let t="error in upload readstream";e&&e.message&&(t+=`: ${e.message}`),clearTimeout(n),a(new Error(t))}),i.on("open",()=>{const t={Bucket:this.config.instance.bucket,Key:r,Body:i};this.config.instance.key_id&&(t.SSEKMSKeyId=this.config.instance.key_id,t.ServerSideEncryption="aws:kms"),e.size&&(t["Content-Length"]=e.size),this.uploadState("progress","incr",{total:e.size});let c=0;const h=s.upload(t,{partSize:10485760,queueSize:1}),u=this.initSessionManager(null,[h.service]);u.sts_expiration=this.sessionManager.sts_expiration,h.on("httpUploadProgress",async e=>{if(this.stopped)a(new Error("stopped"));else{this.uploadState("progress","incr",{bytes:e.loaded-c}),c=e.loaded,clearTimeout(n),n=setTimeout(l,1e3*(this.config.options.uploadTimeout+5));try{await u.session()}catch(t){this.log.warn(`Error refreshing token: ${String(t)}`)}}}),h.promise().then(()=>{this.log.info(`${e.id} S3 upload complete`),i.close(),clearTimeout(n),this.uploadComplete(r,e).then(()=>{o(e)}).catch(e=>{a(e)}).finally(()=>{this.uploadState("progress","decr",{total:e.size,bytes:e.size})})}).catch(t=>{this.log.warn(`${e.id} uploadStreamError ${t}`),a(t)})})})}async uploadComplete(e,t){this.log.info(`${t.id} uploaded to S3: ${e}`);const i={bucket:this.config.instance.bucket,outputQueue:this.config.instance.outputQueueName,remote_addr:this.config.instance.remote_addr,user_defined:this.config.instance.user_defined||null,apikey:this.config.options.apikey,id_workflow_instance:this.config.instance.id_workflow_instance,id_master:this.config.instance.id_workflow,utc:(new Date).toISOString(),path:e,prefix:e.substring(0,e.lastIndexOf("/"))};if(this.config.instance.chain)try{i.components=JSON.parse(JSON.stringify(this.config.instance.chain.components)),i.targetComponentId=this.config.instance.chain.targetComponentId}catch(r){return this.log.error(`${t.id} exception parsing components JSON ${String(r)}`),Promise.reject(r)}if(this.config.instance.key_id&&(i.key_id=this.config.instance.key_id),this.config.options.agent_address)try{i.agent_address=JSON.parse(this.config.options.agent_address)}catch(n){this.log.error(`${t.id} Could not parse agent_address ${String(n)}`)}i.components&&Object.keys(i.components).forEach(e=>{"uploadMessageQueue"===i.components[e].inputQueueName&&(i.components[e].inputQueueName=this.uploadMessageQueue),"downloadMessageQueue"===i.components[e].inputQueueName&&(i.components[e].inputQueueName=this.downloadMessageQueue)});let o={};try{const e=await this.discoverQueue(this.config.instance.inputQueueName),s=await this.sessionedSQS();this.log.info(`${t.id} sending SQS message to input queue`),o=await s.sendMessage({QueueUrl:e,MessageBody:JSON.stringify(i)}).promise()}catch(a){return this.log.error(`${t.id} exception sending SQS message: ${String(a)}`),Promise.reject(a)}return this.realtimeFeedback("workflow_instance:state",{type:"start",id_workflow_instance:this.config.instance.id_workflow_instance,id_workflow:this.config.instance.id_workflow,component_id:"0",message_id:s(o).MessageId,id_user:this.config.instance.id_user}).catch(e=>{this.log.warn(`realtimeFeedback failed: ${String(e)}`)}),this.log.info(`${t.id} SQS message sent. Mark as uploaded`),this.db.uploadFile(t.path)}async fetchTelemetry(){if(!this.config||!this.config.instance||!this.config.instance.summaryTelemetry)return Promise.resolve();const e=f.join(le(),"instances"),s=f.join(e,this.config.instance.id_workflow_instance),i=[];Object.keys(this.config.instance.summaryTelemetry).forEach(e=>{const o=this.config.instance.summaryTelemetry[e]||{},r=o[Object.keys(o)[0]];if(!r)return;const n=f.join(s,`${e}.json`);i.push(this.REST.fetchContent(r).then(e=>{t.writeJSONSync(n,e),this.log.debug(`fetched telemetry summary ${n}`)}).catch(e=>{this.log.debug(`Error fetching telemetry: ${String(e)}`)}))});let o=0;try{await Promise.all(i)}catch(r){o+=1}return o&&this.log.warn("summary telemetry incomplete"),Promise.resolve()}}ce.version=F.version,ce.REST=se,ce.utils=F,ce.SessionManager=oe,ce.EPI2ME_HOME=le(),ce.Profile=ee,ce.Factory=class{constructor(e,t){this.EPI2ME=e,this.options=t,this.masterInstance=new this.EPI2ME(this.options),this.log=this.masterInstance.log,this.REST=this.masterInstance.REST,this.graphQL=this.masterInstance.graphQL,this.runningInstances={}}async startRun(e){const t=new this.EPI2ME(this.options);let s;try{s=await t.autoStart(e),this.runningInstances[s.id_workflow_instance]=t}catch(i){this.log.error(`Experienced error starting ${String(i)}`);try{await t.stopEverything()}catch(o){this.log.error(`Also experienced error stopping ${String(o)}`)}}return t}};export default ce;
